[
["index.html", "EPI 563: Spatial Epidemiology, Fall 2020 How to use this eBook Strategy for using this eBook", " EPI 563: Spatial Epidemiology, Fall 2020 Michael Kramer Last updated: 2020-08-15 How to use this eBook Welcome to Concepts &amp; Applications in Spatial Epidemiology (EPI 563)! This eBook is one of several sources of information and support for your progress through the semester. For an overview of the course, expectations, learning objectives, assignments, and grading, please review the full course syllabus on Canvas. This eBook serves to provide a ‘jumping off point’ for the content to be covered each week. Specifically, the content herein will introduce key themes, new vocabulary, and provide some additional detail that is complementary to the asynchronous (pre-recorded) video lectures, and foundational to the synchronous (in class) work. Strategy for using this eBook There is a separate module or chapter for each week’s content. In general, the content within each week’s section is divided into two sections focusing on spatial thinking and spatial analysis. This dichotomy does not always hold, but in broad terms you can expect these sections to be more specific to content in class on Tuesday versus Thursday respectively. Spatial thinking for epidemiology: This section introduces vocabulary, concepts, and themes that are important to the incorporation of spatialized or geo-referenced data into epidemiologic work. At a minimum, plan to read this content prior to class Tuesday, although you will likely benefit from reading both sections before Tuesday. Spatial analysis for epidemiology: This section is more focused on data management, visualization, spatial statistics, and interpretation. This content is relevant for our work together on Tuesday’s, but is essential for successful work in the Thursday lab activities. Please note that I will be continually updating the eBook throughout the semester, so if you choose to download, please double-check the Last updated date to be sure you have the most recent version. "],
["software-installation.html", "Software installation Installing R on your computer Installing RStudio on your computer", " Software installation The information in this module follow on the pre-class video on setting up R and RStudio on your computer. Installing R on your computer As of August 2020, the most up-to-date version of R is 4.0. The R Project for Statistical Computing are continually working to update and improve R, and as a result there are new versions 1-2 times per year. If you already have R installed, you can open the console and check your current version by doing this: R.Version()$version.string If you do not have R or have an older version than 3.5.1 you can install R by going to the R repository: https://www.r-project.org/. Note that there are many ‘mirrors’ or servers where the software is stored. Generally it is wise to select one that is geographically close to you, although any should work in theory. One mirror that is relatively close to Atlanta is here: http://archive.linux.duke.edu/cran/ Installing RStudio on your computer The current version of RStudio 1.3.1056. If you do not have RStudio or have a version older than 1.2 please install/update. TO INSTALL: go to https://www.rstudio.com/products/rstudio/download/ TO UPDATE: Open RStudio and go to Help Menu and choose ‘Check for Updates’ "],
["installing-packages-for-this-course.html", "Installing packages for this course Installing Rtools40 Installing packages used for general data science Installing packages use for geographic data Installing packages used for spatial data manipulation &amp; visualization Installing packages used for spatial analysis", " Installing packages for this course While base R has a great deal of essential functionality, most of the power of R comes from the rapidly growing list of user-created and contributed ‘packages’. A package is simply a bundle of functions and tools, sometimes also including example datasets, basic documentation, and even tutorial ‘vignettes’. You can see all the official R packages by going here: https://cran.r-project.org/web/packages/. The most common way to install package in R is with the install.pacakges() command. For instance to install the package ggplot2 you do this: install.packages(\"ggplot2\") Notice that for install.packages() you need quotes around the package name. Remember that you only need to install a package once (although you may have to update packages occasionally – see the green Update button in the Packages tab in R Studio). When you want to actually use a package (for example ggplot2) you call it like this: library(ggplot2) Notice that for the library() function you do not need quotes around the package name (unlike the install.packages() above). If your call to library() is working, nothing visible happens. However if you see errors, they might be because your package is out of date (and thus needs to be updated/reinstalled), or because some important dependencies are missing. Dependencies are other packages on which this package depends. Typically these are installed by default, but sometimes something is missing. If so, simply install the missing package and then try calling library(ggplot2) again. While most packages can be installed as mentioned above (e.g. using install.packages()), there are instances where an installation requires additional tools, for instance to install from source or from github. Luckily there is a package for that! It is called Rtools, and you should install that before you install the packages below. As you submit each installation request, note the output. If you get a warning that says installation was not possible because you are missing a package ‘namespace’, that suggests you are missing a dependency. Try installing the pacakge mentioned in the error. If you have trouble, reach out to the TA’s! Installing Rtools40 If your laptop uses a Windows operating system, you may need Rtools40 installed. This is a supplemental package that resides outside of R but is needed to install some packages from source code. However it appears that if you have a MacOS, these tools are already built-in. So you do not need Rtools40 for Mac. If you are running Windows, navigate to this website: https://cran.r-project.org/bin/windows/Rtools/ and follow the instructions specific to your operating system. Installing packages used for general data science For the rest of this page, copy and paste the provided code in order to install packages necessary for this course. Notice if you hover to the right of a code-chunk in the html version of the eBook, you will see a copy icon for quick copying and pasting. These packages will support some of our general work in R, including working with RMarkdown and R Notebooks, as well as data manipulation tools from the tidyverse. You can learn more about the tidyverse here: https://tidyverse.tidyverse.org/. The tidyverse is actually a collection of data science tools including the visualization/plotting package ggplot2 and the data manipulation package dplyr. For that reason, when you install.packages('tidyverse') below, you are actually installing multiple packages! The packages tinytex, rmarkdown, and knitr are all necessary for creating R Notebooks, which is the format by which many assignments will be submitted. install.packages(&#39;tidyverse&#39;) install.packages(c(&#39;tinytex&#39;, &#39;rmarkdown&#39;, &#39;knitr&#39;)) tinytex::install_tinytex() # this function installs the tinytex LaTex on your # computer which is necessary for rendering (creating) PDF&#39;s Installing packages use for geographic data There are many ways to get the data we want for spatial epidemiology into R. Because we often (but don’t always) use census geographies as aggregating units, and census populations as denominators, the following packages will be useful. They are designed to quickly extract both geographic boundary files (e.g. ‘shapefiles’) as well as attribute data from the US Census website via an API. NOTE: For these to work you have to request a free Census API key. Notice the help() function below to get instructions on how to do this. install.packages(c(&#39;tidycensus&#39;,&#39;tigris&#39;)) help(&#39;census_api_key&#39;,&#39;tidycensus&#39;) Installing packages used for spatial data manipulation &amp; visualization This section installs a set of tools specific to our goals of importing, exporting, manipulating, visualizing, and analyzing spatial data. The first line of packages have functions for defining, importing, exporting, and manipulating spatial data. The second line has some tools we will use for visualizing spatial data (e.g. making maps!). install.packages(c(&#39;sp&#39;, &#39;sf&#39;, &#39;rgdal&#39;, &#39;rgeos&#39;, &#39;maptools&#39;, &#39;OpenStreetMap&#39;)) install.packages(c(&#39;tmap&#39;, &#39;tmaptools&#39;, &#39;ggmap&#39;, &#39;shinyjs&#39;, &#39;shiny&#39;, &#39;micromap&#39;)) Installing packages used for spatial analysis Finally these are packages specifically to spatial analysis tasks we’ll carry out. install.packages(c(&#39;spdep&#39;, &#39;CARBayes&#39;, &#39;sparr&#39;, &#39;spatialreg&#39;)) install.packages(c(&#39;GWmodel&#39;, &#39;spgwr&#39;) ) "],
["locating-spatial-epidemiology.html", "Week 1 Locating Spatial Epidemiology 1.1 Getting Ready 1.2 Spatial Thinking in Epidemiology 1.3 Spatial Analysis in Epidemiology", " Week 1 Locating Spatial Epidemiology 1.1 Getting Ready 1.1.1 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Explain the potential role of spatial analysis for epidemiologic thinking and practice. Produce simple thematic maps of epidemiologic data in R. 1.1.2 Additional Resources Geocompution with R by Robin Lovelace. This will be a recurring ‘additional resource’ as it provides lots of useful insight and strategy for working with spatial data in R. I encourage you to browse it quickly now, but return often when you have qusetiona about how to handle geogrpahic data (especially of class sf) in R. A basic introduction to the ggplot2 package. This is just one of dozens of great online resources introducing the grammar of graphics approach to plotting in R. A basic introduction to the tmap package This is also only one of many introductions to the tmap mapping package. tmap builds on the grammar of graphics philosophy of ggplot2, but brings a lot of tools useful for thematic mapping! 1.1.3 Important Vocabulary TABLE 1.2: Vocabulary for Week 1 TermDefinition Unit of analysisThe unit or object that is measured, analyzed, and about which you wish to make inference. Examples of units of analysis are person, neighborhood, city, state, or hospital. Attribute dataNonspatial information about a geographic feature in a GIS, usually stored in a table and linked to the feature by a unique identifier. For example, attributes of a county might include the population size, density, and birth rate for the resident population Geometry dataSpatial information about a geogrpahic feature. This could include the x, y coordinates for points or for vertices of lines or polygons, or the cell coordinates for raster data Spatial data model: vectorA coordinate-based data model that represents geographic features as points, lines, and polygons. Each point feature is represented as a single coordinate pair, while line and polygon features are represented as ordered lists of vertices. Attributes are associated with each vector feature, as opposed to a raster data model, which associates attributes with grid cells (see figure below) Spatial data model: rasterA spatial data model that defines space as an array of equally sized cells arranged in rows and columns, and composed of single or multiple bands. Each cell contains an attribute value and location coordinates. Unlike a vector structure, which stores coordinates explicitly, raster coordinates are contained in the ordering of the matrix. Groups of cells that share the same value represent the same type of geographic feature (see Figure below) Geographic coordinate systemA reference system that uses latitude and longitude to define the locations of points on the surface of a sphere or spheroid. A geographic coordinate system definition includes a datum, prime meridian, and angular unit DatumThe reference specifications of a measurement system, usually a system of coordinate positions on a surface (a horizontal datum) or heights above or below a surface (a vertical datum) ProjectionA method by which the curved surface of the earth is portrayed on a flat surface. This generally requires a systematic mathematical transformation of the earth's graticule of lines of longitude and latitude onto a plane. Some projections can be visualized as a transparent globe with a light bulb at its center (though not all projections emanate from the globe's center) casting lines of latitude and longitude onto a sheet of paper. Generally, the paper is either flat and placed tangent to the globe (a planar or azimuthal projection) or formed into a cone or cylinder and placed over the globe (cylindrical and conical projections). Every map projection distorts distance, area, shape, direction, or some combination thereof 1.2 Spatial Thinking in Epidemiology When first learning epidemiology, it can be difficult to distinguish between the concepts, theories, and purpose of epidemiology versus the skills, tools, and methods that we use to implement epidemiology. But these distinctions are foundational to our collective professional identity, and to the way we go about doing our work. For instance do you think of epidemiologists as data analysts, scientists, data scientists, technicians or something else? These questions are bigger than we can address in this class, but their importance becomes especially apparent when learning an area such as spatial epidemiology. This is because there is a tendency for discourse in spatial epidemiology to focus primarily on the data and the methods without understanding how each of those relate to the scientific questions and population health we are ultimately responsible for. Distinguishing these threads is an overarching goal of this course, even as we learn the data science and spatial analytic tools. One quite simplistic but important example of how our questions and methods are inter-related is apparent when we think of data. Data is central to quantitative analysis, including epidemiologic analysis. So how is data different in spatial epidemiology? The first thing that might come to mind is that we have explicitly geographic or spatial measures contained within our data. But even more fundamental than the content is thinking about the unit of analysis. While in many other examples in your epidemiology coursework, the explicit (or sometimes implicit) unit of analysis has been the individual person. It is certainly possible for individuals to be the unit of analysis in spatial epidemiology (e.g. we might measure the exact place where an individual lives). However oftentimes the units we observe and measure in spatial epidemiology – and therefore the units that compose much of our data – are not individuals but instead are geographic units (e.g. census tract, county, state, etc) and by extension the collection or aggregation of all the individuals therein. This has implications for precision, bias, and ultimately for inference (e.g. the meaning we can make from our analysis), as we’ll discuss throughout the semester. One concrete implication of the above discussion is that you should always be able to answer a basic question about any dataset you wish to analyze: “what does one row of data represent?” A row of data is one way to think of the unit of analysis, and often (but not always) in spatial epidemiology a row of data is a summary of the population contained by a geographic unit. Said another way it is an ecologic summary of the population. As we move through the semester, I encourage you to dig deep into how methods work, but also to step back and ask questions like “Why would I choose this method?” or “What question in epidemiology is this useful for?” 1.3 Spatial Analysis in Epidemiology 1.3.1 Spatial data storage foramts If you have worked with spatial or GIS data using ESRI’s ArcMap, you will be familiar with what are called shapefiles. This is one very common format for storing geographic data on computers. ESRI shapefiles are not actually a single file, but are anywhere from four to eight different files all with the same file name but different extensions (e.g. .shp, .prj, .shx, etc). Each different file (corresponding to an extension) contains a different portion of the data ranging from the geometry data, the attribute data, the projection data, an index connecting it all together, etc. What you may not know is that shapefiles are not the only (and in my opinion definitely not the best) way to store geographic data. In this class I recommend storing data in a format called geopackages indicated by the .gpkg extension. Geopackages are an open source format that were developed to be functional on mobile devices. They are useful when we are storing individual files in an efficient and compact way. It is worth noting that many GIS programs including ArcMap and QGIS can both read and write the geopackage format; so there is no constraint or limitation in terms of software when data are stored in .gpkg format. 1.3.2 Representing spatial data in R Just as our conceptualization of, or thinking about data in spatial epidemiology requires some reflection, the actual storage and representation of that data with a computer tool such as R also requires some attention. Specifically spatial data in R is not exactly like the conventional aspatial epidemiologic data, but it is also need not be as complex as spatial data in software platforms like ESRI’s ArcMap. First, it may be obvious, but spatial data is more complex than simple rectangular attribute data (e.g. data tables where a row is an observation and a column is a variable). To be spatial, a dataset must have a representation of geography, spatial location, or spatial relatedness, and that is most commonly done with either a vector or raster data model (see description above in vocabulary). Those spatial or geographic representations must be stored on your computer and/or held in memory, hopefully with a means for relating or associating the individual locations with their corresponding attributes. For example we want to know the attribute (e.g. the count of deaths for a given place), and the location of that place, and ideally we want to the two connected together. Over the past 10+ years, R has increasingly been used to analyze and visualize spatial data. Early on, investigators tackling the complexities of spatial data analysis developed a number of ad hoc, one-off approaches to these data. This worked in the short term for specific applications, but it created new problems as users needed to generalize a method to a new situation, or chain together steps. In those settings it was not uncommon to convert a dataset to multiple different formats to accomplish all tasks. An eventual response to this early tumult was a thoughtful and systematic approach to defining a class of data that tackled the unique challenges of spatial data in R. Roger Bivand, Edzer Pebesma and others developed the sp package which defined spatial data classes, and provided functional tools to interact with them. The sp package defined specific data classes to hold points, lines, and polygons, as well as raster/grid data; each of these data classes can contain geometry only (these have names like SpatialPoints or SpatialPolygons) or could contain geometry plus related data attributes (these have names like SPatialPointsDataFrame or SpatialPolygonsDataFrame). Each spatial object can contain all the information spatial data might include: the spatial extent (min/max x, y values), the coordinate system or spatial projection, the geometry information, the attribute information, etc. Because of the flexibility and power of the sp* class of objects, they became a standard up until the last few years. sp* classes continue to be the only format allowed by a few of the packages we will use this semester. However analysts sometimes find the complexity of the sp* objects to be a hindrance to efficient processing of geographic data. Specifically the information is stored in numerous ‘slots’ (e.g. special storage structures useful for computer programmers but less useful for applied analysts). As the number of ways to manipulate and visualize data increases, there was a desire to make spatial data behave more like tabular or rectangular data. This led the same team (e.g. Bivand, Pebesma, others) to develop the Simple Features set of spatial data classes for R. Loaded with the sf package, this data format has quickly become the standard for handling spatial data in R. Recognizing that many users and functions prefer the older sp* objects, the sf package includes a number of utility functions for easily converting back and forth. In this class we will use sf* class objects as the preferred data class, but because some of the tools we’ll learn require sp* we will occasionally go back and forth. sf* data classes are designed to hold all the essential spatial information (projection, extent, geometry), but do so with an easy to evaluate data.frame format that integrates the attribute information and the geometry information together. The result is more intuitive sorting, selecting, aggregating, and visualizing. 1.3.3 Benefits of sf data classes As Robin Lovelace writes in his online eBook, Gecomputation in R, sf data classes offer an approach to spatial data that is compatible with QGIS and PostGIS, important non-ESRI open source GIS platforms, and sf functionality compared to sp provides: Fast reading and writing of data Enhanced plotting performance sf objects can be treated as data frames in most operations sf functions can be combined using %&gt;% pipe operator and works well with the tidyverse collection of R packages (see Tips for using dplyr for examples) sf function names are relatively consistent and intuitive (all begin with st_) 1.3.4 Working with spatial data in R Here and in lab, one example dataset we will use, called ga.mvc quantifies the counts and rates of death from motor vehicle crashes in each of Georgia’s \\(n=159\\) counties. The dataset is vector in that it represents counties as polygons with associated attributes (e.g. the mortality information, county names, etc). 1.3.4.1 Importing spatial data into R It is important to distinguish between two kinds of data formats. There is a way that data is stored on a computer hard drive, and then there is a way that data is organized and managed inside a program like R. The shapefiles (.shp) popularized by ESRI/ArcMap is an example of a format for storing spatial data on a hard drive. In contrast, the discussion above about the sf* and sp* data classes refer to how data is organized inside R. Luckily, regardless of how data is stored on your computer, it is possible to import almost any format into R, and once inside R it is possible to make it into either the sp* or sf* data class. That means if you receive data as a .shp shapefile, as a .gpkg geopackage, or as a .tif raster file, each can be easily imported. All sf functions that act on spatial objects begin with the prefix st_. Therefore to import (read) data we will use st_read(). This function determines how to import the data based on the extension of the file name you specify. Look at the help documentation for st_read(). Notice that the first argument dsn=, might be a complete file name (e.g. myData.shp), or it might be a folder name (e.g. mygeodatabase.gdb). So if you had a the motor vehicle crash data saved as both a shapefile (mvc.shp, which is actually six different files on your computer), and as a geopackage (mvc.gpkg) you can read them in like this: # this is the shapefile mvc.a &lt;- st_read(&#39;GA_MVC/ga_mvc.shp&#39;) # this is the geopackage mvc.b &lt;- st_read(&#39;GA_MVC/ga_mvc.gpkg&#39;) We can take a look at the defined data class of the imported objects within R: class(mvc.a) ## [1] &quot;sf&quot; &quot;data.frame&quot; class(mvc.b) ## [1] &quot;sf&quot; &quot;data.frame&quot; First, note that when we use the st_read() function, the data class (e.g. the way the data are defined and organized within R) is the same for both mvc.a (which started as a .shp file) and mvc.b (which started as a .gpkg file). That is because st_read() automatically classifies spatial data using sf classes when it imports. You will also notice that when we examined the class() of each object, they are classified as both sf and data.frame class. That is incredibly important, and it speaks to an elegant simplicity of the sf* data classes! That it is classified as sf is perhaps obvious; but the fact that each object is also classified as data.frame means that we can treat the object for the purposes of data management, manipulation and analysis as a relatively simple-seeming object: a rectangular data.frame. How does that work? We will explore this more in lab but essentially each dataset has rows (observations) and columns (variables). We can see the variable/column names like this: names(mvc.a) ## [1] &quot;GEOID&quot; &quot;NAME&quot; &quot;MVCRATE_17&quot; &quot;geometry&quot; names(mvc.b) ## [1] &quot;GEOID&quot; &quot;NAME&quot; &quot;MVCRATE_17&quot; &quot;geom&quot; We can see that each dataset has the same attribute variables (e.g. GEOID, NAME, MVCRATE_17), and then a final column called geometry in one and called geom in another. These geometry columns are unique in that they don’t hold a single value like the other columns; each ‘cell’ in those columns actually contains an embedded list of \\(x,y\\) coordinates defining the vertices of the polygons for each of Georgia’s counties. Combining these two observations, we now know that we can work with a wide range of spatial data formats, and that once imported we can conceive of (and manipulate!) these data almost as if they were simple rectangular datasets. This has implications for subsetting, recoding, merging, and aggregating data as we’ll learn in the coming weeks. 1.3.4.2 Exporting spatial data from R While importing is often the primary challenge with spatial data and R, it is not uncommon that you might modify or alter a spatial dataset and wish to save it for future use, or to write it out to disk to share with a colleague. Luckily the sf package has the same functionality to write an sf spatial object to disk in a wide variety of formats including shapefiles (.shp) and geopackages (.gpkg). Again, R uses the extension you specify in the filename to determine the target format. # Write the file mvc to disk as a shapefile format st_write(mvc, &#39;GA_MVC/ga_mvc_v2.shp&#39;) # Write the file mvc to disk as a geopackage format st_write(mvc, &#39;GA_MVC/ga_mvc_v2.gpkg&#39;) 1.3.5 Basic visual inspection/plots The base-R plot() function is extended by the sf package. That means that if you call plot() on a spatial object without having loaded sf, the results will be different than if plot() called after loading sf. When you plot() with sf, by default it will try to make a map for every variable in the data frame! Try it once. If this is not what you want, you can force it to only plot some variables by providing a vector of variable names. plot(mvc) # this plots a panel for every column - or actually the first 10 columns ## Warning: plotting the first 9 out of 17 attributes; use max.plot = 17 to plot ## all plot(mvc[&#39;MVCRATE_05&#39;]) # this plots only a single variable, the MVC mortality rate for 2005 plot(mvc[c(&#39;MVCRATE_05&#39;, &#39;MVCRATE_17&#39;)]) # this plots two variables: MVC rate in 2005 &amp; 2017 You might only want to see the geometry of the spatial object (e.g. not attributes) if you are checking its extent, the scale, or otherwise confirming something about the spatial aspects of the object. Here are two approaches to quickly plot the geometry: plot(st_geometry(mvc)) # st_geometry() returns the geom information to plot plot(mvc$geom) # this is an alternative approach...directly plot the &#39;geom&#39; column 1.3.6 Working with CRS and projection If CRS (coordinate reference system) and projection information was contained in the original file you imported, it will be maintained. If there is NO CRS information imported it is critical that you find out the CRS information from the data source! The most unambiguous way to describe a projection is by using the EPSG code, which stands for European Petroleum Survey Group. This consortium has standardized hundreds of projection definitions in a manner adopted by several R packages including rgdal and sf. This course is not a GIS course, and learning about the theory and application of coordinate reference systems and projections is not our primary purpose. However some basic knowledge is necessary for successfully working with spatial epidemiologic data. Here are several resources you should peruse to learn more about CRS, projections, and EPSG codes: A useful overview/review of coordinate reference systems in R Robin Lovelace’s Geocompuation in R on projections with sf EPSG website: This link is to a searchable database of valid ESPG codes Here are some useful EPSG codes We already saw the CRS/projection information when we used the head() function above; it was at the top and read WGS 84. Recall there are two main types of CRS: purely geographic which is to say coordinate locations are represented as latitude and longitude degrees; and projected which means the coordinate values have been transformed for representation of the spherical geoid onto a planar (Euclidean) coordinate system. WGS 84 is a ubiquitous geographic coordinate system common to boundary files retrieved from the U.S. Census bureau. An important question when you work with a spatial dataset is to understand whether it is primarily a geographic or projected CRS, and if so which one. st_is_longlat(mvc) ## [1] TRUE This quick logical test returns TRUE or FALSE to answer the question “Is the sf object simply a longitude/latitude geographic CRS?”. The answer in this case is TRUE because WGS 84 is a geographic (longlat) coordinate system. But what if it were FALSE or we wanted to know more about the CRS/projection? st_crs(mvc) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;unknown&quot;], ## AREA[&quot;World&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] This somewhat complicated looking output is a summary of the CRS stored with the spatial object. There are two things to note about this output: At the top, the User input is WGS 84 At the bottom of the section labeled GEOGCRS it says ID[\"EPSG\",4326\"] While there are literally hundreds of distinct EPSG codes describing different geographic and projected coordinate systems, for this semester there are three worth remembering: EPSG: 4326 is a common geographic (unprojected or long-lat) CRS EPSG: 3857 is also called WGS 84/Web Mercator, and is the dominant CRS used by Google Maps EPSG: 5070 is the code for a projected CRS called Albers Equal Area which has the benefit of representing the visual area of maps in an equal manner. Once the CRS/projection is clearly defined, you may choose to transform or project the data to a different system. The sf package has another handy function called st_transform() that takes in a spatial object (dtaaset) with one CRS and outputs that object transformed to a new CRS. # This uses the Albers equal area USA, mvc.aea &lt;- st_transform(mvc, 5070) # This uses the Web Mercator CRS (EPSG 3857) which is just barely different from EPSG 4326 mvc.wm &lt;- st_transform(mvc, 3857) # Now let&#39;s look at them side-by-side plot(st_geometry(mvc), main = &#39;EPSG 4326&#39;) plot(st_geometry(mvc.wm), main = &#39;Web Mercator (3857)&#39;) plot(st_geometry(mvc.aea), main = &#39;Albers Equal Area (5070)&#39;) Do you see the difference between the three? Because EPSG 4326 and 3857 are both unprojected (e.g. they are long/lat), they appear quite similar but are not identical. Albers Equal Area, on the other hand, is more distinct. In general we will prefer to use ‘projected’ rather than ‘unprojected’ (long/lat only) data for both visualization and analysis. That means that whenever you bring in a new dataset you will need to check the CRS and project or transform as desired. Important: It is important to distinguish between defining the current projection of data and the act of projecting or transforming data from one known system to a new CRS/projection. We cannot transform data until we correctly define its current or original CRS/projection status. The above function tells us what the current status is. In some cases data do not have associated CRS information and this might be completely blank (for instance if you read in numerical \\(x,y\\) points from a geocoding or GPS process). In those cases you can set the underlying CRS using st_set_crs() to define it, but this assumes you know what it is. There are two arguments to this function: the first is x = objectName, and the second is value = xxx where ‘xxx’ is a valid EPSG code. "],
["cartography-for-epidemiology-i.html", "Week 2 Cartography for Epidemiology I 2.1 w2 Learning objectives 2.2 w2 Additional Resources 2.3 w2 Spatial Thinking in Epidemiology 2.4 w2 Spatial Analysis in Epidemiology", " Week 2 Cartography for Epidemiology I 2.1 w2 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Design a cartographic representation of epidemiologic data that is consistent with best practices in public health data visualization. Apply data processing functions to accomplish foundational data management and preparation for spatial epidemiology (e.g. summarize, aggregate, combine, recode, etc) 2.2 w2 Additional Resources 2.3 w2 Spatial Thinking in Epidemiology Include Vocabulary Concepts and themes as they relate to epidemiology and spatial thinking 2.4 w2 Spatial Analysis in Epidemiology Include additional vocabulary (if necessary) Mostly includes "],
["cartography-for-epidemiology-ii.html", "Week 3 Cartography for Epidemiology II 3.1 w3 Learning objectives 3.2 w3 Additional Resources 3.3 w3 Spatial Thinking in Epidemiology 3.4 w3 Spatial Analysis in Epidemiology", " Week 3 Cartography for Epidemiology II 3.1 w3 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Describe potential threats to privacy and research ethics that arise when population health data is represented geographically Critique spatial epidemiologic literature based on consistency with ethical principles of privacy, avoidance of harm through stigmatization, and balance of benefit and risk 3.2 w3 Additional Resources 3.3 w3 Spatial Thinking in Epidemiology Include Vocabulary Concepts and themes as they relate to epidemiology and spatial thinking 3.4 w3 Spatial Analysis in Epidemiology Include additional vocabulary (if necessary) Mostly includes "],
["disease-mapping-i.html", "Week 4 Disease Mapping I 4.1 w4 Learning objectives 4.2 w4 Additional Resources 4.3 w4 Spatial Thinking in Epidemiology 4.4 w4 Spatial Analysis in Epidemiology", " Week 4 Disease Mapping I 4.1 w4 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Determine and defend appropriate disease mapping strategies consistent with basic epidemiologic concepts (e.g. study design, sampling strategy, measurement error, and systematic bias) Create statistically smoothed, age-adjusted disease maps of epidemiologic parameters including SMR, disease risk or rate, and measures of estimate precision/stability 4.2 w4 Additional Resources 4.3 w4 Spatial Thinking in Epidemiology Content coming soon… 4.4 w4 Spatial Analysis in Epidemiology Content coming soon… "],
["disease-mapping-ii.html", "Week 5 Disease Mapping II 5.1 w5 Learning objectives 5.2 w5 Additional Resources 5.3 w5 Spatial Thinking in Epidemiology 5.4 w5 Spatial Analysis in Epidemiology", " Week 5 Disease Mapping II 5.1 w5 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Compare and contrast the operationalization of distance or contiguity in spatial statistics to sociologic and demographic theories of health relevant processes and relationships in space Apply and justify contrasting definitions of spatial weights matrix in estimation of statistically smoothed disease maps 5.2 w5 Additional Resources 5.3 w5 Spatial Thinking in Epidemiology Content coming soon… 5.4 w5 Spatial Analysis in Epidemiology Content coming soon… "],
["disease-mapping-iii.html", "Week 6 Disease Mapping III 6.1 w6 Learning objectives 6.2 w6 Additional Resources 6.3 w6 Spatial Thinking in Epidemiology 6.4 w6 Spatial Analysis in Epidemiology", " Week 6 Disease Mapping III 6.1 w6 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Discuss the meaning and interpretation of basic functions of spatial point processes including intensity, stationarity, heterogeneity Produce spatially smoothed estimates of epidemiologic parameters using kernel density estimators for point and polygon data 6.2 w6 Additional Resources 6.3 w6 Spatial Thinking in Epidemiology Content coming soon… 6.4 w6 Spatial Analysis in Epidemiology Content coming soon… "],
["disease-mapping-iv.html", "Week 7 Disease Mapping IV 7.1 w7 Learning objectives 7.2 w7 Additional Resources 7.3 w7 Spatial Thinking in Epidemiology 7.4 w7 Spatial Analysis in Epidemiology", " Week 7 Disease Mapping IV 7.1 w7 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Summarize the basic conceptual and statistical benefits of implementing fully Bayesian modeling for epidemiologic disease maping. Estimate geographically-referenced posteriors from spatial Bayesian model 7.2 w7 Additional Resources 7.3 w7 Spatial Thinking in Epidemiology Content coming soon… 7.4 w7 Spatial Analysis in Epidemiology Content coming soon… "],
["spatial-structure-and-clustering-i.html", "Week 8 Spatial Structure and Clustering I 8.1 w8 Learning objectives 8.2 w8 Additional Resources 8.3 w8 Spatial Thinking in Epidemiology 8.4 w8 Spatial Analysis in Epidemiology", " Week 8 Spatial Structure and Clustering I 8.1 w8 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Compare and contrast the statistical, epidemiologic, and policy meaning of geospatial 'clustering' of disease Calculate and visually summarize global and local tests for spatial clustering 8.2 w8 Additional Resources 8.3 w8 Spatial Thinking in Epidemiology Content coming soon… 8.4 w8 Spatial Analysis in Epidemiology Content coming soon… "],
["spatial-structure-and-clustering-ii.html", "Week 9 Spatial Structure and Clustering II 9.1 w9 Learning objectives 9.2 w9 Additional Resources 9.3 w9 Spatial Thinking in Epidemiology 9.4 w9 Spatial Analysis in Epidemiology", " Week 9 Spatial Structure and Clustering II 9.1 w9 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Evaluate statistical estimation of spatial clustering in population health to generate epidemiologic hypotheses Apply spatial scan statistics to epidemiologic data and interpret results 9.2 w9 Additional Resources 9.3 w9 Spatial Thinking in Epidemiology Content coming soon… 9.4 w9 Spatial Analysis in Epidemiology Content coming soon… "],
["spatial-regression-i.html", "Week 10 Spatial Regression I 10.1 w10 Learning objectives 10.2 w10 Additional Resources 10.3 w10 Spatial Thinking in Epidemiology 10.4 w10 Spatial Analysis in Epidemiology", " Week 10 Spatial Regression I 10.1 w10 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Choose and justify spatial analytic methods that aligns with the epidemiologic research question or objective Describe the modfiable areal unit problem and discuss strategies for evaluating bias arising from MAUP Calculate and interpret spatial patterns of residuals from an aspatial multivariable regression model 10.2 w10 Additional Resources 10.3 w10 Spatial Thinking in Epidemiology Content coming soon… 10.4 w10 Spatial Analysis in Epidemiology Content coming soon… "],
["spatial-regression-ii.html", "Week 11 Spatial Regression II 11.1 w11 Learning objectives 11.2 w11 Additional Resources 11.3 w11 Spatial Thinking in Epidemiology 11.4 w11 Spatial Analysis in Epidemiology", " Week 11 Spatial Regression II 11.1 w11 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… Explain and relate spatial non-stationarity to epidemiologic concepts of heterogeneity Use geographically weighted regression to produce and interpret epidemiologic parameters from point and polygon data 11.2 w11 Additional Resources 11.3 w11 Spatial Thinking in Epidemiology Content coming soon… 11.4 w11 Spatial Analysis in Epidemiology Content coming soon… "],
["spatial-regression-iii.html", "Week 12 Spatial Regression III 12.1 w12 Learning objectives 12.2 w12 Additional Resources 12.3 w12 Spatial Thinking in Epidemiology 12.4 w12 Spatial Analysis in Epidemiology", " Week 12 Spatial Regression III 12.1 w12 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… 12.2 w12 Additional Resources 12.3 w12 Spatial Thinking in Epidemiology Content coming soon… 12.4 w12 Spatial Analysis in Epidemiology Content coming soon… "],
["wrap-up-project.html", "Week 13 Wrap-up &amp; Project 13.1 w13 Learning objectives 13.2 w13 Additional Resources 13.3 w13 Spatial Thinking in Epidemiology 13.4 w13 Spatial Analysis in Epidemiology", " Week 13 Wrap-up &amp; Project 13.1 w13 Learning objectives TABLE 1.1: Learning objectives by weekly module After this module you should be able to… 13.2 w13 Additional Resources 13.3 w13 Spatial Thinking in Epidemiology Content coming soon… 13.4 w13 Spatial Analysis in Epidemiology Content coming soon… "],
["tips-for-reproducibility-in-r.html", "Week 14 Tips for reproducibility in R Additional Resources 14.1 Workflows to enhance reproducibility Why R Notebooks? What you need to know Important R Notebook functions Typing text Adding R Code Making tables Workflow Optional functions Customizing your YAML Simple formatting of your Notebook Text formatting Final Note", " Week 14 Tips for reproducibility in R Additional Resources R Markdown Cheatsheet Comprehensive guide to using R Markdown Chapter within the R Markdown guide specific to Notebooks Working with Projects in R R for Data Science - Workflow Projects 14.1 Workflows to enhance reproducibility Because R and RStudio are often used for data preparation, analysis, and reporting, the fundamental importance of reproducibility (making analytic processes transparent, interpretable and repeatable) is built-in through many features. This Appendix introduces several strategies that are important for reproducibility broadly, and also important for the work you do in this course. First, there is a brief introduction to projects in RStudio, and then there is a slightly more in-depth description of a specific file format, rmarkdown and how it can be used to create Notebooks. 14.1.1 Using Projects in R A project in R functions in terms of organization the same way you may use folders on your computer to sort and separate into some logical scheme. In other words, it is a place where you put multiple documents or files that are related to one another. For instance, you might choose to have a single project for each week of this class, and perhaps a separate project for each assignment. In each project directory (folder) you could store the data, the scripts or code, and any outputs (e.g. saved maps or other saved objects) that are specific to that week or assignment. The advantage of creating a formal project in RStudio (rather than just a regular folder, for example), is that RStudio projects have certain benefits for your coding workflow. When you open a project, the working directory (e.g. the root directory or file path where R looks for files when you import) is automatically set to be inside the project folder. This means that if you keep your data inside the project, you will never have to worry about broken links or incorrect file paths that occur because data was moved. Projects remember environmental settings in RStudio, so you may customize something to a specific project and that will be remembered each time you open the project. If you ever work with a version control system such as Github, projects are the natural strategy to contain a repository To create a new project: 1.Look in the upper-right corner of RStudio for the blue-ish R symbol that will likely say ‘Project’. Click the pull-down menu and select New Project 2. You will see the Project Wizard open with three options: + If you have not yet created the folder on your computer that will be your project, choose New Directory + If you already have a folder (e.g. perhaps it is named ‘Week1’), choose Existing Directory + If you are are forking or checking out a repository from Github, GitLab or other system, choose Version Control 3. Navigate to the location you want your new folder to be, or else the location where you existing folder already is 4. Name the project and click Create Project Once the project is created, you can navigate via your finder to that folder. You will notice a new file with extension .Rproj. If you double-click this file, your project will open, including whatever files and settings you have already worked on. Why R Notebooks? For most assignments in this course, at least a portion of the deliverable will be a fully-functional, annotated R Notebook. These notebooks are actually a specific case of rmarkdown which itself is a format for creating reproducible documents with interspersed R code, analytic results and text. For example this eBook, and many other resources in this course are created using rmarkdown or related packages such as bookdown. But as I said, R Notebooks are a specific instance or case of markdown that is incorporated into R Studio and has some nice features for the applied data analyst. Notebooks contain text that explains what is happening, interprets findings, or notes areas in need of further exploration Notebooks contain functional R code that when run places the results inside the Notebook document Notebooks work in an interactive mode. This means that as you are coding and working you can see the results in the document. When you save the Notebook the text, the code and the results are saved! So the reason for using Notebooks is that they provide a means for clear annotation and documentation combined with ready reproducitiblity. Reproducibility means that someone else (or a future you!) could come back and get the same result again. To benefit from the advantages above, I recommend you gain familiarity with the basic (and perhaps the optional) formatting described below. I also recommend you develop a knack for rich annotation and documentation, not just brief (often cryptic) comments that we are all used to writing in SAS and other code! Document what you plan to do. Document what you did. Document what the results means. Document what else needs to be done. What you need to know This file summarizes both important and just a small handful of optional functions for effectively using R Notebooks. The important functions are those necessary to effectively intersperse narrative text and description communicating what you did and what it means, with clear R code and the resulting output. The optional parts are about some simple formatting tools. These are not necessary for your homework (our goal is documentation of analytic process not being ‘pretty’), but you may find them useful. Important R Notebook functions The YAML --- title: &quot;Title of your notebook&quot; author: &quot;Your Name Here&quot; date: &quot;Submission date here&quot; output: html_notebook: number_sections: yes toc: yes toc_float: yes --- When you create a new R Notebook or R Markdown file from within R Studio (e.g. via File &gt; New File &gt; R Notebook), a ‘YAML’ will automatically be created at the top of the script delineated by three dash lines ---. YAML stands for “yet another markup language” and it is a set of instructions about how the finished notebook will look and be structured. You can accept the default YAML structure (of course modifying the title) or copy/paste the YAML from the top of this script. You can also read more online about additional customizations to the YAML, but none are necessary for this course. Typing text The utility of R Notebooks is the ability to more completely document your thinking and your process as you carry out analyses. It is not necessary to be wordy just for the sake of taking up space, but this is an opportunity to clearly delineate goals, steps, data sources, interpretations, etc. You can just start typing text in the script to serve this purpose. Some text formatting functions are summarized later in this document, and in Cheatsheets and online resources linked to elsehwhere. Adding R Code R Notebooks let you write R code within your Markdown file, and then run that code, seeing the results appear right under the code (rather than only in the Console, where they usually appear). There are 2 ways to add a new chunk of R code: Click the green C-Insert button at the top of the editor panel in R Studio. The top option is for R code. Use a keyboard short cut: Mac Command + Shift + I Windows Ctrl + Alt + I Notice these R code chunks are delineated by three back-ticks (sort of like apostrophers)…these back-ticks are typically on the same key as the tilde (~) on the upper left of most keyboards. The space between the sets of 3 back-ticks is where the R code goes and this is called a code chunk. Inside these ‘chunks’ you can type R code just as you would in a regular R script. When you want to run it you can either: Place your cursor on a line and click Ctrl+enter (Windows) or CMD+Return (Mac), or you can click the Run button at th etop of the editor pane in R Studio. To run all of the code within a chunk click the green Run Current Chunk button at the upper-right of the code chunk. Below is some code and the corresponding results. head(mtcars) TABLE 14.1: mpgcyldisphpdratwtqsecvsamgearcarb 21&nbsp;&nbsp;61601103.9&nbsp;2.6216.50144 21&nbsp;&nbsp;61601103.9&nbsp;2.8817&nbsp;&nbsp;0144 22.84108933.852.3218.61141 21.462581103.083.2119.41031 18.783601753.153.4417&nbsp;&nbsp;0032 18.162251052.763.4620.21031 plot(cars) In this way you can iterate through your analytic process…switching between running code, viewing output, documenting in free text. If you want to see what your current work-in-progress looks like in HTML, you can click the Preview button at the top of the panel. This will both save your document, and open the Viewer panel. Making tables While not required, you may want to summarize data in a table in R Markdown. There are packages devoted to creating tables, but you can create a quick-and-dirty table just using keyboard symbols. First start by making a header row. Separate each column name with a ‘pipe’ symbol, | Put a continuous line of dashes (-----) under each column name, separating columns with pipe symbole (|) Now type text corresponding to each row and column. Separate columns with pipe (|) and separate rows by carriage return/Enter So the following text typed directly into the Markdown file (e.g. not inside a code chunk): Column 1 | Column 2 | Column 3 ----------|----------|----------- Text 1 | Text 2 | Text 3 Next line | Next line 2 | Next line 3 Will produce the following output: Column 1 Column 2 Column 3 Text 1 Text 2 Text 3 Next line Next line 2 Next line 3 Workflow The benefit of Notebooks (slightly different from regular Markdown) is that you can work interactively with your code, seeing results immediately just as you would with a regular script. In contrast a ‘regular’ Markdown file doesn’t run the code until you click ‘Knit’. Here is what I recommend for workflow: Name your script and save it to the desired location. Wherever you save it will become the default Working Directory for any code run from within the Notebook. Carry out your analysis, inserting code chunks, running them, and documenting them as you go. As a final check of reproducibility (the assurance that your code is self-contained and not dependent on steps you did outside the script) I recommend you always end by clicking the RUN button at the top of the panel. Specifically, choose Restart R and Run all Chunks. If there is an error when you did this, then something is missing in your code. Optional functions The list of formatting functions is long. I include only a couple I find useful (but not mandatory) here: Customizing your YAML While the default YAML is perfectly fine, the YAML at the top of this script includes a few added functions including: Specify a table of contents - this only works if you use headers Specify section numbering Specify that the table of contents should be ‘floating’ which means that in html it is visible even when you scroll. For PDF rendering, ‘float’ is not an option. Simple formatting of your Notebook Often it is helpful use headers to separate tasks or steps in your code. You can easily create headers using the hastag/pound sign #. Specifically… # at the beginning of the line denotes a top-level (level-1) header that will be large and bold. ## at the beginning of the line denotes level-2 header ### unsurprisingly is a level-3 header! Make sure there is a space between the # and the text Always leave a blank line (return/enter) between the header text and the ‘regular’ text. You can also make numbered or bulleted lists if that is helpful. A line that begins with either an asterisk (*) or a number will begin a bulleted or numbered list. Headers are populated into the table of contents, if specified. Text formatting The R Markdown Cheatsheets have lots of examples of formatting. Three things that I use more frequently are bold, italics, and numbered or bulleted lists. Key stroke Result *italics* italics **bold** bold Numbered lists start with number, and each line must end with 2 space (or have blank line between). Instead of numbers you can use letters Bulleted lists can be initiated with an asterisk or +, and also must have 2 spaces (or blank carriage return) at end of each item. Final Note Remember that a final step when you think you are done with a project, is to Click Restart R and Run all Chunks, and then save/preview the Notebook after doing this to be sure it is what you expect. "],
["sf-overview.html", "Week 15 Tips for working with sf data class 15.1 st_set_geom() 15.2 st_as_sf()", " Week 15 Tips for working with sf data class 15.1 st_set_geom() There is a feature of sf class data in that the special column containing the geometry information (often labeled geom) is different from other variables. Specifically it is sticky. Stickiness in a variable means that as you manipulate an sf data object, the geom column almost always sticks to the rest of the data even when you try to remove it. Imagine what would happen in a regular data.frame if you typed this code into the console mvc[1, 1:2]. Typically that kind of numerical indexing would cause R to return row 1 for columns 1 to 2. However, when we try this in R with an sf object this is what happens: library(sf) library(tidyverse) mvc &lt;- st_read(&#39;../DATA/GA_MVC/ga_mvc.gpkg&#39;) ## Reading layer `ga_mvc&#39; from data source `C:\\Users\\mkram02\\Box\\SpatialEpi-2020\\DATA\\GA_MVC\\ga_mvc.gpkg&#39; using driver `GPKG&#39; ## Simple feature collection with 159 features and 17 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -85.60516 ymin: 30.35785 xmax: -80.83973 ymax: 35.00066 ## geographic CRS: WGS 84 mvc[1, 1:2] TABLE 15.1: GEOIDNAMEgeom 13001Appling County, Georgialist(list(c(-82.550691, -82.547445, -82.544961, -82.520158, -82.520251, -82.51888, -82.515199, -82.497531, -82.458682, -82.431362, -82.431449, -82.430452, -82.431531, -82.430151, -82.42475, -82.42315, -82.417848, -82.412147, -82.409746, -82.408046, -82.407546, -82.408445, -82.407245, -82.405545, -82.403544, -82.402044, -82.401943, -82.404143, -82.404242, -82.402242, -82.400142, -82.397941, -82.39324, -82.392039, -82.391139, -82.388839, -82.387138, -82.385837, -82.385437, -82.385737, -82.389136, -82.387635, -82.384456, -82.383151, -82.380919, -82.379533, -82.372096, -82.371549, -82.371749, -82.372549, -82.374844, -82.377849, -82.378749, -82.378049, -82.377149, -82.37242, -82.36503, -82.362334, -82.359314, -82.358009, -82.354891, -82.353063, -82.347596, -82.337325, -82.33284, -82.329672, -82.323277, -82.314482, -82.313138, -82.30887, -82.307997, -82.307755, -82.310688, -82.312161, -82.315331, -82.316096, -82.315085, -82.311542, -82.309508, -82.308608, -82.306539, -82.306015, -82.304517, -82.303405, -82.302795, -82.301112, -82.298094, -82.295326, -82.293344, -82.29137, -82.288309, -82.286418, -82.283269, -82.279546, -82.277617, -82.271493, -82.270334, -82.268199, -82.267161, -82.263156, -82.261386, -82.259705, -82.256037, -82.255343, -82.254586, -82.251187, -82.249035, -82.248458, -82.248007, -82.24681, -82.245312, -82.242915, -82.241717, -82.238223, -82.236925, -82.233129, -82.231832, -82.231637, -82.22964, -82.225042, -82.223645, -82.221145, -82.219345, -82.216145, -82.210444, -82.206644, -82.204844, -82.201344, -82.199144, -82.195244, -82.193144, -82.191344, -82.190744, -82.192244, -82.191344, -82.187044, -82.182044, -82.165843, -82.163243, -82.159243, -82.155443, -82.151543, -82.148643, -82.145143, -82.142843, -82.140443, -82.138043, -82.135743, -82.132943, -82.131743, -82.131143, -82.131543, -82.134153, -82.136486, -82.135543, -82.133443, -82.129443, -82.125543, -82.121042, -82.119542, -82.117442, -82.115442, -82.116242, -82.116042, -82.116842, -82.116642, -82.114914, -82.113858, -82.111818, -82.110966, -82.110242, -82.110542, -82.112042, -82.110742, -82.108542, -82.107842, -82.108042, -82.109442, -82.109842, -82.109625, -82.11066, -82.116413, -82.116627, -82.115478, -82.113263, -82.109529, -82.107235, -82.104401, -82.101288, -82.098322, -82.095586, -82.094244, -82.092008, -82.088864, -82.084506, -82.082262, -82.081309, -82.072376, -82.066071, -82.061137, -82.058628, -82.056356, -82.048582, -82.048775, -82.052842, -82.056121, -82.058747, -82.061306, -82.064981, -82.068098, -82.070077, -82.072024, -82.072053, -82.074253, -82.077084, -82.079896, -82.080119, -82.082037, -82.082647, -82.083848, -82.086099, -82.087398, -82.087242, -82.087247, -82.10654, -82.106627, -82.107028, -82.089338, -82.089437, -82.133013, -82.133077, -82.132957, -82.133086, -82.133135, -82.133308, -82.133044, -82.133396, -82.133717, -82.133465, -82.13361, -82.133339, -82.133144, -82.132942, -82.147948, -82.147622, -82.147926, -82.132867, -82.13299, -82.132794, -82.135122, -82.141021, -82.144131, -82.151123, -82.155075, -82.159729, -82.163864, -82.181175, -82.194997, -82.198207, -82.201971, -82.208306, -82.210344, -82.216173, -82.219044, -82.221496, -82.225145, -82.226585, -82.23024, -82.231323, -82.231994, -82.236604, -82.240772, -82.244255, -82.247484, -82.249401, -82.251627, -82.257327, -82.2628, -82.266227, -82.269732, -82.274078, -82.280136, -82.2835, -82.28701, -82.293174, -82.296225, -82.299904, -82.30416, -82.307186, -82.310237, -82.311516, -82.312438, -82.315736, -82.318077, -82.323302, -82.326103, -82.327446, -82.334882, -82.339264, -82.343666, -82.349571, -82.352242, -82.355185, -82.359787, -82.364243, -82.367041, -82.368835, -82.371406, -82.375099, -82.37706, -82.380279, -82.385063, -82.386543, -82.387993, -82.390144, -82.397249, -82.402222, -82.405373, -82.407509, -82.408432, -82.409945, -82.412837, -82.414078, -82.413633, -82.416448, -82.418602, -82.42319, -82.429245, -82.431855, -82.43573, -82.43782, -82.439522, -82.439888, -82.442413, -82.443909, -82.446671, -82.448021, -82.45089, -82.451584, -82.451187, -82.451851, -82.453484, -82.460335, -82.461639, -82.462707, -82.464195, -82.466591, -82.467171, -82.469826, -82.471031, -82.472725, -82.473839, -82.476639, -82.4776, -82.476753, -82.477364, -82.478607, -82.480202, -82.482445, -82.482819, -82.486778, -82.487831, -82.487946, -82.489555, -82.492889, -82.493515, -82.495866, -82.52142, -82.520895, -82.520567, -82.550714, -82.550691, 31.749112, 31.749126, 31.74896, 31.74919, 31.838388, 31.838393, 31.838377, 31.838329, 31.838104, 31.837993, 31.844374, 31.938742, 31.966182, 31.965991, 31.96419, 31.96339, 31.959589, 31.957189, 31.956788, 31.955689, 31.953289, 31.951289, 31.949488, 31.949188, 31.949788, 31.951688, 31.953687, 31.956487, 31.958487, 31.960286, 31.960186, 31.959086, 31.955585, 31.951084, 31.949584, 31.948384, 31.948184, 31.949284, 31.950583, 31.952483, 31.957082, 31.959682, 31.960022, 31.959458, 31.956011, 31.954977, 31.953976, 31.953493, 31.951793, 31.950793, 31.949693, 31.946893, 31.945093, 31.942093, 31.941693, 31.941456, 31.943641, 31.94369, 31.941736, 31.939686, 31.939561, 31.939022, 31.93856, 31.937165, 31.935602, 31.934777, 31.934495, 31.930783, 31.930912, 31.932698, 31.935067, 31.937485, 31.938461, 31.939392, 31.942805, 31.945225, 31.946611, 31.947515, 31.946541, 31.945552, 31.94161, 31.938688, 31.93792, 31.93608, 31.933901, 31.93327, 31.933989, 31.938865, 31.94084, 31.942483, 31.942784, 31.942443, 31.94076, 31.937978, 31.937071, 31.936272, 31.935871, 31.93279, 31.928621, 31.927953, 31.928995, 31.932406, 31.932089, 31.931417, 31.927471, 31.923515, 31.921861, 31.920673, 31.9173, 31.9151, 31.914599, 31.9151, 31.916, 31.9201, 31.9205, 31.919199, 31.917099, 31.913399, 31.911798, 31.913074, 31.913398, 31.912798, 31.909698, 31.909898, 31.909498, 31.910298, 31.909298, 31.906298, 31.906099, 31.906799, 31.906799, 31.905899, 31.903499, 31.901799, 31.900499, 31.901199, 31.901199, 31.903599, 31.903899, 31.903899, 31.903099, 31.901, 31.8988, 31.8981, 31.8993, 31.9052, 31.9071, 31.9081, 31.9074, 31.9058, 31.9038, 31.9017, 31.897408, 31.894548, 31.8917, 31.8904, 31.8904, 31.8897, 31.886301, 31.886001, 31.886601, 31.888801, 31.891901, 31.8952, 31.8974, 31.8997, 31.900982, 31.901188, 31.900487, 31.899634, 31.897901, 31.895501, 31.891301, 31.889601, 31.885701, 31.883701, 31.882401, 31.880001, 31.878001, 31.870308, 31.869177, 31.866346, 31.864283, 31.862846, 31.862981, 31.863934, 31.863797, 31.861782, 31.86023, 31.858385, 31.854908, 31.853645, 31.852208, 31.850759, 31.847941, 31.843444, 31.842806, 31.840437, 31.838215, 31.834523, 31.832357, 31.830948, 31.827075, 31.826791, 31.827801, 31.82785, 31.827445, 31.825471, 31.825591, 31.827441, 31.825956, 31.825906, 31.82738, 31.829296, 31.829068, 31.827619, 31.825966, 31.824944, 31.823474, 31.822825, 31.82311, 31.821622, 31.821003, 31.799509, 31.799798, 31.797484, 31.786101, 31.785903, 31.773434, 31.773404, 31.761886, 31.758148, 31.737982, 31.706003, 31.701698, 31.697016, 31.653169, 31.651706, 31.630796, 31.618551, 31.617651, 31.58648, 31.56931, 31.569123, 31.561434, 31.557308, 31.557514, 31.545909, 31.471262, 31.470407, 31.469426, 31.469253, 31.469924, 31.471116, 31.473505, 31.475912, 31.489265, 31.501904, 31.505171, 31.507567, 31.510566, 31.511809, 31.516237, 31.518709, 31.521563, 31.52761, 31.530699, 31.538139, 31.543049, 31.544741, 31.551307, 31.555677, 31.557953, 31.559532, 31.559856, 31.559373, 31.560667, 31.563363, 31.566832, 31.571882, 31.574191, 31.576214, 31.577534, 31.579334, 31.582848, 31.584245, 31.585463, 31.586525, 31.587705, 31.589528, 31.591002, 31.591842, 31.592757, 31.594286, 31.599538, 31.601891, 31.602228, 31.606913, 31.610743, 31.615719, 31.621344, 31.622572, 31.62282, 31.625238, 31.628664, 31.630212, 31.631031, 31.631247, 31.630497, 31.631269, 31.633789, 31.634325, 31.634914, 31.636969, 31.638048, 31.637947, 31.639761, 31.638998, 31.6392, 31.641005, 31.641984, 31.642212, 31.643061, 31.644518, 31.645639, 31.648174, 31.651362, 31.656731, 31.658139, 31.660656, 31.661352, 31.660812, 31.659809, 31.658842, 31.659376, 31.659273, 31.660301, 31.661381, 31.662766, 31.664581, 31.666393, 31.668243, 31.670374, 31.671827, 31.6747, 31.676493, 31.676914, 31.6779, 31.67857, 31.679762, 31.680157, 31.683493, 31.684902, 31.687771, 31.68894, 31.690025, 31.691519, 31.691942, 31.694094, 31.695879, 31.697725, 31.699923, 31.701645, 31.702145, 31.7062, 31.708311, 31.710588, 31.710796, 31.732011, 31.736201, 31.736334, 31.749112))) Notice that we did get the first row, and the first and second column but we also got the geom column even though we didn’t request it. This stickiness is generally desirable, because it is so important to keep geographic/geometry data connected to attribute data. However there are times when we want to drop that information. There are several ways to do so, but here is the most explicit way: mvc2 &lt;- st_set_geometry(mvc, NULL) This literally erases or sets to NULL the geometry column. It cannot be retrieved without going back to the original data. # look at the class of the original and the modified object class(mvc) ## [1] &quot;sf&quot; &quot;data.frame&quot; class(mvc2) ## [1] &quot;data.frame&quot; # look at the first row and 1-2nd column after NULLing geom mvc2[1, 1:2] TABLE 15.2: GEOIDNAME 13001Appling County, Georgia 15.2 st_as_sf() There are also times when, inextricably, your data set that seems like an sf object gets rejected by a function as not having geometry information or not being sf. Sometimes data manipulation steps strip away the sf data class even though the geom column still exists. When this happens you can reinstate the class status by calling st_as_sf(). Essentially this is a formal way for declaring an object to be sf by explicitly defining the spatial component. "],
["dplyr.html", "Week 16 Tips for using dplyr 16.1 mutate() 16.2 filter() 16.3 arrange() 16.4 %&gt;% Pipe operator 16.5 group_by() and summarise() 16.6 Reshaping (transposing) data", " Week 16 Tips for using dplyr As is the case in many software packages, there is always more than one way to get something done in R! Base-R tools can accomplish all kinds of tasks, but sometimes they are cumbersome and inefficient. Because most of our use of R as epidemiologists is focused on the tools of data science, you might find the diverse and continually evolving tidyverse a great toolbox to explore. Originated by Hadley Wickham (founder of RSTudio), the packages constituting the tidyverse are now contributed by lots of different people. What they have in common is an interest in handling data in tidy ways. R for Data Science is an authoritative guide to tidy data, and many of the tools constituting the tidyverse including ggplot2, dplyr and more. This appendix is a very brief introduction to the dplyr package which is a set of data manipulation functions. In other words it is the epidemiologists’ go-to package for data manipulation, recoding, and preparation in R. There are two high-level observations about how we will use dplyr this semester: dplyr functions can be thought of as verbs. That means each one is a tool to act on your data, producing a change. So your question is “what do I want to change?” Functions in dplyr (and in many other parts of the tidyverse for that matter) can be stand alone code. Or alternatively they can be chained together in a sequence. This chaining (called piping because the tool to connect or chain steps is called a pipe and looks like this: %&gt;%) can make your code both easier for humans to read, and also helps run a sequence of steps more efficiently. For the examples below, I will use the Georgia motor vehicle crash mortality dataset where the unit of observation (e.g. the content of one row of data) is a Georgia county, and the columns are variable names. This dataset is also explicitly spatial meaning that it includes geography information regarding the boundaries of each county, contained with the geom column, as is typical for sf class data in R. Here is what the first few rows of the dataset looks like (minus the geom column): ## Reading layer `ga_mvc&#39; from data source `C:\\Users\\mkram02\\Box\\SpatialEpi-2020\\DATA\\GA_MVC\\ga_mvc.gpkg&#39; using driver `GPKG&#39; ## Simple feature collection with 159 features and 17 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -85.60516 ymin: 30.35785 xmax: -80.83973 ymax: 35.00066 ## geographic CRS: WGS 84 TABLE 16.1: GEOIDNAMEvariableestimateCountyMVCDEATHS_05MVCDEATHS_14MVCDEATH_17TPOP_05TPOP_14TPOP_17NCHS_RURAL_CODE_2013nchs_coderuralMVCRATE_05MVCRATE_14MVCRATE_17 13001Appling County, GeorgiaB00001_0011.5e+03&nbsp;Appling County44101.78e+041.85e+041.85e+046Non-coreRural22.521.654&nbsp;&nbsp; 13003Atkinson County, GeorgiaB00001_001875&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Atkinson County5138.1e+03&nbsp;8.22e+038.34e+036Non-coreRural61.812.236&nbsp;&nbsp; 13005Bacon County, GeorgiaB00001_001945&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bacon County7501.06e+041.13e+041.13e+046Non-coreRural66.344.30&nbsp;&nbsp; 13007Baker County, GeorgiaB00001_001390&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Baker County1113.97e+033.26e+033.2e+03&nbsp;4Small metronon-Rural25.230.731.2 13009Baldwin County, GeorgiaB00001_0012.94e+03Baldwin County68134.63e+044.59e+044.49e+045Micropolitannon-Rural13&nbsp;&nbsp;17.428.9 13011Banks County, GeorgiaB00001_0011.77e+03Banks County4861.67e+041.83e+041.86e+046Non-coreRural24&nbsp;&nbsp;43.732.2 ## select() The first verb of dplyr is called select() and it is useful when you want to remove or select specific columns/variables. For instance, as mentioned this dataset has 17 attribute columns plus the geom column. But perhaps we only need three variables, and we decided it would be easier to exclude the unneeded variables? Then we can select() those we want (or inversely we can select those we don’t want). There are three useful tips on using select() with spatial data: To select variables to keep simply list them (e.g. select(data, var1, var2, var3)) If it is easier to only omit specific variables (e.g .perhaps you have 100 variables and you only want to drop 3), place a negative sign before the name (e.g. select(data, -var5, -var6)). Finally, something specific to working with sf spatial data is that the geometry column (typically named geom or geometry) is sticky. That means that it’s hard to get rid of it. That’s actually a good thing. You usually want the geometry to stick with the attribute data. But occasionally you might want to convert you spatial sf data object into an aspatial data.frame. To do this you must first set the geometry to be null like this: aspatial.df &lt;- st_set_geometry(spatial.df, NULL). See additional info here. Let’s do that with the motor vehicle crash data. # First we read in the dataset, which is stored as a geopackage mvc &lt;- st_read(&#39;GA_MVC/ga_mvc.gpkg&#39;) # Look at column names names(mvc) # FOr this example we do not want the geom column because it is too big to view mvc2 &lt;- st_set_geometry(mvc, NULL) # Creating a new object with only 4 attributes mvc2 &lt;- select(mvc2, GEOID, NAME, rural, MVCRATE_05, MVCRATE_17) # look at column names names(mvc2) ## [1] &quot;GEOID&quot; &quot;NAME&quot; &quot;variable&quot; ## [4] &quot;estimate&quot; &quot;County&quot; &quot;MVCDEATHS_05&quot; ## [7] &quot;MVCDEATHS_14&quot; &quot;MVCDEATH_17&quot; &quot;TPOP_05&quot; ## [10] &quot;TPOP_14&quot; &quot;TPOP_17&quot; &quot;NCHS_RURAL_CODE_2013&quot; ## [13] &quot;nchs_code&quot; &quot;rural&quot; &quot;MVCRATE_05&quot; ## [16] &quot;MVCRATE_14&quot; &quot;MVCRATE_17&quot; &quot;geom&quot; ## [1] &quot;GEOID&quot; &quot;NAME&quot; &quot;rural&quot; &quot;MVCRATE_05&quot; &quot;MVCRATE_17&quot; 16.1 mutate() ANother frequently needed verb is called mutate() and as you might guess it changes your data. Specifically mutate() is a function for creating a new variable, possibly as a recode of an older variable. The mvc data object has 159 rows (one each for n=159 counties). Let’s imagine that we wanted to create a map that illustrated the magnitude of the change in the rate of death from motor vehicle crashes between 2005 and 2017. To do this we want to create two new variables that we will name delta_mr_abs (the absolute difference in rates) and delta_mr_rel (the relative diference in rates). # Now we make a new object called mvc2 mvc3 &lt;- mutate(mvc2, delta_mr_abs = MVCRATE_05 - MVCRATE_17, delta_mr_rel = MVCRATE_05 / MVCRATE_17) If you look at the help documentation for mutate() you’ll see that the first argument is the input dataset, in this case mvc. Then anywhere from one to a zillion different ‘recode’ steps can be included inside the parentheses, each separated by a comma. Above, we created two new variables, one representing the absolute and the other representing the relative difference in rates between the two years. We can look at the first few rows of selected columns to see the new variables: head(mvc3) TABLE 16.2: GEOIDNAMEruralMVCRATE_05MVCRATE_17delta_mr_absdelta_mr_rel 13001Appling County, GeorgiaRural22.554&nbsp;&nbsp;-31.5&nbsp;0.417 13003Atkinson County, GeorgiaRural61.836&nbsp;&nbsp;25.8&nbsp;1.72&nbsp; 13005Bacon County, GeorgiaRural66.30&nbsp;&nbsp;66.3&nbsp;Inf&nbsp;&nbsp;&nbsp;&nbsp; 13007Baker County, Georgianon-Rural25.231.2-6.040.807 13009Baldwin County, Georgianon-Rural13&nbsp;&nbsp;28.9-16&nbsp;&nbsp;&nbsp;0.448 13011Banks County, GeorgiaRural24&nbsp;&nbsp;32.2-8.220.745 16.2 filter() While select() above was about choosing which columns you keep or drop, filter() is about choosing which rows you keep or drop. If you are more familiar with SAS, filter() does what an if or where statement might do. Imagine we wanted to only map the urban counties, and omit the rural counties. We could do this be defining a filtering rule. A rule is any logical statement (e.g. a relationship that can be tested in the data and return TRUE or FALSE). Here we create a new dataset, mvc4 that is created from mvc3 but is restricted only to non-Rural counties: mvc4 &lt;- filter(mvc3, rural == &#39;non-Rural&#39;) dim(mvc3) # dimensions (rows, columns) of the mvc3 object ## [1] 159 7 dim(mvc4) # dimensions (rows, columns) of the restricted mvc4 object ## [1] 102 7 As you can see the original object (mvc3) had 159 rows, while the filtered object (mvc4) has only 102, reflecting the number of non-Rural counties in Georgia. 16.3 arrange() Occasionally you might want to sort a dataset, perhaps to find the lowest or highest values of a variable, or to group like values together. Sorting with dplyr uses the arrange() verb. By default, data is arranged in ascending order (either numberical or alphabetical for character variables), but you can also choose descending order as below: mvc5 &lt;- arrange(mvc3, desc(MVCRATE_17)) head(mvc5) TABLE 16.3: GEOIDNAMEruralMVCRATE_05MVCRATE_17delta_mr_absdelta_mr_rel 13307Webster County, GeorgiaRural38.8115&nbsp;&nbsp;-76.30.337 13269Taylor County, GeorgiaRural22.673.7-51.10.306 13165Jenkins County, GeorgiaRural47&nbsp;&nbsp;57&nbsp;&nbsp;-10&nbsp;&nbsp;0.824 13001Appling County, GeorgiaRural22.554&nbsp;&nbsp;-31.50.417 13087Decatur County, Georgianon-Rural18.252.4-34.20.347 13191McIntosh County, Georgianon-Rural16.149.6-33.50.325 16.4 %&gt;% Pipe operator Everything we’ve done up until now has been one step at a time, and we created five different datasets to avoid overwriting our original. But one source of coding efficiency in R comes from the careful chaining or piping together of multiple steps. While every verb above required an input dataset as the first argument, when we chain steps, the functions take the output of the previous step as the input for the current step. For example this code chunk does everything we did above in one step: mvc6 &lt;- mvc %&gt;% st_set_geometry(NULL) %&gt;% # remove geom column select(GEOID, NAME, rural, MVCRATE_05, MVCRATE_17) %&gt;%# select target variables mutate(delta_mr_abs = MVCRATE_05 - MVCRATE_17, # recode variables delta_mr_rel = MVCRATE_05 / MVCRATE_17) %&gt;% filter(rural == &#39;non-Rural&#39;) %&gt;% # filter (restrict) rows arrange(desc(MVCRATE_17)) # sort by MVCRATE_17 dim(mvc6) ## [1] 102 7 head(mvc6) TABLE 16.4: GEOIDNAMEruralMVCRATE_05MVCRATE_17delta_mr_absdelta_mr_rel 13087Decatur County, Georgianon-Rural18.252.4-34.2&nbsp;0.347 13191McIntosh County, Georgianon-Rural16.149.6-33.5&nbsp;0.325 13033Burke County, Georgianon-Rural34.940&nbsp;&nbsp;-5.090.873 13189McDuffie County, Georgianon-Rural18.737.2-18.5&nbsp;0.502 13055Chattooga County, Georgianon-Rural11.836.3-24.6&nbsp;0.324 13227Pickens County, Georgianon-Rural29.334.8-5.490.842 In practice, it takes some experience to write a whole chain of steps that do what you want. I often go iteratively, adding one step at a time and checking that each step is doing what I expected. 16.5 group_by() and summarise() A dplyt verb that can be incredibly important for spatial epidemiology is the combination of group_by() with summarise(). These two are used to aggregate and summarize data. For instance if you had data arranged with individual persons as the unit of analysis (e.g. 1 person = 1 row of data), but you wanted to aggregate them so you got counts per census tract, you could use group_by() to arrange the rows into groups defined by census tract, and then use summarise() to do some calculation (e.g. count, mean, sum, etc) separately for each group. An important feature of sf data objects when operated on by dplyr verbs, is that there is built in functionality to handle geography/geometry data. So for instance, imagine we wanted to create a map where we aggregated all of the rural counties and separately all of the non-rural counties. mvc7 &lt;- mvc %&gt;% group_by(rural) %&gt;% summarise(avg_mr_17 = mean(MVCRATE_17)) ## `summarise()` ungrouping output (override with `.groups` argument) mvc7 %&gt;% st_set_geometry(NULL) TABLE 16.5: ruralavg_mr_17 non-Rural18.8 Rural29.2 As you can see (and as you might have predicted), the aggregation changed our dataset from 159 rows to 2 rows: one row for rural and one for non-rural. Let’s see what it did to the spatial data by first mapping the original data, and then mapping the aggregated data. Read more about qtm()) and other tmap functions. # Using the qtm() function from tmap to create map of original data m1 &lt;- qtm(mvc, &#39;MVCRATE_17&#39;) # Using the qtm() function from tmap package to create a map m2 &lt;- qtm(mvc7, &#39;avg_mr_17&#39;) tmap_arrange(m1, m2) 16.6 Reshaping (transposing) data There are numerous other intermediate to advanced data manipulation options available in dplyr and the tidyverse, but most are outside of the scope of this course. One final verb that represents a more sophisticated kind of data change, is however useful in preparing spatial data. That is the tools to transpose or reshape a rectangular dataset from wide to long or vice versa. Transposing is useful when, for example, you have a column with a disease rate for each of several years (these data are wide), but you want a dataset where a single column contains the rate and a separate column indicates the year (these data are long). This article introduces the notion of pivoting data; you can also review this section in R for Data Science Two related verbs help pivot tidy data one direction or the other: 16.6.1 pivot_longer() for going from wide to long Reviewing the article linked in the previous paragraph (or searching the help documentation) will give you more detail. But as an example we will look at how to take our current mvc dataset, which contains the motor vehicle crash mortality rate for each county from three different years (2005, 2014, 2017) as separate columns (e.g. it is wide): # this code shows the first 6 rows (the head) of the relevant variables mvc %&gt;% st_set_geometry(NULL) %&gt;% select(GEOID, NAME, MVCRATE_05, MVCRATE_14, MVCRATE_17) %&gt;% head() TABLE 16.6: GEOIDNAMEMVCRATE_05MVCRATE_14MVCRATE_17 13001Appling County, Georgia22.521.654&nbsp;&nbsp; 13003Atkinson County, Georgia61.812.236&nbsp;&nbsp; 13005Bacon County, Georgia66.344.30&nbsp;&nbsp; 13007Baker County, Georgia25.230.731.2 13009Baldwin County, Georgia13&nbsp;&nbsp;17.428.9 13011Banks County, Georgia24&nbsp;&nbsp;43.732.2 For mapping a time-series it is often beneficial to have the data long, which is to say we want the data with a single column for mvc_rate and a separate column for year, and then we can choose to create a map for each subset (defined by year) of the data. mvc_long &lt;- mvc %&gt;% select(GEOID, NAME, MVCRATE_05, MVCRATE_14, MVCRATE_17) %&gt;% as_tibble() %&gt;% pivot_longer(cols = starts_with(&quot;MVCRATE&quot;), names_to = c(&quot;.value&quot;, &quot;year&quot;), values_to = &quot;mvc_rate&quot;, names_sep = &quot;_&quot;) %&gt;% mutate(year = 2000 + as.numeric(year)) %&gt;% st_as_sf() First let’s look at the results, and then I’ll walk through each of the steps in the code chunk above: mvc_long %&gt;% st_set_geometry(NULL) %&gt;% head() TABLE 16.7: GEOIDNAMEyearMVCRATE 13001Appling County, Georgia2e+03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;22.5 13001Appling County, Georgia2.01e+0321.6 13001Appling County, Georgia2.02e+0354&nbsp;&nbsp; 13003Atkinson County, Georgia2e+03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;61.8 13003Atkinson County, Georgia2.01e+0312.2 13003Atkinson County, Georgia2.02e+0336&nbsp;&nbsp; As you can see, we now have 3 rows for Appling County (GEOID 13001): one for each of the three years, with different MVCRATE in each. That is a long dataset. So how did that code work? Here is a step-by-step of what that code chunk did: The first step was to create a new object, mvc_long that will be the outcome of all of the steps piped together. The input to the pipe is the original dataset, mvc. The use of as_tibble() is a current work around to an annoying ‘feature’ of the pivot_* functions and that is that they don’t play well with sf data classes. So when we use as_tibble() we are essentially removing the class designation, (making it a tibble which is a tidy object); importantly this is different from st_set_geometry(NULL) which actually omits the geometry column (e.g. see additional detail here). I used select() to pull out only the variables of interest, although you could leave other variables if desired. pivot_longer() can be called in several ways. The way I call it here, I first specified which columns to pivot by defining the cols = argument to be all variables that start with the phrase MVCRATE. starts_with() is another utility function in dplyr. So this step told R that the columns I wanted changed were the three called MVCRATE_05, MVCRATE_12 and MVCRATE_17 The names_to = argument defines the column name in the new dataset that will delineate between the three variables (e.g. MVCRATE_05, etc). In our case we wanted the value to be the year not the word MVCRATE_12. To accomplish this we had to do some extra work: First, note that we used the option names_sep = '_'. That is another utility function that says we want to break a string into parts wherever a designated separated (e.g. the underscore, _) occurs. So this will take the column name MVCRATE_05 and break it at the underscore to return two parts: MVCRATE and 05. Because breaking it into two would produce two answers, we had to make two variable names in the names_to = to hold them. Thus names_to = c(\".value\", \"year\"). In other words the column labeled .variable will hold the value MVCRATE and the column year will hold the value 05 '.value' is actually a special value in this instance. It is a way of designating that the first part was essentially junk. As such it will automatically be discarded. values_to = 'mvcrate'. This is where we define the name in the new dataset that will hold the actual value (e.g. the MVC mortality rate itself.) The mutate() step is just a way to take the year fragment (e.g. 05, 12, 17) and make it into calendar years by first making it numeric, and then simply adding 2000. The final step, st_as_sf() is because all of the manipulations above actually removed the objects designation as class sf. Importantly, it DID NOT remove the geom column, but the object is not recognized (e.g. by tmap) as a spatial object. So st_as_sf() simply declares that it is in fact sf. The best way to wrap your head around this is to start trying to reshape or transpose data you have on hand. You may need to look for additional help or examples online, but with time it will become more intuitive. To see why you might have gone through all of that work, we will use the tm_facets() (read more abouttmap_facets() here). tm_shape(mvc_long) + tm_fill(&#39;MVCRATE&#39;) + tm_borders() + tm_facets(by = &#39;year&#39;) 16.6.2 pivot_wider() Of course it is also possible to go the other way, from long to wide. This often is easier. Here is some code to return to our original shape: mvc_wide &lt;- mvc_long %&gt;% as_tibble() %&gt;% mutate(my_var = paste0(&#39;MVCRATE &#39;, year)) %&gt;% select(-year) %&gt;% pivot_wider(names_from = my_var, values_from = MVCRATE) %&gt;% st_as_sf() Take a look at the output: mvc_wide %&gt;% st_set_geometry(NULL) %&gt;% head() TABLE 16.8: GEOIDNAMEMVCRATE 2005MVCRATE 2014MVCRATE 2017 13001Appling County, Georgia22.521.654&nbsp;&nbsp; 13003Atkinson County, Georgia61.812.236&nbsp;&nbsp; 13005Bacon County, Georgia66.344.30&nbsp;&nbsp; 13007Baker County, Georgia25.230.731.2 13009Baldwin County, Georgia13&nbsp;&nbsp;17.428.9 13011Banks County, Georgia24&nbsp;&nbsp;43.732.2 It appears that we have returned to 1 row per county. So what were all of those steps? Once again, we start by removing the class designation sf but calling as_tibble() mutate() is called to re-create a variable that will become the column names. We no longer need the old year variable so I omit it with select(-year) Finally the pivot_wider() call has arguments defining which current variable contains the informatino that will be the new column name (names_from =) and which current variable contains the information that will population the cells within that column (values_from =). "],
["intro-tmap.html", "Week 17 Tips for using tmap 17.1 tmap mode 17.2 Quick maps: qtm() 17.3 Building maps with tmap 17.4 Making small-multiple maps 17.5 Small multiples with tmap_arrange() 17.6 Summarizing small multiples 17.7 Saving maps", " Week 17 Tips for using tmap Base-R has capable data visualization and plotting capabilities, but these fall short when doing anything but the most simple maps with spatial data. Many other packages including sp and ggplot2 also have functionality specifically optimized for the data visualization needs of the spatial epidemiologist. We will have brief introductions to these and other packages. But for this semester the workhorse mapping/cartography tool will be the tmap (thematic mapping) package. This package builds on the grammar of graphics logic built into ggplot2 where data visualizations are conceived of as a series of layers of information (e.g. axes, plot space, data points, lines, fill, legends, titles, etc) systematically stacked one on top of another. With tmap we start with a spatial object (e.g. a data object of either sf or sp class) and build a visualization by similarly combining or adding together sequential layers. Once again, we will use data from the motor vehicle crash mortality dataset of Georgia counties (a vector polygon spatial data file), along with information about highways (vector line data file) and trama centers (vector point data). First load the package, tmap and browse the help index: # load the tmap and sf packages library(tmap) library(sf) help(&#39;tmap&#39;) After seeing the range of functions within tmap, we will import three datasets all stored in the geopackage format to begin visualizing: # import (read) three spatial datasets stored in geopackage format mvc &lt;- st_read(&#39;GA_MVC/ga_mvc.gpkg&#39;) hwy &lt;- st_read(&#39;GA_MVC/ga_hwy.gpkg&#39;) trauma &lt;- st_read(&#39;GA_MVC/trauma_centers.gpkg&#39;) 17.1 tmap mode One nice feature of tmap is that it has two modes for plotting maps. You may develop a general preference for one over another, although in my opinion they serve slightly different purposes. The plot mode produces conventional static maps that are viewed in the plot plane or can be saved to a file. These will be the main maps for dissemination in papers, posters, or many presentations. The view mode is a more interactive plot in an html browser-like window. This mode allows the user to interact with the map including panning, zooming, and clicking on spatial objects to view underlying data. This is great for data exploration, and has extensions for web-served maps. However it is not so useful for any non-web-based dissemination or when you want to control the map. You select the mode with the function tmap_mode() and either 'plot' or 'view' in the parentheses. Note that when you set the mode, all the subsequent maps are in that mode…you must re-submit the tmap_mode() call to switch back again. By default, the tmap_mode() is 'plot', which means it produces static maps. We will plot some static maps, then switch to ’view' mode to compare. 17.2 Quick maps: qtm() The function qtm() stands for Quick Thematic Maps, and provides a step up from the simple plot() functions for quickly plotting spatial objects. The fundamental argument when submitting qtm() is the name of the object to be plotted. qtm(mvc) This produces the geometry but no other information (note that unlike plot(), it does not plot a map for every variable!). To produce a choropleth map (e.g. one in which objects are shaded to represent an underlying statistic or value), simply add the name of the variable. qtm(mvc, &#39;MVCRATE_05&#39;) Can you tell how the legend cut-points are determined? We’ll talk about when this matters and how to change it later. Now try switching tmap_mode(): tmap_mode(&#39;view&#39;) ## tmap mode set to interactive viewing Try these things in view mode: By default it will be visible in your R Studio Viewer pane; there is an icon with a screen and arrow that allows you to show in new window…do this so it is bigger Zoom in and out Pan Hover over counties (what do you see with hovering?) Click on counties (what do you see when you click?) Underneath the zoom + / - is an icon like a stack of papers. This changes the background map (how does the background information change as you zoom in/out?) Click on icon that looks like stack of pages. This lets you change the background map (assuming you are currently connected to internet) To change back (if you like) do this: tmap_mode(&#39;plot&#39;) ## tmap mode set to plotting 17.2.1 Customizing qtm() for polygons For polygon data, you might like to control several features including the title, the color palette, and the style by which the continuous variables are categorized in the legend. qtm(mvc, fill = &#39;MVCRATE_17&#39;, fill.style = &#39;quantile&#39;, fill.palette = &#39;YlGnBu&#39;, fill.title = &#39;MVC Mortality \\n(2017)&#39;) The syntax above customizes the original plot in several ways: By changing the fill.style (which is the style by which continuous variables are categorized in order to plot in a sequential ramp choropleth map) from the default (fixed or equal intervals) to a quantile style (by default quantiles have \\(n=5\\) so they are quintiles although other schemes including tertiles or quartiles are possible also) By choosing a custom color palette, in this case the Yellow-Green-Blue (YlGnBu) palette, which is one of several built-in options. Providing a more informative title to the legend, rather than the default variable name. 17.2.2 Customizing qtm() for lines qtm() (and tmap generally) can also handle other types of spatial data including line shape objects, and can provide some customization of the results. Try these for the highway dataset: qtm(hwy, lines.lwd = 2, lines.col = &#39;red&#39;) The basic plot of highways uses default colors and sizes, but the plot here uses the lines.lwd= argument to specify the line width or thickness. The lines.col= sets the color. 17.2.3 Customizing qtm() for points And not surprisingly, there is similar control for point spatial objects, in this case the locations of the trauma centers. qtm(trauma, symbols.size = &#39;LEVEL_number&#39;, symbols.shape = &#39;LEVEL&#39;) Because symbols.size and symbols.shape were specified, the symbolized variables by modifying the size and shape. There are also settings for color. If you study the help documentation, notice that some arguments require numbers (and thus use LEVEL_number which is an integer) and some allow character/factors (and thus use LEVEL). 17.2.4 Finding valid color options In base R there are many ways to specify colors including using standardized character strings, as well as HEX codes which are complicated alphanumeric labels that are used across industries to identify unique colors. Here is one of many lists of base-R color names: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf However for mapping we often want not just single colors, but reasonable sets of colors for symbolizing sequential values, categorical values, or diverging values. In the coming weeks we will talk about how to choose color and style for symbolizing maps, but it is worth knowing of a quick tool in tmap (or actually in an add-on package you installed called tmaptools) for seeing built-in color palettes. NOTE: Occasionally this next step has caused my session of R to crash. Therefore I usually open a second instance of R Studio just to do the next thing. To do that simply go to Session in the R Studio menu and click New Session. This creates another completely independent instance of R Studio (e.g. none of the packages or data loaded in this current session are present in the new session unless you specify them). tmaptools::palette_explorer() Why did we use the package name (tmaptools) followed by a double colon (::)? This is a shortcut in R that lets you call a single function from a package without loading the package. Basically this says “go look in the package called tmaptools and load this specified function”. I use this shortcut (in general, not only for tmaptools) in one of two situations: There is a function with the same name in two or more packages, and specifying which package identifies the one I mean. For instance we will soon learn the package dplyr and the function select() in this package is also the name of a function in another package for handling spatial data called raster. So I often use dplyr::select() to disambiguate. In situations like tmaptools::palette_explorer() where I really only need the one function but currently do not need anything else from the package. As you may discover with experimentation, the tmaptools::palette_explorer() function is actually a small interactive app that opens in a new window and lets you see an array of color palettes. You can see them divided by sequential, divergent, and categorical color ramps and you can move the slider to change how many categories and see the color ranges. The thing you want from this explorer is the abbreviated names to the left of each color ramp. You could also browse these same palettes by going to the Color Brewer website which is a compilation of recommended color palettes for cartography. 17.3 Building maps with tmap qtm() is great for quickly making a map, but when you want more control over the map, you will want to shift to the full functions of tmap. 17.3.1 Building blocks in tmap tmap produces maps using the grammar of graphics approach which means building up a final product as the ‘sum’ of several fundamental components, plus possible options layers. There are three broad components of maps in tmap: Specify the spatial object to map using tm_shape(). Following the call to tm_shape() you generally specify the layers you wish to symbolize or map. In other words specifying a shape doesn’t plot anything…it just is the starting point. The layers are the actual things from that object/shape to plot. In the case of polygons you will usually use tm_fill() to specify a layer about the fill of the polygon, although other layers are available (e.g. see the base and derived layers listed when you look at help('tmap')). Finally, in many instances you want to customize other map layout features, such as the title or the legend, and you might like to add elements such as a North arrow or a scale bar. For a given map, the various layers or steps are connected together in R code with a plus sign (+); this highlights that a map is the sum of many parts. NOTE: the use of the pipe (%&gt;%) and the plus (+) is seemingly the same in that they both connect steps together but they are not! It is perhaps unfortunate that ggplot2 and tmap do not use the same pipe as dplyr. Beware that you choose the correct connector for the function at hand! Note that steps 1 and 2 can be repeated for as many spatial objects as you wish to layer. So if you wanted to put points and lines on top of a polygon shape, you would specify tm_shape() and the corresponding layers for each spatial object in turn. This code replicates our first map with qtm(), and basically says, \"Start with the object mvc and symbolize it with two layers: the first fills the polygons to represent MVCRATE_17 and the second adds polygon borders: tm_shape(mvc) + tm_fill(&#39;MVCRATE_17&#39;) + tm_borders() Look at the help documentation for tm_fill() to see the myriad ways you can customize this map! It’s a little overwhelming, but I’d suggest looking at the style and palette arguments, and using the above-mentioned palette_explorer() to try out different colors and different styles for cut-points. 17.3.2 Customizing text on maps There are several ways you may wish to customize the text on maps. For example you may want to provide a name for the legend, new labels for the categories, or a title, subtitle or caption for the whole map. To give a title to the legend in a map use the title = 'xxx' in the tm_fill() (or other layer function) call. To change the labels of the legend To add a source or credits annotation # First, I create a vector of my custom legend labels # (note, there must be same number of labels as there are categories in map) myLabels &lt;- c(&#39;Low (Q1)&#39;, &#39;Q2&#39;, &#39;Q3&#39;, &#39;Q4&#39;, &#39;Hi (Q5)&#39;) tm_shape(mvc) + tm_fill(&#39;MVCRATE_17&#39;, style = &#39;quantile&#39;, title = &#39;MVC Rate in 2017&#39;, n = 5, labels = myLabels) + tm_borders() + tm_layout(title = &#39;Motor Vehicle Crashes per capita in Georgia&#39;, legend.outside = T) + tm_credits(&#39;Source: Georgia OASIS, retrieved 2019&#39;) SIDE NOTE: The tm_fill() option creates 5 bins or categories for plotting by default. For that reason it was unnecessary for me to put n = 5 to specify how many categories. However I did so to be explicit about the number of categories because I am provide a vector of 5 labels to correspond to the categories. Of course one could choose a non-default number of categories (e.g. n = 3 or n = 7), and if custom labels are provided there should be as many labels as categories. 17.3.3 Adding two or more spatial objects in one map Just like in ArcGIS, additional spatial layers can be added up to produce a more informative map. For instance if we were interested in how highways and trauma centers related to motor vehicle mortality rates we could add these layers. tm_shape(mvc) + tm_fill(&#39;MVCRATE_17&#39;, style = &#39;quantile&#39;, palette = &#39;Purples&#39;) + tm_borders() + tm_shape(hwy) + tm_lines(lwd = 2, col = &#39;red&#39;) + tm_shape(trauma) + tm_bubbles(shape = &#39;LEVEL&#39;, col = &#39;pink&#39;) Several things to note about above code: There are three separate spatial objects plotted, and each is called by starting with tm_shape() followed by some additional function specific to the layer. See the help documentation, or the Tenekes article on Canvas for a table of which layers are available for which kinds of shapes (e.g. polygons, points, or lines). Each function (e.g. each call with parentheses) is connected together with plus signs Within each function (e.g. within the parentheses), arguments are separated with commas I organize my code vertically because I think it makes it more readable than all on one line. However this is a point of style, not a requirement. Try changing these arguments or try substituting different options! 17.4 Making small-multiple maps Small multiples refers to the production of multiple maps to be presented as a set. We often desire small multiples as a way to visually compare two or more features when it is not easy to put them both on the same map. There are three ways to prepare small multiples in tmap. As you look at these, notice how they differ with respect to the number of legends produced, the range of the legends, and the content or flexibility of customization within and between map panels. 17.4.1 Small multiples as a vector of variables To plot side-by-side maps of two or more variables from the same spatial object, simply call a vector of variable names when specifying the layer or symbolization. tm_shape(mvc) + tm_fill(c(&#39;MVCRATE_05&#39;, &#39;MVCRATE_17&#39;), palette = &#39;Purples&#39;, style = &#39;quantile&#39;) + tm_borders() 17.4.2 Small multiples with facets Facet plotting is something common in the package ggplot2. It refers to the production of two or more plot figures stratified by a ‘grouping’ variable. Typically in facet plots from ggplot2, the scale of the \\(x\\) and \\(y\\) axis are held constant across the set of plots so that the values plotted are readily comparable. In tmap, facet plotting means creating multiple map plots that are distinguished by slicing or stratifying the spatial units along some by group. Faceting can be useful for highlighting patterns among different sub-groups in your spatial data. Unlike ggplot2, the scale of the legend and bounds of the x, y coordinate extent are not enforced to be the same across all panel maps by default. Instead the min/max x, y coordinates can vary according to the scope of content in each panel (e.g. by default, free.coords = T). By default the range and cut-points of the legend are held constant across maps (e.g. a single legend is produced to represent the data in all maps). If you would like to force consistency between panels (e.g. either for better contextualization or for comparability), that can be specified. Argument free.coords = FALSE (e.g. each map should NOT have its own min/max x, y coordinate range) and free.scale=FALSE (e.g. each map should NOT have its own spatial scale or ‘zoom’ appropriate to the contents of that panel). Here is a strange facet map produced by stratifying on the NCHS urban/rural six-level categorization scheme. First I have code for what happens by default, and then with setting the free.coords and free.scales to FALSE. You can see that by default, each map frame zooms to maximize the selected object, so the scale is different in each. In contrast when forced to maintain a constant scale it is easier to see the relative size and locations of each subset. # Basic facet map with defaults tm_shape(mvc) + tm_fill(&#39;MVCRATE_17&#39;) + tm_borders() + tm_facets(by = &#39;nchs_code&#39;) # With facet parameters set to FALSE tm_shape(mvc) + tm_fill(&#39;MVCRATE_17&#39;) + tm_borders() + tm_facets(by = &#39;nchs_code&#39;, free.coords = FALSE, free.scales = FALSE) 17.4.3 Facets for time-series How small multiples from vector-of-variables and facets differ: One point, which might not be obvious at first, that distinguishes these first two methods of small multiple map productions is how they use data to separate maps. Notice that the first option above (supplying a vector of variables to plot using the c() call within tm_fill() for example) is good for mapping things that are wide in your data. In other words it maps separate columns as different maps. In contrast the tm_facets() creates separate maps by stratifying rows of data. In other words it is good for mapping things that are long in your data. If you are not used to the idea of long versus wide data this might seem confusing, but its a relatively common distinction in data handling. An extension of this idea is that if you wanted to map a time-series (e.g. maps of disease rates each year over a series of years), you could create a long dataset by year. Imagine a dataset with a row of data for every county in Year 1; then a separate dataset with a row of data for every county in Year 2; and so on. By stacking these datasets your dataset becomes as long as the number of geographic units X the number of years. You could not do this easily in ArcGIS, but it is perfectly allowable with sf class spatial objects. When plotting, simply use tm_facets() with by = YEAR to produce your series. Here is an example of taking our current ‘wide’ dataset (e.g. we currently have 3 years in separate columns), and making it a long dataset (e.g. a single column for MVCRATE, and a separate column for year to distinguish which year-rate we are talking about). Then we produce time-series faceted maps. In this case we use the tidy functionality of the pivot_* verbs (e.g. read more about use of pivot verbs here) nrow(mvc) # N = 159 rows corresponds to N=159 Georgia counties ## [1] 159 mvc_long &lt;- mvc %&gt;% select(GEOID, NAME, MVCRATE_05, MVCRATE_14, MVCRATE_17) %&gt;% as_tibble() %&gt;% pivot_longer(cols = starts_with(&quot;MVCRATE&quot;), names_to = c(&quot;.value&quot;, &quot;year&quot;), values_to = &quot;mvc_rate&quot;, names_sep = &quot;_&quot;) %&gt;% mutate(year = 2000 + as.numeric(year)) %&gt;% st_as_sf() nrow(mvc_long) # N =477 rows corresponds to 3 years each for N =159 counties ## [1] 477 Now, plot that long sf object ignoring the fact that there are three rows of data for every county. Can you tell what happens? # This is the WRONG way to plot a long dataset! tm_shape(mvc_long) + tm_fill(&#39;MVCRATE&#39;) + tm_borders() # If you want a single map from a long dataset, use the subset() function ... tm_shape(subset(mvc_long, year == 2017)) + tm_fill(&#39;MVCRATE&#39;) + tm_borders() Notice how both maps above are the same? Try changing the YEAR == 2017 to a different year. You can see that when we ignored the long format, tmap essentially plotted the Georgia counties 3 times, with the last layer (e.g. 2017) being on top and thus the one we see. So beware… Now let’s take advantage of the long format dataset to facet or sub-divide the dataset into separate maps as delineated by the year variable: tm_shape(mvc_long) + tm_fill(&#39;MVCRATE&#39;) + tm_borders() + tm_facets(by = &#39;year&#39;, ncol = 1) 17.5 Small multiples with tmap_arrange() The third way to make small multiples, and one that gives maximum control over each separate panel, is to create them one at a time, and then combining them into a panel using the function tmap_arrange(). The notable difference here is that we name each map object as we create it, and then provide the list of names to tmap_arrange(). m1 &lt;- tm_shape(mvc) + tm_fill(&#39;MVCRATE_05&#39;) + tm_borders() m2 &lt;- tm_shape(trauma) + tm_symbols(shape = &#39;LEVEL&#39;, col = &#39;LEVEL&#39;) tmap_arrange(m1, m2) For this example I used two totally different shape objects to illustrate the point that tmap_arrange() is particularly good for combining things that are not simply wide or long subsets of a single dataset. This approach is also good if you are taking a totally different approach to symbolizing two variables in the same dataset, as it doesn’t assume you are trying to keep anything the same. 17.6 Summarizing small multiples Small multiples are not a common visualization in GIS software like ArcGIS. To do small multiples there you need to create multiple data frames and manipulate them in Layout view; it is often difficult to get consistent scales, legends, or coordinates. In R, the idea of faceting is quite common and has much potential for spatial epidemiology, which is why it is emphasized here. Below I summarize some of the overarching differences among the three approaches above for future reference. Feature Vectors c() of variables tm_facets() tmap_arrange() Approach Different map for different columns/variables Different map for different rows/subsets Completely independent map images Scale or legend Separate legend for each variable Choose either single scale across panels, or separate Each panel independent Coordinates Same for all variables from same sf object Option of same or different for each panel Each panel independent Typical use Quickly view set of variables Highlight spatial sub-regions Custom creation of figure 17.7 Saving maps Saving maps for use in other programs or applications is important. Images can be saved in the same output formats available in other R image functions. In other words we can save files as .png, .pdf, .jpg, .tiff, etc. A quick way to do is to use the export button from the plot pane in R studio. Recall that the way any graphic in R looks is shaped in part by the active graphic device. Your screen plot pane is the default graphic device and things are arranged to look good on your screen. However when you save to a different graphic device (e.g. a jpg device), things might look different. So sometimes you have to do some trial-and-error troubleshooting with the width, height, and dpi options. To specify the save via code, rather than the export button (which is a good idea in terms of reproducible code!) use tmap_save(). To save the final two-panel map I created from the previous step I could do this: # First make it an object by giving it a name, m3 m3 &lt;- tmap_arrange(m1, m2) tmap_save(m3, filename = &#39;mvc_maps.png&#39;) ## Map saved to C:\\Users\\mkram02\\Box\\SpatialEpi-2020\\EPI563-SpatialEPI\\mvc_maps.png ## Resolution: 2100 by 2100 pixels ## Size: 7 by 7 inches (300 dpi) You should now have the skills to make a wide variety of maps in R. To fine-tune how tmap works and to customize for each desired purpose, you will likely spend a lot of time looking at the help documentation or other online resources. While sometimes tedious, this process of figuring out how to make just the map you want is valuable. With time you will be able to create sophisticated maps quickly and efficiently. "]
]
