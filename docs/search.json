[{"path":"index.html","id":"how-to-use-this-ebook","chapter":"How to use this eBook","heading":"How to use this eBook","text":"Welcome Concepts & Applications Spatial Epidemiology (EPI 563) Fall 2021 Semester! eBook one several sources information support progress semester. overview course, expectations, learning objectives, assignments, grading, please review full course syllabus Canvas. eBook serves provide ‘jumping point’ content covered week. Specifically, content herein introduce key themes, new vocabulary, provide additional detail complementary asynchronous (pre-recorded) video lectures, foundational synchronous (class) work.","code":""},{"path":"index.html","id":"strategy-for-using-this-ebook","chapter":"How to use this eBook","heading":"Strategy for using this eBook","text":"separate module chapter week’s content. general, content within week’s section divided two sections focusing spatial thinking spatial analysis. dichotomy always hold, broad terms can expect sections specific content class Tuesday versus Thursday respectively.Spatial thinking epidemiology: section introduces vocabulary, concepts, themes important incorporation spatialized geo-referenced data epidemiologic work. minimum, plan read content prior class Tuesday, although likely benefit reading sections Tuesday.Spatial analysis epidemiology: section focused data management, visualization, spatial statistics, interpretation. content relevant work together Tuesday’s, essential successful work Thursday lab activities.Throughout book concepts ideas may highlighted call-blocks.block denotes potential pitfall area caution.block denotes additional bit information additional idea note topic hand.block denotes tip advice best practices efficiency.Please note continually updating eBook throughout semester, choose download, please double-check Last updated date (colored bar bottom screen) sure recent version.\neBook licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"software-installation.html","id":"software-installation","chapter":"Software installation","heading":"Software installation","text":"information module follows pre-class video setting R RStudio computer.","code":""},{"path":"software-installation.html","id":"installing-r-on-your-computer","chapter":"Software installation","heading":"Installing R on your computer","text":"August 2021, --date version R 4.1.1. R Project Statistical Computing continually working update improve R, result new versions 1-2 times per year.already R installed, can open console check current version : R.Version()$version.stringIf R older version listed , can install R going R repository: https://www.r-project.org/. Note many ‘mirrors’ servers software stored. Generally wise select one geographically close , although work theory. One mirror relatively close Atlanta : http://archive.linux.duke.edu/cran/","code":""},{"path":"software-installation.html","id":"installing-rstudio-on-your-computer","chapter":"Software installation","heading":"Installing RStudio on your computer","text":"R-Studio one several integrated development environments (IDE) working R. means wrapper around core R functionality makes coding project work R much easier without. develop projects analyses using R within IDE R-Studio. Using R-Studio lets us robust code-editing debugging, code syntax highlighting (e.g. coloring different words according use, identifying possible errors), assistance file management, working larger projects, outputting results.current version RStudio 1.4.1717. RStudio version older 1.2 please install/update.INSTALL: go https://www.rstudio.com/products/rstudio/download/UPDATE: Open RStudio go Help Menu choose ‘Check Updates’R-Studio Cheatsheat provides quick reference guide many ways R-Studio makes work R easier.","code":""},{"path":"installing-packages-for-this-course.html","id":"installing-packages-for-this-course","chapter":"Installing packages for this course","heading":"Installing packages for this course","text":"base R great deal essential functionality, power R comes rapidly growing list user-created contributed ‘packages.’ package simply bundle functions tools, sometimes also including example datasets, basic documentation, even tutorial ‘vignettes.’ can see official R packages going : https://cran.r-project.org/web/packages/.common way install package R install.packages() command. instance install package ggplot2 :install.packages(\"ggplot2\")Remember need install package (although may update packages occasionally – see green Update button Packages tab R Studio). want actually use package (example ggplot2) call like :library(ggplot2)call library() working, nothing visible happens. However see errors, might package date (thus needs updated/reinstalled), important dependencies missing. Dependencies packages package depends. Typically installed default, sometimes something missing. , simply install missing package try calling library(ggplot2) .Notice function install.packages('yourPackage') must use quotes around package name. contrast function library(youPackage) use quotes.submit installation request, note output R console. get warning says installation possible missing package ‘namespace’, suggests missing dependency (e.g. something main package needs work correctly). Try installing package mentioned error. trouble, reach TA’s!","code":""},{"path":"installing-packages-for-this-course.html","id":"installing-rtools40-windows-only","chapter":"Installing packages for this course","heading":"Installing Rtools40 (Windows Only)","text":"packages can installed mentioned (e.g. using install.packages()), instances installation requires complex ‘unpacking’ course code installation github. Mac OS Unix capability , Windows machine may require additional tools. Luckily package ! called Rtools40, install install packages .running Windows, navigate website: https://cran.r-project.org/bin/windows/Rtools/ follow instructions.","code":""},{"path":"installing-packages-for-this-course.html","id":"installing-packages-used-for-general-data-science","chapter":"Installing packages for this course","heading":"Installing packages used for general data science","text":"rest page, copy paste provided code order install packages necessary course. Notice hover right code-chunk html version eBook, see copy icon quick copying pasting.Although copying pasting code, take moment look output. get error messages package install? , re-check code try .packages support general work R:rmarkdown allows creation mixed output documents combine code, documentation results single, readable format.packages tinytex knitr necessary creating R documents including PDF output required submitting assignments.use many data manipulation tools tidyverse. can learn tidyverse : https://tidyverse.tidyverse.org/, can see applications tidyverse packages R Epidemiologists Handbook. tidyverse actually collection data science tools including visualization/plotting package ggplot2 data manipulation package dplyr. reason, install.packages('tidyverse') , actually installing multiple packages!packages pacman utilities help simplify file pathnames package loading behavior.","code":"\ninstall.packages('tidyverse')   \ninstall.packages(c('pacman', 'here'))\ninstall.packages(c('tinytex', 'rmarkdown', 'knitr')) \ntinytex::install_tinytex()  \n# this function installs the tinytex LaTex on your\n#  computer which is necessary for rendering (creating) PDF's "},{"path":"installing-packages-for-this-course.html","id":"installing-packages-use-for-geographic-data","chapter":"Installing packages for this course","heading":"Installing packages use for geographic data","text":"many ways get data want spatial epidemiology R. often (don’t always) use census geographies aggregating units, census populations denominators, following packages useful. designed quickly extract geographic boundary files (e.g. ‘shapefiles’) well attribute data US Census website via API. NOTE: work request free Census API key. Notice help() function get instructions .","code":"\ninstall.packages(c('tidycensus','tigris')) \n\nhelp('census_api_key','tidycensus')"},{"path":"installing-packages-for-this-course.html","id":"installing-packages-used-for-spatial-data-manipulation-visualization","chapter":"Installing packages for this course","heading":"Installing packages used for spatial data manipulation & visualization","text":"section installs set tools specific goals importing, exporting, manipulating, visualizing, analyzing spatial data.first line packages functions defining, importing, exporting, manipulating spatial data.second line tools use visualizing spatial data (e.g. making maps!).","code":"\ninstall.packages(c('sp', 'sf', 'rgdal', 'raster', 'RColorBrewer', 'rgeos', 'maptools', 'OpenStreetMap'))  \ninstall.packages(c('tmap', 'tmaptools', 'ggmap', 'shinyjs', 'shiny', 'micromap')) "},{"path":"installing-packages-for-this-course.html","id":"installing-packages-used-for-spatial-analysis","chapter":"Installing packages for this course","heading":"Installing packages used for spatial analysis","text":"Finally packages specifically spatial analysis tasks ’ll carry .","code":"\ninstall.packages(c('spdep', 'CARBayes', 'sparr', 'spatialreg',  'DCluster', 'SpatialEpi'))\ninstall.packages(c('GWmodel', 'spgwr') )"},{"path":"locating-spatial-epidemiology.html","id":"locating-spatial-epidemiology","chapter":"Week 1 Locating Spatial Epidemiology","heading":"Week 1 Locating Spatial Epidemiology","text":"","code":""},{"path":"locating-spatial-epidemiology.html","id":"getting-ready","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.1 Getting Ready","text":"","code":""},{"path":"locating-spatial-epidemiology.html","id":"learning-objectives","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"locating-spatial-epidemiology.html","id":"additional-resources","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.1.2 Additional Resources","text":"Geocomputation R Robin Lovelace. recurring ‘additional resource’ provides lots useful insight strategy working spatial data R. encourage browse quickly now, return often questions handle geographic data (especially class sf) R.basic introduction ggplot2 package. just one dozens great online resources introducing grammar graphics approach plotting R.basic introduction tmap package also one many introductions tmap mapping package. tmap builds grammar graphics philosophy ggplot2, brings lot tools useful thematic mapping!R SAS users cheat sheet","code":""},{"path":"locating-spatial-epidemiology.html","id":"important-vocabulary","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 1","code":""},{"path":"locating-spatial-epidemiology.html","id":"spatial-thinking-in-epidemiology","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.2 Spatial Thinking in Epidemiology","text":"first learning epidemiology, can difficult distinguish concepts, theories, purpose epidemiology versus skills, tools, methods use implement epidemiology. distinctions foundational collective professional identity, way go work. instance think epidemiologists data analysts, scientists, data scientists, technicians something else? questions bigger can address class, importance becomes especially apparent learning area spatial epidemiology. tendency discourse spatial epidemiology focus primarily data methods without understanding relate scientific questions health population ultimately responsible. Distinguishing threads overarching goal course, even learn data science spatial analytic tools.One quite simplistic important example questions methods inter-related apparent think data. Data central quantitative analysis, including epidemiologic analysis. data different spatial epidemiology?first thing might come mind explicitly geographic spatial measures contained within data. content spatial data distinct: addition geographic spatial location may illuminate otherwise aspatial attributes. even fundamental content thinking unit analysis.likely many examples epidemiology coursework, explicit (sometimes implicit) unit analysis individual person. Spatial epidemiology can definitely align individual-level analysis. ’ll see, common units observe measure spatial epidemiology – therefore units compose much data – individuals instead geographic units (e.g. census tract, county, state, etc) extension collection aggregation individuals therein. distinction unit analysis important implications epidemiologic concerns including precision, bias, ultimately inference (e.g. meaning can make analysis), ’ll discuss throughout semester.One concrete implication discussion always able answer basic question dataset wish analyze: “one row data represent?” row data one way think unit analysis, often (always) spatial epidemiology row data summary population contained geographic unit boundary. Said another way ecologic summary population. stated , simplistic example important learn spatial statistics methods, also maintain perspective epidemiology population health science. advance public health need good methods also need critical understanding populations support, data analyze, conclusions can reliably draw work.move semester, encourage dig deep methods work, also step back ask questions like “choose method?” “question epidemiology useful ?”","code":""},{"path":"locating-spatial-epidemiology.html","id":"spatial-analysis-in-epidemiology","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3 Spatial Analysis in Epidemiology","text":"","code":""},{"path":"locating-spatial-epidemiology.html","id":"spatial-data-storage-formats","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.1 Spatial data storage formats","text":"worked spatial GIS data using ESRI’s ArcMap, familiar called shapefiles. one common format storing geographic data computers. ESRI shapefiles actually single file, anywhere four eight different files file name different extensions (e.g. .shp, .prj, .shx, etc). different file (corresponding extension) contains different portion data ranging geometry data, attribute data, projection data, index connecting together, etc.may know shapefiles (opinion definitely best) way store geographic data. class recommend storing data format called geopackages indicated .gpkg extension. Geopackages open source format developed functional mobile devices. useful storing individual files efficient compact way. clear, many formats make claim geopackages ultimate format; just happen meet needs course, much work spatial epidemiologists. worth noting many GIS programs including ArcMap QGIS can read write geopackage format; constraint limitation terms software data stored .gpkg format.","code":""},{"path":"locating-spatial-epidemiology.html","id":"representing-spatial-data-in-r","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.2 Representing spatial data in R","text":"work course assumes basic R user; need expert, assume understand data objects (e.g. data.frame, list, vector), basic operations including subsetting index (e.g. using square brackets extract modify information: []), base-R plotting, simple modeling. familiar R, need quick self-directed learning.good online resources R skills, instructor TA’s can point additional resources needed:Epidemiologist R HandbookR Data Science, particularly introductory chaptersR TutorialJust conceptualization , thinking data spatial epidemiology requires reflection, actual structure representation data computer tool R also requires attention. Specifically, spatial data R exactly like conventional aspatial epidemiologic data often arranged rectangular data.frame (e.g. like spreadsheet rows observations columns variables). spatial data complex just spreadsheet, need complex spatial data software platforms like ESRI’s ArcMap.spatial, dataset must representation geography, spatial location, spatial relatedness, commonly done either vector raster data model (see description vocabulary). spatial geographic representations must stored computer /held memory, hopefully means relating associating individual locations corresponding attributes. example want know attribute (e.g. count deaths given place), location place, ideally want two connected together.past 10+ years, R increasingly used analyze visualize spatial data. Early , investigators tackling complexities spatial data analysis R developed number ad hoc, one-approaches data. worked short term specific applications, created new problems users needed generalize method new situation, chain together steps. settings uncommon convert dataset multiple different formats accomplish task sequence; resulted convoluted error-prone coding, lack transparency analysis.eventual response early tumult thoughtful systematic approach defining class data tackled unique challenges spatial data R. Roger Bivand, Edzer Pebesma others developed sp package defined spatial data classes, provided functional tools interact . sp package defined specific data classes hold points, lines, polygons, well raster/grid data; data classes can contain geometry (names like SpatialPoints SpatialPolygons) contain geometry plus related data attributes (names like SPatialPointsDataFrame SpatialPolygonsDataFrame). spatial object can contain information spatial data might include: spatial extent (min/max x, y values), coordinate system spatial projection, geometry information, attribute information, etc.flexibility power sp* class objects, became standard last years. Interestingly, perhaps sophistication sp* class began undermine . sp* class data well-designed programming point view, still little cumbersome (frankly confusing) applied analysts new users. Analysis spatial epidemiology primarily computer programming, producing transparent reliable data pipelines conduct valid, reliable, reproducible analysis. Thus epidemiologists, data scientists, desired spatial tools incorporated growing toolbox data science tools R.calls user-friendly intuitive approach spatial data led team (e.g. Bivand, Pebesma, others) develop Simple Features set spatial data classes R. Loaded sf package, data format quickly become standard handling spatial data R. power sf class, discussed , makes spatial data behave like rectangular data thus makes amenable manipulation using tool works data.frame tibble objects. Recognizing many users functions prefer older sp* objects, sf package includes number utility functions easily converting back forth.class use sf* class objects preferred data class, tools ’ll learn still require sp* occasionally go back forth.sf* data classes designed hold essential spatial information (projection, extent, geometry), easy evaluate data.frame format integrates attribute information geometry information together. result intuitive sorting, selecting, aggregating, visualizing.","code":""},{"path":"locating-spatial-epidemiology.html","id":"benefits-of-sf-data-classes","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.3 Benefits of sf data classes","text":"Robin Lovelace writes online eBook, Gecomputation R, sf data classes offer approach spatial data compatible QGIS PostGIS, important non-ESRI open source GIS platforms, sf functionality compared sp provides:Fast reading writing dataEnhanced plotting performancesf objects can treated data frames operationssf functions can combined using %>% pipe operator works well tidyverse collection R packages (see Tips using dplyr examples)sf function names relatively consistent intuitive (begin st_)","code":""},{"path":"locating-spatial-epidemiology.html","id":"working-with-spatial-data-in-r","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.4 Working with spatial data in R","text":"lab, one example dataset use, called ga.mvc quantifies counts rates death motor vehicle crashes Georgia’s \\(n=159\\) counties. dataset vector represents counties polygons associated attributes (e.g. mortality information, county names, etc).","code":""},{"path":"locating-spatial-epidemiology.html","id":"importing-spatial-data-into-r","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.4.1 Importing spatial data into R","text":"important distinguish two kinds data formats. way data stored computer hard drive, way data organized managed inside program like R. shapefiles (.shp) popularized ESRI/ArcMap example format storing spatial data hard drive. contrast, discussion sf* sp* data classes refer data organized inside R.Luckily, regardless data stored computer, possible import almost format R, inside R possible make either sp* sf* data class. means receive data .shp shapefile, .gpkg geopackage, .tif raster file, can easily imported.sf functions act spatial objects begin prefix st_. Therefore import (read) data use st_read(). function determines import data based extension file name specify. Look help documentation st_read(). Notice first argument dsn=, might complete file name (e.g. myData.shp), might folder name (e.g. mygeodatabase.gdb). motor vehicle crash data saved shapefile (mvc.shp, actually six different files computer), geopackage (mvc.gpkg) can read like :can take look defined data class imported objects within R:Notice two objects class (e.g. type data stored within R), even though two different kinds files stored computer: one shapefile one geopackage. st_read() can automatically detect storage format based extension, use appropriate interpreter import data. nice means can bring many types spatial data R!also notice examined class() object, classified sf data.frame class. incredibly important, speaks elegant simplicity sf* data classes! classified sf perhaps obvious spatial object; fact object also classified data.frame means can treat object purposes data management, manipulation analysis relatively simple-seeming object: rectangular data.frame. work? explore lab essentially dataset rows (observations) columns (variables).can see variable/column names like :can see dataset attribute variables (e.g. GEOID, NAME, MVCRATE_17), final column called geometry one called geom another.geometry columns different usual run---mill column variables don’t hold single value. Instead, ‘cell’ columns actually contains embedded list \\(x,y\\) coordinates defining vertices polygons Georgia’s counties. spatial location information row contained single variable called geom (alternately, geometry).Another way learn sf object use head() function. addition displaying top rows data (typical behavior head() function), sf objects head() also print important metadata file.summarize, sf within R powerful :limited data arrive us. acquire data (web, colleague, etc) shapefile, geopackage, raster formats, can imported R.inside R (stored sf data objects), can treat datasets almost aspatial, rectangular datasets. means use subsetting, filtering, recoding, merging, aggregating without losing spatial information!","code":"\n# this is the shapefile\nmvc.a <- st_read('GA_MVC/ga_mvc.shp')\n\n# this is the geopackage\nmvc.b <- st_read('GA_MVC/ga_mvc.gpkg')\nclass(mvc.a)## [1] \"sf\"         \"data.frame\"\nclass(mvc.b)## [1] \"sf\"         \"data.frame\"\nnames(mvc.a)## [1] \"GEOID\"      \"NAME\"       \"MVCRATE_17\" \"geometry\"\nnames(mvc.b)## [1] \"GEOID\"      \"NAME\"       \"MVCRATE_17\" \"geom\"\nhead(mvc.a)## Simple feature collection with 6 features and 3 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -84.64195 ymin: 31.0784 xmax: -82.04858 ymax: 34.49172\n## Geodetic CRS:  WGS 84\n##   GEOID                     NAME MVCRATE_17                       geometry\n## 1 13001  Appling County, Georgia   53.99276 MULTIPOLYGON (((-82.55069 3...\n## 2 13003 Atkinson County, Georgia   35.96260 MULTIPOLYGON (((-83.141 31....\n## 3 13005    Bacon County, Georgia    0.00000 MULTIPOLYGON (((-82.62819 3...\n## 4 13007    Baker County, Georgia   31.25000 MULTIPOLYGON (((-84.64166 3...\n## 5 13009  Baldwin County, Georgia   28.94936 MULTIPOLYGON (((-83.42674 3...\n## 6 13011    Banks County, Georgia   32.19921 MULTIPOLYGON (((-83.66862 3..."},{"path":"locating-spatial-epidemiology.html","id":"exporting-spatial-data-from-r","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.4.2 Exporting spatial data from R","text":"importing often primary challenge spatial data R, uncommon might modify alter spatial dataset wish save future use, write disk share colleague. Luckily sf package functionality write sf spatial object disk wide variety formats including shapefiles (.shp) geopackages (.gpkg). , R uses extension specify filename determine target format.","code":"\n# Write the file mvc to disk as a shapefile format\nst_write(mvc, 'GA_MVC/ga_mvc_v2.shp')\n\n# Write the file mvc to disk as a geopackage format\nst_write(mvc, 'GA_MVC/ga_mvc_v2.gpkg')"},{"path":"locating-spatial-epidemiology.html","id":"basic-visual-inspectionplots","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.5 Basic visual inspection/plots","text":"want see spatial data? base-R powerful function called plot() can used create easy incredibly complex visualizations graphical representation data. package sf, functionality plot() extended handle uniqueness spatial data. means call plot() spatial object without loaded sf, results different plot() called loading sf.plot() sf, default try make map every variable data frame! Try . want (usually ), can force plot variables providing vector variable names.Sometimes want know something spatial size, extent, shape data. can easily plot geometry spatial object (e.g. attributes). two approaches quickly plot geometry:","code":"\nplot(mvc) # this plots a panel for every column - or actually the first 10 columns## Warning: plotting the first 9 out of 17 attributes; use max.plot = 17 to plot\n## all\nplot(mvc['MVCRATE_05']) # this plots only a single variable, the MVC mortality rate for 2005\nplot(mvc[c('MVCRATE_05', 'MVCRATE_17')]) # this plots two variables: MVC rate in 2005 & 2017\nplot(st_geometry(mvc)) # st_geometry() returns the geom information to plot\nplot(mvc$geom)  # this is an alternative approach...directly plot the 'geom' column"},{"path":"locating-spatial-epidemiology.html","id":"working-with-crs-and-projection","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.6 Working with CRS and projection","text":"Maps used describe geographical spatial location particular objects representation things planet Earth. maps printed paper screens. words, maps identify locations somewhere planet earth represent flat.world latitude longitude lines painted ground, earth flat! Instead earth nearly spherical (really geoid) universal reference start measuring.two reasons, maps require minimum coordinate reference system (CRS) define numbers coordinates relate actual places. addition maps best interpreted formally projecting data account artifact induced pretending earth flat.unambiguous way describe CRS /projection using EPSG code, stands European Petroleum Survey Group. consortium standardized hundreds projection definitions manner adopted several R packages including rgdal sf.given dataset already CRS (possibly projection). CRS projection information contained original file imported, usually maintained use st_read(). However sometimes missing must first find . known, might choose change transform CRS projection specific purpose. discuss class.CRS information imported critical find CRS information data source!course GIS course (e.g. assumed already exposure geographic information systems generally), learning theory application coordinate reference systems projections primary purpose semester. However basic knowledge necessary successfully working spatial epidemiologic data. several resources peruse learn CRS, projections, EPSG codes:useful overview/review coordinate reference systems RRobin Lovelace’s Geocompuation R projections sfEPSG website: link searchable database valid ESPG codesHere useful EPSG codes\nFIGURE 1.1: Comparing CRS\nchoice CRS /projection substantial impact rendered map looks, evident figure (source image).already saw CRS/projection information mvc object used head() function ; top read WGS 84.Recall two main types CRS: purely geographic say coordinate locations represented latitude longitude degrees; projected means coordinate values transformed representation spherical geoid onto planar (Euclidean) coordinate system. WGS 84 ubiquitous geographic coordinate system common boundary files retrieved U.S. Census bureau.important question work spatial dataset understand whether primarily geographic projected CRS, one.quick logical test returns TRUE FALSE answer question “sf object simply longitude/latitude geographic CRS?”. answer case TRUE WGS 84 geographic (longlat) coordinate system. FALSE wanted know CRS/projection?somewhat complicated looking output summary CRS stored spatial object. two things note output:top, User input WGS 84At bottom section labeled GEOGCRS says ID[\"EPSG\",4326\"]literally hundreds distinct EPSG codes describing different geographic projected coordinate systems, semester three worth remembering:EPSG: 4326 common geographic (unprojected long-lat) CRSEPSG: 3857 also called WGS 84/Web Mercator, dominant projection used Google MapsEPSG: 5070 code projected CRS called Albers Equal Area benefit representing visual area maps equal manner.One rule thumb determine data degrees lat/long versus linear units meters miles look xmin, ymin, xmax, ymax printed top output whenever use head(xxx). Degrees latitude (y-axis values) range \\(-90^\\circ\\) \\(+90^\\circ\\), degrees longitude (x-axis values) range \\(0^\\circ\\) \\(180^\\circ\\).contrast projected data cartesian linear units (rather degrees), usually numbers much higher 180.CRS/projection clearly defined, may choose transform project data different system. sf package another handy function called st_transform() takes spatial object (dtaaset) one CRS outputs object transformed new CRS.see difference three? Although EPSG 4326 unprojected EPSG 3857 projected (e.g. Mercator conical projection), appear similar, although identical. Mercator projection known increased distortion equator. general prefer use ‘projected’ rather ‘unprojected’ (long/lat ) data visualization analysis, specifically almost always prefer equal area projections choropleth maps, coloring area represented communicates something intensity measure.Whenever bring new dataset need check CRS project transform needed.Important: important distinguish defining current projection data act projecting transforming data one known system new CRS/projection.transform data correctly define current original CRS/projection status. function tells us current status . cases data associated CRS information might completely blank (instance read numerical \\(x,y\\) points geocoding GPS process). cases can set underlying CRS using st_set_crs() define , assumes know . two arguments function: first x = objectName, second value = xxx ‘xxx’ valid EPSG code.","code":"\nst_is_longlat(mvc)## [1] TRUE\nst_crs(mvc)## Coordinate Reference System:\n##   User input: WGS 84 \n##   wkt:\n## GEOGCRS[\"WGS 84\",\n##     DATUM[\"World Geodetic System 1984\",\n##         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n##             LENGTHUNIT[\"metre\",1]]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"geodetic latitude (Lat)\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"geodetic longitude (Lon)\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     USAGE[\n##         SCOPE[\"Horizontal component of 3D system.\"],\n##         AREA[\"World.\"],\n##         BBOX[-90,-180,90,180]],\n##     ID[\"EPSG\",4326]]\n# This uses the Albers equal area USA, \nmvc.aea <- st_transform(mvc, 5070)\n\n# This uses the Web Mercator CRS (EPSG 3857) which is just barely different from EPSG 4326\nmvc.wm <- st_transform(mvc, 3857)\n\n# Now let's look at them side-by-side\nplot(st_geometry(mvc), main = 'EPSG 4326')\nplot(st_geometry(mvc.wm), main = 'Web Mercator (3857)')\nplot(st_geometry(mvc.aea), main = 'Albers Equal Area (5070)')"},{"path":"cartography-for-epidemiology-i.html","id":"cartography-for-epidemiology-i","chapter":"Week 2 Cartography for Epidemiology I","heading":"Week 2 Cartography for Epidemiology I","text":"","code":""},{"path":"cartography-for-epidemiology-i.html","id":"getting-ready-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.1 Getting Ready","text":"","code":""},{"path":"cartography-for-epidemiology-i.html","id":"learning-objectives-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"cartography-for-epidemiology-i.html","id":"additional-resources-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.1.2 Additional Resources","text":"CDC Guidance Cartography Public Health Data (complements required reading)Maps LieColor Brewer Website color guidance choropleth mapsAnalyzing US Census Data: Methods, Maps, Models R","code":""},{"path":"cartography-for-epidemiology-i.html","id":"important-vocabulary-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 2","code":""},{"path":"cartography-for-epidemiology-i.html","id":"spatial-thinking-in-epidemiology-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2 Spatial Thinking in Epidemiology","text":"Making pretty maps full extent spatial epidemiology. However, epidemiologic cartography can sometimes beginning end spatial epidemiology given purpose. even epidemiologic analysis goes well beyond mapping (perhaps incorporate aspatial analysis, incorporate sophisticated spatial analysis), ability produce clear, concise, interpretable map important skill.Robb, et al1 write:Disease mapping can used provide visual cues disease etiology, particularly relates environmental exposures….Mapping things allows visualization baseline pattern spatial structure disease, potential detection disease clusters, initial investigation exposure-disease relationship.aspects cartography map design general thematic maps quantitative data. issues seem especially pertinent us epidemiologists quantitative population health scientists. include decisions make color choice process categorizing numerical data visual representation map.especially important epidemiology? primary purpose map visually represent something meaningful spatial geographic variation health health-relevant feature (e.g. exposure resource). Communicating meaningful representing variation matters solely technical GIS task; requires epidemiologic insight.instance approach representing ratio measures odds ratio risk ratio different represent risk rate data, understand scale units distinct case. Similarly, understand characterizing variation heterogeneity normal Gaussian (bell-shaped curve) distribution different uniform highly skewed distribution long right tail. insight scales values differently interpreted epidemiologically must translated sensible choices mapping.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"color-choices","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.1 Color choices","text":"thematic maps, color flexible important tool communication. Color, hue, contrast can accentuate map elements themes minimize others. result can completely change story map tells seemingly small changes use color. means clear explicit choose given color sequence colors, beware unintentionally misrepresenting data color choices.producing choropleth maps, often talk collections colors color ramps color palettes, single color interesting. quick scan either tmaptools::palette_explorer() utility, Color Brewer website demonstrate many colors choose , just matter preference? Perhaps, guidelines keep mind.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"sequential-palettes","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.1.1 Sequential palettes","text":"color palettes use color hue, value, saturation represent symbolize values underlying statistical parameter interest. parameter statistic naturally ordered, sequential monotonic, makes sense choose colors range light dark. Conventionally lighter neutral tones represent lower smaller numbers darker colors intense tones represent higher larger numbers. dark colors jump viewer readily, occasionally inverse used emphasize small values, done caution can counter intuitive.\nFIGURE 2.1: Sequential color palettes\nSequential palettes useful epidemiologic parameters prevalence, risk, rates, continuous exposure values emphasis distinguishing higher values lower values.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"diverging-palettes","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.1.2 Diverging palettes","text":"less common choice, one especially important epidemiologic parameters, diverging palette. pattern, neutral color center sequence, two different color hues become darker intense go center.\nFIGURE 2.2: Diverging color palettes\nmight choose color sequence one two reasons:wish show units vary around overall mean median, highlighting larger versus smaller overall mean/median. instance diverging palettes might emphasize areas particularly high burden disease (therefore need additional attention), well unexpectedly low burden disease (therefore worthy understanding protective factors).mapping epidemiologic measure effect (e.g. ratio difference measure) values null ratio \\(1.0\\) (ratio) \\(0\\) (difference). example map Standardized Mortality/Morbidity Ratios, risk odds ratios, prevalence ratios, potentially diverging data. exception ratio values side null (e.g. \\(>>1\\) \\(<<1\\)).\nFIGURE 2.3: Mapping ratio measure divergent palette\nmap , SMR (ratio county-specific prevalence low birth weight infants overall statewide live birth prevalence) varies \\(0.13\\) \\(2.30\\). words, county SMR \\(1.0\\) average prevalence low birthweight. range data sequential way risk prevalence. Instead neutral color assigned counties range \\(0.90-1.10\\), around null. way indicating counties average typical. contrast, counties increasing excess morbidity darker green, substantially lower morbidity darker purple.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"qualitative-palettes","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.1.3 Qualitative palettes","text":"Qualitative refers categories naturally ordered sequential. instance counties assigned values single leading cause death county, might choose qualitative palette, sequential diverging palette might mislead viewer thinking natural ordering causes less intense color.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"choropleth-binning-styles","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.2 Choropleth binning styles","text":"second topic relevant intersection cartography epidemiologic thinking means choose cut-points visualizing data. words map visually represent underlying statistical value, assign map numerical values colors. depends greatly intended message story map needs tell. interested distinguish units rank higher lower values? primarily focused finding extreme outliers, variation ‘middle’ distribution less interest? distinct purposes give rise different decisions assign colors numerical values data.\nFIGURE 2.4: Comparing binning styles data\ndiscussed lecture, numerous methods styles categorizing continuous data choropleth mapping (e.g. identical data summarized four different styles figure ). Cynthia Brewer (ColorBrewer fame) Linda Pickle (2002) sought evaluate styles effective communicating spatial patterns epidemiologic data.cartographers, Brewer & Pickle critical epidemiologists’ -reliance quantile cut-points, given many strategies seemed cartographic advantages. However, randomizing map ‘readers’ interpret maps underlying epidemiologic data using seven different styles, determined readers accurately reliably interpret disease patterns maps using quantile cut-points. benefits styles purposes, common use communicating spatial areas rank higher lower terms disease burden, quantiles straightforward.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"mapping-time-series","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.2.1 Mapping time series","text":"common spatial epidemiology want map spatial patterns disease several different snapshots time series observe evolution disease burden time. changing patterns time raises additional questions make cuts data. several options determining cut-points time series:Pool years data together calculating cut-points (e.g. using quantiles). Use pooled cut-points years.Create custom year-specific cut-points reflect distribution data year separately.Create cut-points based single year apply years.\nFIGURE 2.5: Georgia MVC deaths year common scale\nmap Georgia motor vehicle crash mortality data three different years (2005, 2014, 2017), created tmap using tm_facet() option = year. result, quantile cut-points represent breaks pooling observations across three years. words cut-points come 159 counties times three years: 477 values.common legend applies three maps, strategy useful comparing differences absolute rates across years.\nFIGURE 2.6: U.S. heart disease mortality year-specific scales\nmap heart disease mortality rates county two years (1973-4; 2009-10) uses quantile breaks calculated separately time period. done part heart disease mortality rate declined much years scale distinguished highs lows one map distinguish anything map. case compared absolute rates relative ranking counties two years.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"spatial-analysis-in-epidemiology-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3 Spatial Analysis in Epidemiology","text":"Every spatial epidemiology project must include attention data acquisition, cleaning, integration, visualization. specific workflow driven largely overarching epidemiologic question, purpose, goal. section use specific question illustrate key steps data preparation epidemiologic cartography.Case Example Objective: Create choropleth map visualizing geographic variation -cause mortality rate U.S. counties 2016-2018. Compare choropleth map % uninsured U.S. counties.objective directly relevant lab week well Visualizing US Mortality, Visual Portfolio, assignment due later semester.Although specific question dictates specific data needs, four types data frequently needed produce map health outcome state:Numerator data, case representing count deaths per county target yearDenominator data, case representing population risk death county target yearContextual covariate data, case prevalence uninsured U.S. countyGeographic (geometry) data representing shapes boundaries U.S. counties","code":""},{"path":"cartography-for-epidemiology-i.html","id":"obtaining-and-preparing-numerator-data","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.1 Obtaining and preparing numerator data","text":"event interest (e.g. numerator risk, rate, prevalence) can come many sources. conducting primary data collection, arises study design measurement. using secondary data, common use surveillance data (e.g. vital records, notifiable diseases, registries, etc) administrative data source health events.using secondary data sources owned managed another entity, one challenge can occur suppression data protect privacy. example National Center Health Statistics mortality data available CDC WONDER suppresses count deaths, well crude mortality rate, whenever numerator count less ten events. can also instances local state public health agency fails report data NCHS, producing missing values.Depending data format, possible either missing suppressed data inadvertently imported R zero-count rather missing. therefore critically important understand data source guidelines. decision manage zero, missing, suppressed data epidemiologic choice, one must addressed creating map.deal data suppression. many reasons target data may fall thresholds suppression. Perhaps outcome event quite rare, stratifying multiple demographic factors, perhaps counting small geographic unit. suppression problematic mapping, consider pooling multiple years, reducing demographic stratification, using larger geographic areas increase event count reduce number suppressed cells.example, downloaded -cause mortality counts county CDC WONDER 2016-2018 (pooling three years reduce suppression). Lab discuss procedure acquiring data web. importing data appears.","code":"\nhead(death)##    FIPS             County Deaths Population     crude\n## 1 01001 Autauga County, AL    536      55601  964.0114\n## 2 01003 Baldwin County, AL   2357     218022 1081.0836\n## 3 01005 Barbour County, AL    312      24881 1253.9689\n## 4 01007    Bibb County, AL    276      22400 1232.1429\n## 5 01009  Blount County, AL    689      57840 1191.2172\n## 6 01011 Bullock County, AL    112      10138 1104.7544"},{"path":"cartography-for-epidemiology-i.html","id":"obtaining-and-preparing-denominator-or-contextual-data","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.2 Obtaining and preparing denominator or contextual data","text":"mortality data accessed CDC included numerator (count deaths) denominator (population risk). However instances may one dataset provides health event data (numerator), need link population denominator order calculate risk, rate, prevalence.U.S. Census Bureau maintains reliable population count data U.S., available aggregates Census Block Group, Census Tract, Zip code tabulation area, City Place, County, State, Region.Census data can aggregated total population stratified age, gender, race/ethnicity, many variables. census data also contains measures social, economic, housing attributes may relevant measure context exposures spatial epidemiologic analyses. two broad types demographic socioeconomic data released Census Bureau.Decennial Census tables (theoretically) count 100% population every 10 years. can cross-classified age, race/ethnicity, sex, householder status (e.g. whether head house owns rents many people live house)American Community Survey (ACS) tables provide much larger number measures based samples rather complete counts. ACS began early 2000’s continually sampled survey. Despite collected every year, many small areas (e.g. census tracts even counties) enough responses single year make reliable estimates. Therefore ACS data pooled 5-year moving-window datasets. instance 2015-2019 ACS reports estimates responses collected time period, available Census Block Group .may accessed Census ACS data directly Census Bureau website classes tasks past. interest reproducibility efficiency, introduce tidycensus package R. excellent tool acquiring either Decennial Census ACS data directly within R. advantage twofold:can quicker learn ;\n2, makes data acquisition fully reproducible without unrecorded steps happening web browsers. words actual code details downloaded (rather un-documented steps clicking downloading browser).practice code next sections lab. included primer. sections walk one way download prepare data quantify county-level prevalence population uninsured, might covariate interest examining spatial variation mortality.selected code relatively efficient, although may find complex confusing. include like explore data-manipulation functions R. Please note need learn functions Census data acquisitions section course, although might find related approaches useful. Note also many ways accomplish anything R, achieve ends different strategies.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"setting-up-census-api","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.2.1 Setting up Census API","text":"access Census products (e.g. attribute tables geographic boundary files) using tidycensus package, need register declaring API key. haven’t already done , go register key.","code":"\n# Only do this if you haven't already done it; it should only need to be done once.\n\ntidycensus::census_api_key('YourKeyHere', install = T) "},{"path":"cartography-for-epidemiology-i.html","id":"choosing-variables","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.2.2 Choosing Variables","text":"far biggest challenge requesting data Census Bureau knowing want, stored. Census data distributed aggregated counts contained specific tables (unique ID), made specific variables (also unique ID composed table ID plus unique ID). two ways find variables:go Census website browse around. instance Census Data Explorer website one way browse topics variablesYou download variables given year R, use filters search .code queries Census website (assuming internet connection) requests list variables ACS 5-year pooled dataset (e.g. acs5) window time ending 2018 (e.g. 2014-2018). also specify cache = T just means save results quicker loading ask future.may easiest look dataset using View() function. , see three variables, option click Filter button (upper left View pane; looks like funnel). Filter option one way search key words either label concept column.interested capturing prevalence uninsured county. Try :Go View mode variables (e.g. View(all_vars))Click Filter buttonType insurance concept fieldType B27001 name field\nFIGURE 2.7: Screenshot RStudio View() ACS variables\nwant list specific tables variable ID’s extract Census. lab use detailed code accomplish goal.may noticed full list ACS variables nearly 27,000 variables! code use tricks filter huge list variables get names want. relies tidyverse package stringr great manipulating character variables (great many data science tasks; read stringr ). case using filter just table want (e.g. B27001), get names variables contain string ‘health insurance’.list variables want acquire; one represents count uninsured multiple age groups. sum get total population uninsured prevalence.","code":"\nlibrary(tidycensus)\n\nall_vars <- load_variables(year = 2018, dataset = 'acs5', cache = T)\n\nhead(all_vars)\na <- all_vars %>% \n  filter(stringr::str_detect(name, 'B27001')) %>%  # this limits to rows for the B27001 table\n  filter(stringr::str_detect(label, 'No health insurance'))  # this limits to rows with this text\n\nmyVars <- c('B27001_001', a$name)##  [1] \"B27001_001\" \"B27001_005\" \"B27001_008\" \"B27001_011\" \"B27001_014\"\n##  [6] \"B27001_017\" \"B27001_020\" \"B27001_023\" \"B27001_026\" \"B27001_029\"\n## [11] \"B27001_033\" \"B27001_036\" \"B27001_039\" \"B27001_042\" \"B27001_045\"\n## [16] \"B27001_048\" \"B27001_051\" \"B27001_054\" \"B27001_057\""},{"path":"cartography-for-epidemiology-i.html","id":"retrieving-data-from-census","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.2.3 Retrieving data from Census","text":"actually retrieve data Census use function get_acs() (getting decennial data function get_decennial()). request data must specify geography (e.g. want counts states, counties, census tracts, census block groups?), variables, year, dataset. Look ?get_acs read options.following code chunks use dplyr tidyverse verbs %>% (pipe) connect data steps together. complex first, worth carefully examining step works. familiar syntax, probably helpful review Appendix section dplyr.Looking first rows data object insure_tidy , might surprised column labeled variable, cells within column actually thought variable names! data structured tidy format, happens long wide. Read transposing data . following steps reshape data useful.code :define geography = county.Specify vector (previously created named myVars) variables downloadSpecify year interest. Note 2018 references 2014-2018 5-year windowspecify survey, often acs5The code necessary variables age-specific counts number uninsured people. one variable, B27001_001 count included table. words, denominator calculating prevalence uninsured. Therefore following code :filter() restricts rows data variable denominator count (B27001_001). Filter like SASrename() way rename variables likingselect() drops variable called variableThe code addresses issue common census tables: may constructed way want . discussed , case values counts age group, want single count entire population county. Therefore, necessary sum across aggregate counts age groups get single count (numerator number uninsured) county.strategy used specific data long format, happens tidy data case. Read changing long wide .code achieves steps:filter() using != mean “equal ”; simply removes denominator variable, summing numerator countsgroup_by() useful dplyr verb; similar using class SAS, tells R something separately group (e.g. GEOID county case)summarise() verb works hand--hand group_by(). grouping declares groups, summarise() tells . case just want count uninsured across age gruops.simple merge, worth mentioning steps:left_join() one famiy merging verbs. left left_join() simply means start first table (one left) merge second table. implications whether rows rows left right (first second) table retained. case left first table insure_denom right second table insure_num)mutate() calculates uninsured prevalenceselect() excludes unnecessary variablesThe code process complex. specific exact scenario, scenario might require different steps. challenge , new spatial analyst, think mind data looks beginning want look end. create sequence steps progresses beginning end. takes practice, worthwhile spatial epidemiology, also data science processing generally.","code":"\n# First, request the data from ACS\ninsure_tidy <- get_acs(geography = 'county',\n                     variables = myVars,\n                     year = 2018, \n                     survey = 'acs5') %>%\n  select(-moe)\n\n# Look at the resulting object\nhead(insure_tidy)## # A tibble: 6 x 4\n##   GEOID NAME                    variable   estimate\n##   <chr> <chr>                   <chr>         <dbl>\n## 1 01001 Autauga County, Alabama B27001_001    54277\n## 2 01001 Autauga County, Alabama B27001_005       36\n## 3 01001 Autauga County, Alabama B27001_008      157\n## 4 01001 Autauga County, Alabama B27001_011      397\n## 5 01001 Autauga County, Alabama B27001_014      354\n## 6 01001 Autauga County, Alabama B27001_017      500\n# Now I pull out the denominator\ninsure_denom <- insure_tidy %>%\n  filter(variable == 'B27001_001') %>%\n  rename(TOTPOP = estimate) %>%\n  select(-variable)\n\n# Look at the resulting object\nhead(insure_denom)## # A tibble: 6 x 3\n##   GEOID NAME                    TOTPOP\n##   <chr> <chr>                    <dbl>\n## 1 01001 Autauga County, Alabama  54277\n## 2 01003 Baldwin County, Alabama 205452\n## 3 01005 Barbour County, Alabama  22882\n## 4 01007 Bibb County, Alabama     20468\n## 5 01009 Blount County, Alabama   57169\n## 6 01011 Bullock County, Alabama   9978\n# Now I sum up all the variables for the numerator\ninsure_num <- insure_tidy %>%\n  filter(variable != 'B27001_001') %>%\n  group_by(GEOID) %>%\n  summarise(no_insure = sum(estimate))\n\nhead(insure_num)## # A tibble: 6 x 2\n##   GEOID no_insure\n##   <chr>     <dbl>\n## 1 01001      3875\n## 2 01003     20864\n## 3 01005      2558\n## 4 01007      1619\n## 5 01009      6303\n## 6 01011      1076\n# Finally, merge the numerator and denominator in order to calculate prevalence\nuninsured <- insure_denom %>%\n  left_join(insure_num, by = 'GEOID') %>%\n  mutate(uninsured = no_insure / TOTPOP) %>%\n  select(GEOID, uninsured)\n\n# Take a look at the resulting object\nhead(uninsured)## # A tibble: 6 x 2\n##   GEOID uninsured\n##   <chr>     <dbl>\n## 1 01001    0.0714\n## 2 01003    0.102 \n## 3 01005    0.112 \n## 4 01007    0.0791\n## 5 01009    0.110 \n## 6 01011    0.108"},{"path":"cartography-for-epidemiology-i.html","id":"obtaining-and-preparing-geographic-data","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.3 Obtaining and preparing geographic data","text":"final type data needed geographic geometry data. , source geometry data varies study specifics: may need polygons (e.g. political administrative boundaries), lines (e.g. transportation networks), points (e.g. hospitals, food stores, toxic waste sites, etc). hand may need data raster format, including weather air pollution surfaces. open-access versions many types geographic data online.choropleth mapping, area units including administrative political boundaries commonly used. U.S. context, Census geographies frequently used, including blocks, block groups, tracts, zip-code tabulation areas, counties, cities & places, metropolitan areas, tribal areas, states, regions. section provide brief introduction downloading census boundary files directly R.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"obtain-geometry-data-from-tidycensus","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.3.1 Obtain geometry data from tidycensus","text":"first option minor modification code previous section acquiring census count data. get_acs() function argument geometry = FALSE default. However, change geometry = TRUE, automatically retrieve data sf object including geometry column!One argument get_acs() demonstrated shift_geo. FALSE default, set shift_geo = TRUE, return boundaries projected Albers Equal Area, states Hawaii Alaska artificially shifted fit thematic map U.S.","code":"\ninsure_tidy <- get_acs(geography = 'county',\n                     variables = myVars,\n                     year = 2018, \n                     geometry = TRUE,   # added geometry = T\n                     survey = 'acs5') "},{"path":"cartography-for-epidemiology-i.html","id":"obtain-geometry-data-from-tigris","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.3.2 Obtain geometry data from tigris","text":"tidycensus package actually requests geometry depending another package called tigris (Census geography files called TIGER files). obtaining attributes (e.g. population counts) geometries time, tidycensus package makes sense. However, sometimes need geometry, perhaps data come sources Census Bureau.want directly obtain areal boundary units, coastline data, road rail networks, voting districts, spatial data maintained Census Bureau, consider using tigris package. Try looking help documentation (e.g. ?tigris, click Index link bottom see options).demonstrate retrieving U.S. county boundaries:code :counties() function one dozens tigris downloading specific kinds boundary datacb = TRUE adjusts level detail resolution boundaries. default cb = FALSE returns detailed data, quite large. Setting cb = TRUE defaults generalized (1:500k scale) shape.resolution = '5m' specification want even generalized boundary file. 1:5 million scale coarse terms resolution curves county boundaries, also smaller file. must decide balance file size resolution specific need.year = 2018 specifies vintage boundary files. Tracts, counties, cities, etc change boundaries year year.class = 'sf' results object returned sf object, rather sp class data (default).can see summary data CRS/projection EPSG code 4269 (unprojected).boundary file look like?Census boundaries include information U.S. counties territories! Therefore map looks way Guam, American Samoa, Puerto Rico, well Hawaii Alaska included. interested mapping “lower 48” contiguous states, exclude . code , also transform project data Albers Equal Area using EPSG code","code":"\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\nus <- counties(cb = TRUE,\n                        resolution = '5m', \n                        year = 2018,\n                        class = 'sf')\nsummary(us)##    STATEFP            COUNTYFP           COUNTYNS           AFFGEOID        \n##  Length:3233        Length:3233        Length:3233        Length:3233       \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##     GEOID               NAME               LSAD               ALAND          \n##  Length:3233        Length:3233        Length:3233        Min.   :8.209e+04  \n##  Class :character   Class :character   Class :character   1st Qu.:1.079e+09  \n##  Mode  :character   Mode  :character   Mode  :character   Median :1.563e+09  \n##                                                           Mean   :2.833e+09  \n##                                                           3rd Qu.:2.367e+09  \n##                                                           Max.   :3.770e+11  \n##      AWATER                   geometry   \n##  Min.   :0.000e+00   MULTIPOLYGON :3233  \n##  1st Qu.:7.038e+06   epsg:4269    :   0  \n##  Median :1.950e+07   +proj=long...:   0  \n##  Mean   :2.161e+08                       \n##  3rd Qu.:6.159e+07                       \n##  Max.   :2.599e+10\nplot(st_geometry(us))\nus <- us %>%\n  filter(!(STATEFP %in% c('02', '15', '66', '60', '78', '72', '69'))) %>%\n  select(GEOID, STATEFP, COUNTYFP, NAME) %>%\n  st_transform(5070)\n\nplot(st_geometry(us))"},{"path":"cartography-for-epidemiology-i.html","id":"merging-attributes-and-geography","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.4 Merging Attributes and Geography","text":"final step data preparation bringing together attribute data geometry data, assuming already incorporated. Assuming attributes data.frame (perhaps tibble, tidyverse data table object), geometry sf object (also class data.frame), merge straightforward. needed merging joining data:Unique key ID variable attribute data matches ID geometry dataUnique key ID variable geometry data matches ID attribute dataMatching ID’s require variable name require variable type.merging several datasets, one sf object, put dataset first sequence, insure final object remains class sf. put sf first, may need re-define object sf end. See Appendix st_as_sf() detail.","code":"\nus2 <- us %>%\n  left_join(death, by = c('GEOID' = 'FIPS')) %>%\n  left_join(uninsured, by = 'GEOID')"},{"path":"cartography-for-epidemiology-i.html","id":"mapping-mortality-uninsured","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.5 Mapping Mortality & Uninsured","text":"","code":"\nlibrary(tmap)\n\nt1 <- tm_shape(us2) + \n  tm_fill('crude',\n          style = 'quantile',\n          palette = 'BuPu',\n          title = 'Rate per 100,000 py') + \n  tm_borders(alpha = 0.2) +\ntm_credits('Source: CDC Wonder',\n           position = c('RIGHT', 'BOTTOM')) + \ntm_layout(main.title = 'All-cause mortality rate, 2016-2018',\n          bg.color = 'grey85')\n\nt2 <- tm_shape(us2) + \n  tm_fill('uninsured',\n          style = 'quantile',\n          palette = 'YlOrRd',\n          title = '% Uninsured',\n          legend.format = list(fun=function(x) paste0(formatC(x * 100, \n                                                              digits=1, \n                                                              format=\"f\"), \"%\"))) +\n  tm_borders(alpha = 0.2) +\ntm_credits('Source: American Community Survey',\n           position = c('RIGHT', 'BOTTOM')) + \ntm_layout(main.title = 'Uninsured Prevalence, 2014-2018',\n          bg.color = 'grey85')\n\n\ntmap_arrange(t1, t2, ncol = 1)"},{"path":"cartography-for-epidemiology-ii.html","id":"cartography-for-epidemiology-ii","chapter":"Week 3 Cartography for Epidemiology II","heading":"Week 3 Cartography for Epidemiology II","text":"","code":""},{"path":"cartography-for-epidemiology-ii.html","id":"learning-objectives-2","chapter":"Week 3 Cartography for Epidemiology II","heading":"3.1 Learning objectives","text":"","code":""},{"path":"cartography-for-epidemiology-ii.html","id":"additional-resources-2","chapter":"Week 3 Cartography for Epidemiology II","heading":"3.2 Additional resources","text":"Report confidentiality issues policies related geospatial data public health applicatins","code":""},{"path":"cartography-for-epidemiology-ii.html","id":"important-vocabulary-2","chapter":"Week 3 Cartography for Epidemiology II","heading":"3.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 3","code":""},{"path":"cartography-for-epidemiology-ii.html","id":"spatial-thinking-in-epidemiology-w3","chapter":"Week 3 Cartography for Epidemiology II","heading":"3.4 Spatial Thinking in Epidemiology, w3","text":"“Progress achieving health depends upon effectively collecting, integrating, \nutilizing medical, public health, socioeconomic, environmental, physical science data.”“Although new technological advances can empower individuals neighborhoods seeking\nresources better health care, also heightened concerns individual privacy\nconfidentiality.”– Confidentiality Issues Policies Related Utilization Dissemination Geospatial Data Public Health ApplicationsEthical concern justice, beneficence, respect persons ground guidelines practices responsible conduct public health research. work geospatial data concerns lessened instead often heightened, power locational information means discerning private information risk intended unintended breaches confidentiality even transmission stigma groups highlighting health status marginalized populations.","code":""},{"path":"cartography-for-epidemiology-ii.html","id":"risks-of-privacy-breaches-in-collection-of-geospatial-information","chapter":"Week 3 Cartography for Epidemiology II","heading":"3.4.1 Risks of privacy breaches in collection of geospatial information","text":"Geographic identifiers scale state (e.g. county, city, census tract, address) considered Protected Health Information  HIPAA connected individual health information. Surveillance research activities routinely collect geospatial information contact notification purposes, reporting, although many consent forms explicitly explain intended purpose use geospatial information.individual expect protection privacy individual PHI date birth name, always explicit information address can uniquely identifiable linkable data. Privacy breached app-based geocodes captured without consent, geospatial information collected without express consent (e.g. research respondent asked report address someone social network without persons consent).respect personal autonomy dictates individuals permitted control private information, can also risks beyond breach privacy. instances, disclosed geospatial information result harms participant others. example collected address information inadvertently released someone seeking commit violence (e.g case intimate partner violence). Similarly, studies collecting geospatial information can () requested force law aid investigation prosecution suspected crimes. Thus collection geospatial information must well reasoned respect risk benefit participant, appropriate notification consenting process, protections place maintain confidentiality.","code":""},{"path":"cartography-for-epidemiology-ii.html","id":"risk-of-confidentiality-breaches-through-unintentional-de-identification","chapter":"Week 3 Cartography for Epidemiology II","heading":"3.4.2 Risk of confidentiality breaches through unintentional de-identification","text":"private geospatial data collected, responsibility data owners (e.g. public health agencies, researchers) protect confidentiality disclosed private information. Confidentiality protection refers secure control confidential data well avoidance unintended re-identification data deemed ‘de-identified’ data linkages.Maintaining data security critical public health research surveillance activities, sometimes geospatial data ignored unique identifier. one instance submitted data request public health agency obtain surveillance data abortion incidence. data delivered Excel sheet individual identifiers name date birth removed, field address residence included. address incredibly powerful unique identifier, particularly combined fields including age sex.Geospatial data can stored separately research attributes, maintaining key linkage instances spatial data needed. needed, less risk accidental disclosure fields.Another risk drives many public health agencies restrictive guidelines around data suppression reporting, concern re-identification individuals aggregated data small cell size ability discern identity quasi-identifiers. example, age, race, ethnicity, health outcome quasi-identifiers instances cross-tabulation make individuals unique nearly .study 1990 decennial census, researchers found 87% U.S. population uniquely identified three variables: exact date birth, zip code, gender! due part combined granularity specificity two variables: date birth zip code. instances, reporting health events zip code level without respect age, perhaps age categorized coarse groups eliminate risk. take home message stratification data prefer better scientific understanding can quickly lead least sub-groups individually nearly individually identifiable.","code":""},{"path":"cartography-for-epidemiology-ii.html","id":"risk-of-stigmatization-of-place","chapter":"Week 3 Cartography for Epidemiology II","heading":"3.4.3 Risk of stigmatization of place","text":"final ethical concern particularly relevant disease mapping activities concern unintentional harm persons populations stigmatization place. can happen map identifies locations marginalized populations spend time, serves either stigmatize group, stigmatize others unassociated group, sharing location. stigmatization can lead psychosocial harms, also can alter behavior institutional forces including social services, law enforcement, health services.Examples stigmatization place include identification venues men sex men seek partners, mapping concentrations commercial sex workers injection drug users. concern stigmatization place also raised point view social epidemiology, predominantly Black brown neighborhoods repeatedly characterized ‘unhealthy.’ potential harm perpetrated maps arise (presumably well-intended) desire highlight unjust burdens, failure similarly highlight resilience face burdens.Relatedly, many spatial representations economic racial disparities fail name factors give rise inequities, including role socio-historical structural discrimination. failing name structural racism policies serve concentrate affluence separately concentrated poverty, maps contribute narrative communities way blame health outcomes.","code":""},{"path":"disease-mapping-i.html","id":"disease-mapping-i","chapter":"Week 4 Disease Mapping I","heading":"Week 4 Disease Mapping I","text":"","code":""},{"path":"disease-mapping-i.html","id":"getting-ready-2","chapter":"Week 4 Disease Mapping I","heading":"4.1 Getting ready","text":"","code":""},{"path":"disease-mapping-i.html","id":"learning-objectives-3","chapter":"Week 4 Disease Mapping I","heading":"4.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"disease-mapping-i.html","id":"additional-resources-3","chapter":"Week 4 Disease Mapping I","heading":"4.1.2 Additional Resources","text":"Arianna Planey blog spatial thinking MAUPWaller L, Gotway C. Applied Spatial Statistics Public Health Data. Hoboken, NJ: John Wiley & Sons, Inc; 2004.Clayton D, Kaldor J. Empirical Bayes estimates age-standardized relative risks use disease mapping. Biometrics. 1987 Sep;43(3):671–81.","code":""},{"path":"disease-mapping-i.html","id":"important-vocabulary-3","chapter":"Week 4 Disease Mapping I","heading":"4.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 4","code":""},{"path":"disease-mapping-i.html","id":"spatial-thinking-in-epidemiology-2","chapter":"Week 4 Disease Mapping I","heading":"4.2 Spatial Thinking in Epidemiology","text":"Disease mapping located intersection statistics, geography, epidemiology. Whereas ---box GIS approach making maps health statistics (e.g. ’ve referring epidemiologic cartography) takes raw data simply shows map, disease mapping typically implies interested going beyond just making pretty maps. Instead driven core epidemiologic questions concerned fundamental epidemiologic statistical issues.","code":""},{"path":"disease-mapping-i.html","id":"why-do-we-need-disease-mapping","chapter":"Week 4 Disease Mapping I","heading":"4.2.1 Why do we need disease mapping?","text":"defining driver purpose epidemiology interest characterizing estimating distribution determinants health populations. Disease mapping primarily focused former (distribution health), providing novel insight geographic patterns health disease. latter (determinants health) can begin addressed Modules 3 4 course focusing Clustering Spatial Regression.spatially describe distribution disease, epidemiologists primarily interested two -arching question:intensity disease health spatially heterogeneous spatially homogeneous?spatial structure spatial correlation location disease intensity?Spatial heterogeneity means differences intensity disease sub-regions compared others. Another way saying , local parameter (e.g. rate, risk, prevalence, etc) least sub-regions study area different global parameter (e.g. overall rate, risk, prevalence, etc) entire study area.contrast, spatial homogeneity means know overall, global parameter, basically know every local parameter, plus minus random variation. Looking heterogeneity whole reason mapping! occurrence disease everywhere, map tell us much! previous weeks mapped disease, epidemiologic cartography efforts date fall short attend following three challenges:Parameter estimate instability due sparse data/rare events;Spurious heterogeneity arising ‘confounding’ nuisance covariates;Biased description spatial heterogeneity arising modifiable areal unit problem (MAUP), form ecologic fallacyThe following three sections provide additional detail challenges.","code":""},{"path":"disease-mapping-i.html","id":"the-problem-and-approach-to-data-sparsity","chapter":"Week 4 Disease Mapping I","heading":"4.2.1.1 The problem and approach to data sparsity","text":"Ideally like maps visualize values target parameter (e.g. risk, rate, prevalence, SMR, etc) accurate, reliable, precise. words don’t want map shows differences simply result random error due small populations small number events.Therefore, reliable precise estimation epidemiologic parameters requires sufficient data (e.g. sample size region mapped) produce summary. either disease quite rare – resulting small numerator – population risk quite sparse – resulting small denominator – estimate disease burden inherently unstable. means adding just one two events persons risk notable impact estimate.example, imagine county 10 people risk death three consecutive years (e.g. die, others born move county). one year, perhaps none die, next year one dies, third year three die. mortality rate estimated 0%, 10% 30%. find implausible actual risk death 0% location; also exceptional actual mortality risk 10%. words estimates mathematically true given year, statistically unstable, frankly implausible. problem estimate mortality rate derived little data.practice, public health agencies often suppress data counts small, concern confidentiality, also resulting estimates unstable potentially misleading. already discussed two approaches address data sparsity resulting parameter instability imprecision:Aggregate counts disease events population risk multiple years time intervals increase opportunity events, extend amount person-timeAggregate counts disease events population risk geographic units pool together larger populations. example data mortality may sparse census tract level might stable pooling tracts respective county level.spend next several weeks exploring range methods together constitute third option: statistical smoothing stabilization. tools use amount information (function sample size) smooth extreme highs extreme lows effort recover plausible ‘true’ amount spatial heterogeneity. critical goal disease rate stabilization smooth necessary, true highs lows persist, spurious unstable values adjusted.week use aspatial global Empirical Bayes estimators first approach parameter stabilization. future weeks explore spatial Empirical Bayes, kernel density estimators, fully Bayesian estimators additional strategies producing maps highlights signal spatial heterogeneity net noise random error.","code":""},{"path":"disease-mapping-i.html","id":"the-problem-and-approach-to-confounding","chapter":"Week 4 Disease Mapping I","heading":"4.2.1.2 The problem and approach to confounding","text":"Confounding epidemiology refers specific causal structure, wherein association putative exposure target disease outcome spuriously associated backdoor path one confounders.disease mapping formal ‘exposure,’ place perhaps stand-unmeasured attributes vary space. Therefore probably call confounding strictest sense word.Instead can imagine covariates simply nuisance. means explain amount spatial heterogeneity, epidemiologist particularly interested role explanation; instead wish know still spatial heterogeneity beyond covariates. example consider comparison mortality rates state:Using crude mortality rate, clear Florida mortality rate perhaps 30% higher Alaska, suggesting something really awful going Florida compared Alaska! However adjust standardize age, actually Alaska slightly higher mortality rate. Depending purpose numbers useful, mapping mortality across states, might think differences age-structure (e.g. many retirees Florida Alaska) nuisance accomplishing goal; disease mapping age-adjusted estimate may useful.strategies spatial epidemiology addressing confounding (e.g. removing effects nuisance variables) similar non-spatial epidemiology. Standardization, stratification, regression control conventional tools. disease mapping quite common use standardization tool balance condition one covariates, age. However methods including fully Bayesian models later spatial regression models, possible control multiple covariates.","code":""},{"path":"disease-mapping-i.html","id":"the-problem-and-approach-to-maup","chapter":"Week 4 Disease Mapping I","heading":"4.2.1.3 The problem and approach to MAUP","text":"interesting article Flint water lead crisis, geographer, Richard Sadler, describes mapping lead-level data Flint early process. alarms raised high levels lead Flint, state-based reporting identify detect anomalies. geographer points , likely state-based reporting based (aggregated ) zip codes. zip codes ideal geographic units disease mapping, may apparent exactly zip codes led public health officials astray Flint. look map zip code boundaries overlaid city boundaries.\nFIGURE 2.1: Zip code boundaries Flint, Michigan\ncan see, seven zip codes Flint area, two fully contained within city limits. others seem evenly split areas inside city limits outside city. became important water system issues produced excess lead exposure constrained households inside city limits. net result aggregation events (high blood lead levels) population risk within zip code area contained mix truly exposed unexposed households. zip code reporting masked obscured true elevations lead poisoning prevalence, diluting early warnings problem.powerful example concern referred geographers modifiable areal unit problem (MAUP). Epidemiologists may familiar related idea: ecologic fallacy ecologic bias. problem inherently aggregation. Instead problem arises way data aggregated results mixing different types people, producing kind cross-level confounding. Flint meant diluting population people exposed clean water, also result enriching specific region people confounding risk factors, producing spurious estimate true experience health within area.two ways MAUP can occur:Arbitrary zoning boundaries create aggregates. case Flint, one (arbitrary) zoning system (zip codes) applied different zoning system (e.g. municipal city boundaries). result mis-alignment actually happening way count .Arbitrary scale level aggregation. occurs aggregate level scale different level scale population health generated. single ‘right’ scale. depends process interest. ‘correct’ scale understanding effect Medicaid expansion ACA (e.g. probably states good scale) likely different ‘correct’ best scale understanding role healthy commercial food retailers obesity (e.g. probably city neighborhood scale appropriate).One key take away discussion bias MAUP arises way carry analysis align way health occurs. words, aggregation zoning similarly harmful. work spatial epidemiologist consider aligned (mis-aligned) available aggregation respect hypothetical process. Sometimes possible explore sensitivity results choice scale zoning repeating analyses alternative boundaries scales.","code":""},{"path":"disease-mapping-i.html","id":"using-statistics-and-probability-models-to-improve-disease-mapping","chapter":"Week 4 Disease Mapping I","heading":"4.2.2 Using statistics and probability models to improve disease mapping","text":"epidemiology, spend lot time trying disentangle ‘noise’ ‘signal’ collections data relationships. evident focus two broad buckets error: random error comes chance related sample size; systematic error comes structural bias (e.g. confounding, selection, misclassification/measurement error) driven sample size therefore fixed increasing sample size).make inference (make meaning decisions) data take account random error adopt statistical probability models describe role chance alone generating values. instance many statistics operate assumptions related Gaussian normal distributions. also rely Poisson binomial distributions evaluate variation differences count binary data respectively.","code":""},{"path":"disease-mapping-i.html","id":"how-are-statistics-different-in-space","chapter":"Week 4 Disease Mapping I","heading":"4.2.2.1 How are statistics different in space?","text":"Spatial statistics huge field, well broader cover week, entire semester. However worth introducing key ideas motivate statistics using.Health outcome events typically occur level individual, individuals can referenced respect location space. Consider, example study region represented blue square image . population distributed across region composed individuals occupying particular \\(x,y\\) location. population defined geographic bounds, may individuals experiencing health event. observe (subset) individuals corresponding point locations point time. observation represents specific realization spatial point process. words can imagine individual random chance experiencing event, set events indexed location one realization version random process evident space.\nFIGURE 2.2: Spatial point process\ndescribe quantify observed describe spatial disease intensity event spatially continuous surface. words every location, intensity amount disease per unit-area. calculate single, global, measure spatial intensity figure divide events area:\\(\\frac{events}{Area}=\\frac{14}{4km^{2}}=\\frac{3.5}{km^{2}}\\)simplistic case assumed population risk evenly distributed across study region. realistically, can normalize events spatially-varying population risk quantify spatial intensity disease.\nFIGURE 2.3: Approximating intensity areal aggregates\noften exact \\(x,y\\) location every person risk every health event, observe full spatial point process thus estimate continuous spatial intensity surface. However, can approximate spatial intensity aggregating health events population summarizing ratio (e.g. risk, rate, prevalence) per areal unit. figure , rectangle contains \\(n=100\\) person-years risk, producing following disease rates estimating spatial intensity disease:data form (e.g. counts events counts population), can use one several parametric statistical probability distributions common epidemiology including Poisson, binomial, negative binomial.probability distributions useful?probability distribution mathematical expression might expect happen simply due random chance; said another way, probability distribution describes expectations null hypothesis. choose different probability distributions suit different types data continuous, binary, count.Relating count disease events probability distribution permits calculation standard errors confidence intervals expressing precision certainty estimate. Alternatively calculate p-value means test evidence consistency null hypothesis.brief summary probability distributions common disease mapping:formulas :\\(Y_i\\) count health events \\(i_{th}\\) areal unit\\(N_i\\) count population risk \\(i_{th}\\) areal unit\\(r_i\\) risk \\(i_{th}\\) areal unit (e.g. estimated \\(Y_i / N_i\\))\\(E_i\\) expected count, calculated \\(N_i\\times r\\), \\(r\\) overall reference level risk (note subscript \\(_i\\); means global risk rather risk local region \\(_i\\). expected simply means burden disease \\(i_{th}\\) areal unit experienced reference risk.\\(\\theta_i\\) relative risk \\(i_{th}\\) areal unit; essentially relative deviation region expected.Don’t panic looking formulas. take away points:Poisson distribution classic distribution use evaluating counts events possibly offsetting time--risk person-years.\nPoisson assumes mean distribution variance distribution.\nPoisson distribution approximates disease intensity rate well rare disease processes. Therefore Poisson good choice outcome rare.\nPoisson assumes mean distribution variance distribution.Poisson distribution approximates disease intensity rate well rare disease processes. Therefore Poisson good choice outcome rare.Binomial distribution useful characterizing disease occurrence non-rare common disease processes.Poisson-gamma Mixture may foreign. However, may heard Negative Binomial distribution count data? Poisson-gamma mixture essentially negative binomial model. probability distribution like Poisson, except without expectation mean equal variance. words robust called -dispersion, variation count greater expected Poisson.\n-dispersion quite common spatial epidemiology often unobserved factors driving occurrence disease area, unobserved differences produce event intensity strictly Poisson nature. use Poisson-gamma reason.\n-dispersion quite common spatial epidemiology often unobserved factors driving occurrence disease area, unobserved differences produce event intensity strictly Poisson nature. use Poisson-gamma reason.want learn Poisson point processes probability distributions spatial epidemiology, highly recommend Lance Waller’s text, Applied Spatial Statistics Public Health Data (Waller & Gotway, 2004). available electronically via Woodruff Library.","code":""},{"path":"disease-mapping-i.html","id":"spatial-analysis-in-epidemiology-2","chapter":"Week 4 Disease Mapping I","heading":"4.3 Spatial Analysis in Epidemiology","text":"example dataset, next four weeks disease mapping aim estimate spatial heterogeneity county level occurrence low birthweight (VLBW; weight birth < 1500 grams) babies 2018-2019. data derived Georgia OASIS website. outcome public health importance high morbidity mortality associated born early small. However, overall rate VLBW 1.8%, rare outcome, likely sparse data many rural counties.maps , can visualize observed VLBW prevalence well prevalence restricted counties meeting NCHS suppression rule natality records (e.g. suppress cell sub-population reporting count < 10). map right 85 159 counties Georgia suppressed data. suggests , even know values (e.g. aren’t suppressed) thinking issues imprecision instability estimates (therefore map overall) many counties sparse data.four disease mapping objectives wish accomplish fully describe data:Test whether statistical evidence spatial heterogeneity versus homogeneityDescribe precision VLBW prevalence estimates countyAccount possibly spurious nuisance differences counties due confounding covariate maternal age structureProduce overall covariate-adjusted smoothed stabilized rate estimates using global Empirical Bayes.","code":""},{"path":"disease-mapping-i.html","id":"disease-mapping-is-there-spatial-heterogeneity","chapter":"Week 4 Disease Mapping I","heading":"4.3.1 Disease mapping: Is there spatial heterogeneity?","text":"","code":""},{"path":"disease-mapping-i.html","id":"calculating-expected-counts-and-the-smr","chapter":"Week 4 Disease Mapping I","heading":"4.3.1.1 Calculating expected counts and the SMR","text":"now primarily represented disease burden using risks, rates, prevalence. However, introduce statistical estimation Poisson Poisson-gamma (negative binomial), often testing whether disease intensity (risk, rate, prevalence) given geographic area deviates expected value, expected value might considered average risk/rate/prevalence entire study region. natural way represent deviation using Standardized Morbidity Ratios (SMRs):\\(SMR_i=\\frac{Y_i}{E_i}\\)standardized morbidity ratio (also standardized mortality, incidence, prevalence depending counted) measure relative excess risk. quantifies deviation population parameter (case live birth prevalence low birthweight geographically-defined population) reference value (case VLBW risk whole state Georgia). SMR calculated Observed count events, \\(Y_i\\), divided Expected count, \\(E_i\\), events.Calculating expected counts VLBW events data straightforward: first calculate overall risk, \\(r\\), multiply population risk county, \\(N_i\\), get events expected homogeneity risk, \\(SMR=1.0\\) counties.can see maps , SMR represents underlying pattern, simply different scale, relative excess risk rather absolute risk.Spatial heterogeneityBoth maps illustrate (qualitatively least) spatial heterogeneity variation VLBW prevalence. absolute scale lets us see absolute burden varies one part state compared another. relative scale SMR specifically highlights counties one three types:Better average means lower risk average, statewide populationWorse average means higher risk average, statewide populationAbout average meaning risk expected given statewide prevalence","code":"\n# the overall ratio of events to population is the global risk\nrisk <- sum(vlbw$VLBW) / sum(vlbw$TOT) \n\n\n# Now add a variable to the dataset representing expected count and SMR\nvlbw <- vlbw %>%\n  mutate(expect = risk * TOT,  # calculate the EXPECTED count for each county\n         SMR = VLBW / expect)  # calcualte the SMR as observed/expected"},{"path":"disease-mapping-i.html","id":"testing-for-spatial-heterogeneity","chapter":"Week 4 Disease Mapping I","heading":"4.3.1.2 Testing for spatial heterogeneity","text":"Perhaps fundamental purpose disease mapping describe represent magnitude patterns spatial heterogeneity variation health across sub-areas study region.isn’t real variation!? instance consider scenarios:numerical values disease intensity mapped practically regions, use arbitrarily narrow cutpoints make regions look different cartographically.appears large differences values sub-areas, counts sparse possible seeming difference simply due chanceFor reasons sensible start evaluating evidence versus heterogeneity. none, little reason proceed spatial analysis. Luckily standard statistical tests designed just purpose: evaluate whether count events significantly different across observations, accounting number trials persons risk.R package DCluster function chi-square test optimized needs aggregated data spatial epidemiology. test called achisq.test() can evaluate variation numerator denominator Poisson Negative Binomial (recall Poisson-gamma) distribution. sf data object containing VLBW information called vlbw; within sf object column named VLBW representing count babies born low birthweight county, another variable named TOT representing count live births. language Poisson, \\(Y_i\\) count variable VLBW county, evaluate count offset log number births risk.Look help documentation function; specify statistical model Poisson. argument R=499 refers number random permutations use calculating empirical p-value.null hypothesis relative risk SMR equal one counties. words, null, significant difference risk counties. Based 499 simulated permutations null, observed data appear quite inconsistent null assumption, evidenced p.value = 0.002. words strictly Poisson probability model, appears significant spatial heterogeneity risk VLBW.conventional statistics often closed form formulas calculating standard errors, confidence intervals p-values. However, spatial statistics simple parametric assumptions always hold. One empirical alternative closed-form formula use random permutations data simulate random data null hypothesis.case achisq.test(), null hypothesis observed count equal expected count. Random permutations take random Poisson draw count county null. repeat hundreds times, distribution random chance produce. compare actual observed values distribution. observed values different set random values, might say evidence null.happen allowed distribution null Negative Binomial (e.g. Poisson-Gamma) rather strictly Poisson? specify re-calculate p-value testing evidence significant heterogeneity:assumption seems give us entirely different picture going ! always occur (e.g. many instances test heterogeneity either Poisson Negative Binomial result consistent determination statistical significance), also complete surprise. two points worth making comparison two results.First, understand possible might help visualize probability distributions fix minds ‘-dispersion’ ‘extra-Poisson variance’ mean. plot 10,000 draws two random distributions, Poisson Negative Binomial. , mean expectation null \\(10\\) events, indicated blue dotted line. left panel histogram many events occurred (assuming expected mean \\(n=10\\)) Negative Binomial, right panel shows distribution random draws Poisson.Negative Binomial distribution fatter, especially right-tail. means even null/expectation \\(n=10\\) true, expect wider range counts (including instances high counts) chance alone Negative Binomial compared Poisson.second point worth making early step – testing aspatial heterogeneity – just : first look. many reasons data behave variance excess Poisson expectation. -dispersion can arise important missing variables predict outcome event vary spatially. quite common. evidence spatial heterogeneity either distribution, might consider throwing towel now. However, given evidence unusual behavior Poisson expectation suggests exploration might worthwhile. However, clearly consider using Poisson-Gamma approach subsequent analyses including Empirical Bayesian smoothing, .","code":"\nDCluster::achisq.test(VLBW~offset(log(TOT)), \n                      data = vlbw, \n                      model = 'poisson',\n                      R = 499)## Chi-square test for overdispersion \n## \n##  Type of boots.: parametric \n##  Model used when sampling: Poisson \n##  Number of simulations: 499 \n##  Statistic:  416.6378 \n##  p-value :  0.002\nDCluster::achisq.test(VLBW~offset(log(TOT)), \n                      data = vlbw, \n                      model = 'negbin',\n                      R = 499)## Chi-square test for overdispersion \n## \n##  Type of boots.: parametric \n##  Model used when sampling: Negative Binomial \n##  Number of simulations: 499 \n##  Statistic:  416.6378 \n##  p-value :  0.822"},{"path":"disease-mapping-i.html","id":"disease-mapping-how-precise-are-county-estimates","chapter":"Week 4 Disease Mapping I","heading":"4.3.2 Disease mapping: How precise are county estimates?","text":"Following question whether global spatial heterogeneity (e.g. least counties \\(SMR\\neq 1.0\\)), natural follow confident precise estimates , counties statistically significantly different null expectation?function estimate continuous p-value associated SMR probmap function package spdep. function calculates probabilities (Poisson probability model) observing event count extreme actually observed, given expected count (e.g. might expect every county overall risk). test one-tailed test, default alternative hypothesis observed less expected, SMR <1.0 (test extremes greater 1.0, set argument alternative = 'greater').frequentist statistics familiar focusing small p-values evidence reject null hypothesis. case continuous p-value returned probmap, can think probabilities either side spectrum. instance, default alternative = 'less', probability \\(p<0.05\\) indicate \\(SMR<1\\) statistically significant (\\(\\alpha=0.05\\)). contrast, \\(p>0.95\\) suggest unusual finding, null, \\(SMR>1\\). \\(p>0.95\\) alternative = 'less' therefore equivalence \\(p<0.05\\) alternative = 'greater' describing significance \\(SMR>1\\).probmap expects several arguments including vector count cases, vector population risk, optionally row.names vector help align observations. objective identify counties SMR excess expected (e.g. >>1), easier interpret change alternative hypothesis one-sided test alternative = 'greater'.function returns expected count VLBW births (yet another way get number!), well SMR (case named relRisk, somewhat oddly function multiplies SMR 100 numbers appear different!), Poisson probability observed count ‘extreme’ actually observed.can see, function calculates:Raw rate, simply \\(\\frac{Y_i}{N_i}\\)Expected count, simply \\(r\\times N_i\\), \\(r\\) overall expected rate based VLBW births Georgia datasetRelative risk, SMR ratio observed expected. Note function multiplies SMR 100. value 103 actually refers SMR 1.03p-value, probability SMR county significantly greater 1.0For mapping, grab SMR (e.g. relRisk divided 100 make conventional) p-value term, pmap, can easily add sf object:","code":"\nlibrary(spdep)\nx <- probmap(n = vlbw$VLBW, x = vlbw$TOT, \n              row.names = vlbw$GEOID,\n             alternative = 'greater')\nhead(x) # look at what is returned##              raw   expCount   relRisk       pmap\n## 13121 0.01885984 424.339856 103.69047 0.22974886\n## 13029 0.01214953  19.461794  66.79754 0.95041991\n## 13135 0.01791989 414.117867  98.52267 0.62470508\n## 13127 0.02416255  33.121426 132.84452 0.04028446\n## 13271 0.01204819   4.528959  66.24038 0.82965314\n## 13279 0.01364256  13.332238  75.00616 0.85502966\nvlbw$pmap <- x$pmap\n\nvlbw$SMR <- x$relRisk / 100"},{"path":"disease-mapping-i.html","id":"mapping-the-p-value-for-the-smr","chapter":"Week 4 Disease Mapping I","heading":"4.3.2.1 Mapping the p-value for the SMR","text":"produce p-value map depicting continuous probability observe SMR extreme observed (specifically case, greater observed), assuming null described expected count true, use probability retrieved previous code map, next map SMR :interesting, perhaps useful quantify probabilities familiar thresholds. example use output probmap() function calculate custom p-value categories.preceding code designed make new spatial object (e.g. create object pv original county object vlbw) whose purpose represent boundaries shapes counties SMR statistically significantly greater 1. use new object map outline highlight statistically significant counties. notes code :REMEMBER: probmap carried 1-sided test, make align results confidence intervals identified counties extreme values either direction, implicitly two-sided, look counties p > 0.975. Thus mutated variable pmap.pv equal 1 county SMR>1 pmap value less 0.05. Otherwise pmap.pv equal zero.using group_by(pmap.pv) along summarise(), first separate counties significant (remember pmap.pv binary 1/0), merge dissolves adjacent counties category (e.g. significant significant). result spatial object general borders around sets significant counties rather around county separately.Finally, using filter(pmap.pv == 1) code removes counties significant. result object boundaries counties statistically significantly higher risk expected. can plot object map.","code":"\nsmr_map <- tm_shape(vlbw) +\n  tm_fill('SMR',\n          style = 'fixed', \n          palette = '-RdYlBu',\n          breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'VLBW in Georgia, 2018-2019',\n            inner.margins = c(0.02, 0.02,0.05,0.2))+\n  tm_shape(ga) +\n  tm_borders(lwd = 2, col = 'black')\n\nprob <- tm_shape(vlbw) + \n  tm_fill('pmap',\n          style = 'cont',\n          palette = 'PiYG',\n          n=7,\n          title = 'Prob SMR > 1\\nby chance alone') + \n  tm_borders() + \n  tm_layout(main.title = 'Probability Map',\n            inner.margins = c(0.02, 0.02,0.05,0.2))+\n  tm_shape(ga) +\n  tm_borders(lwd = 2, col = 'black')\n\ntmap_arrange(smr_map, prob)\npv <- vlbw %>%\n  mutate(pmap.pv = ifelse(SMR > 1 & pmap < 0.05, 1, 0)) %>%\n  group_by(pmap.pv) %>%\n  summarise() %>%\n  filter(pmap.pv == 1)\ntm_shape(vlbw) +\n  tm_fill('SMR',\n          style = 'fixed',\n          palette = '-RdYlBu',\n          breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'SMR of VLBW, GA 2018-2019',\n            inner.margins = c(0.1, 0.02,0.05,0.2)) +\n  # Add dark borders for significant\n  tm_shape(pv) +\n  tm_borders(lwd = 2, col = 'black') +\n  #tm_shape(ga) + \n  tm_borders(lwd = 1.5, col = 'black') +\n  tm_credits('Counties with higher than expected risk (p<0.05) highlighted with dark borders')+\n  tm_shape(ga) +\n  tm_borders(lwd = 1, col = 'black')"},{"path":"disease-mapping-i.html","id":"disease-mapping-adjusting-for-covariates","chapter":"Week 4 Disease Mapping I","heading":"4.3.3 Disease mapping: Adjusting for covariates","text":"SMR straightforward overall total, also possible calculate SMR adjusts covariate, maternal age, using indirect standardization. means apply reference rate within strata (e.g. age case) population--risk within county-age strata.may recall earlier classes (perhaps EPI 530) learned direct indirect age-standardization (familiar direct indirect standardization, helpful review old Epi Methods course notes refresher!).standardization may mentioned much since , tool adjust confounding, just might stratification \\(2\\times 2\\) tables, multivariable regression. way adjust individual-level covariates spatial analysis, common approach 1 perhaps 2 categorical covariates.Calculating expected count indirect standardization categorical variable (e.g. maternal age) requires data arranged row data within county representing count deaths age-strata. hand-calculate standardized expected counts, convenience function calculating expected counts using covariate strata may find easier.convenience function part SpatialEpi package. expected() function expects 3 arguments:vector count population risk, including row every covariate strata within every region;vector count number events cases (separately strata covariate region);number strata within region (e.g. many age covariate categories possible within county?)age-adjust vlbw data, need different object including counts county, age-category within county. Luckily can retrieve Georgia OASIS. object age structure vlbw except instead \\(159\\) rows \\(159\\) counties \\(1431\\) rows \\(159\\times 9\\) age categories. Said another way, data long.expected() function take covariate-stratified counts, calculate single expected count region (e.g. “many VLBW births expect maternal age structure mothers county \\(x\\) statewide maternal age structure?”). can used produce age-adjusted SMR’s. Notice output following function vlbw object, N=159 rows data, despite inputs (e.g. information right assignment arrow) age object, \\(1431\\) rows.might wonder whether age-adjustment impact. can see plot , showing unadjusted SMR versus age-adjusted, case indirect adjustment age created extreme outliers. may result stratifying already-sparse events even smaller cells, producing instability estimates. revisit global Empirical Bayes smoothing rate stabilization.","code":"##   GEOID    NAME  AGECAT VLBW TOT\n## 1 13001 Appling 10 - 14    0   1\n## 2 13001 Appling 15 - 17    2  12\n## 3 13001 Appling 18 - 19    0  30\n## 4 13001 Appling 20 - 24    4 149\n## 5 13001 Appling 25 - 29    4 141\n## 6 13001 Appling 30 - 34    0  72\nlibrary(SpatialEpi)\n# First, must insure that data are sorted by county and covariate category\nage <- age %>%\n  arrange(GEOID, AGECAT)\n\n# Calculate the age-adjusted expected counts\nvlbw$expected_indirect <- SpatialEpi::expected(population = age$TOT, \n                                          cases = age$VLBW,\n                                          n.strata = 9)\n# Remember, if you added 0.5 to observed above, do so here as well!\n\nvlbw$SMR_adj <- vlbw$VLBW / vlbw$expected_indirect"},{"path":"disease-mapping-i.html","id":"disease-mapping-rate-stabilization-with-global-empirical-bayes","chapter":"Week 4 Disease Mapping I","heading":"4.3.4 Disease mapping: Rate stabilization with global Empirical Bayes","text":"Everything covered focused representing precision/stability/certainty SMR observed data, possibly adjusted covariates. However, case VLBW (many small-area mapping projects), may want try extract signal underlying spatial trend data, net random noise induced small event counts widely varying population sizes. process sometimes referred small area estimation goes beyond just showing observed values, instead trying estimate underlying true trend.Empirical Bayes (EB) estimation one technique producing robust small area parameter estimates. EB estimation approach parameter shrinkage, wherein extreme estimates (e.g. SMR) judged reliable -reliable based variance, function number events. words county extreme SMR, small event count, SMR parameter less reliable. absence information, might guess extreme small event count try adjust, shrink, back range reasonable values.hand county relatively extreme SMR, many events, extreme value might deemed reliable. result, shrunk less. EB estimation just : uses overall average rate (SMR) global reference shrinks, adjusts, SMR towards global mean, inversely proportionate variance. ideal result true patterns persist, noise eliminated.DISCLAIMER: need understand Bayesian statistical theory work effectively EB estimators class. However provide superficial discussion happening want . less interested, focus code producing estimates credible intervals. really interested, likely superficial intro unsatisfying. can point resources desired!","code":""},{"path":"disease-mapping-i.html","id":"a-bit-about-bayes","chapter":"Week 4 Disease Mapping I","heading":"4.3.4.1 A bit about Bayes…","text":"may learned Bayes Theorem statistics, may gone much . Bayesian statistics take slightly different perspective analysis inference compared frequentist statistics underlying conventionally use.\nFIGURE 4.1: Bayes Theorem\nBayes theorem familiar likelihood component, essentially estimate observed data. likelihood piece inference based frequentists. Bayesians, theorem posits prior belief combined likelihood provides new updated posterior belief.fully Bayesian analysis, prior actually probability distribution , Empirical Bayes, prior derived observed data. Often prior expectation overall rate (either globally today, locally next week). Therefore, combine prior expectation observed data, can produce statement updated belief large small SMR . posterior typically just single number, fully distribution, can also say something precision certainty estimate area.","code":""},{"path":"disease-mapping-i.html","id":"poisson-gamma-mixture-model","chapter":"Week 4 Disease Mapping I","heading":"4.3.4.2 Poisson-Gamma mixture model","text":"Recall assumption Poisson distribution mean variance . uncommon real dataset roughly Poisson-distributed, perhaps processes (e.g. unmeasured predictors outcome) may extra-Poisson dispersion (e.g. mean >> variance).excess variation called -dispersion. problem leads biased statistical testing. may also learned alternative Poisson distribution Negative Binomial distribution, also works count data, extra dispersion parameter. However instead using Negative binomial directly, look Poisson-Gamma mixture model, achieves similar ends, natural fit Bayesian framework common many disease mapping applications.Poisson-Gamma mixture model pairing two parametric distributions better account squirrely data possible extra-Poisson variance. specifically gamma distribution serves prior Poisson mean parameter, \\(\\theta\\). words describes variable deviations Poisson mean can .package SpatialEpi, function called eBayes() estimates Empirical Bayes smoothed estimates disease burden (specifically relative excess risk SMR), based Poisson-Gamma mixture.First, let’s estimate EB-smoothed relative risk. function expects argument, Y, vector event counts, argument, E, expected count. Note also option include covariate matrix, wanted estimate covariate-adjusted EB-smoothed rates.Notice object global_eb1 returned function eBayes() actually list 5 elements. includes SMR (based observed data, smoothed!), well RR (mean estimate smoothed relative risk), RRmed (median estimate smoothed relative risk, case nearly identical mean). Notice also estimates \\(\\beta\\) (beta) \\(\\alpha\\) (alpha) parameters Gamma prior estimated data.can now add smoothed stabilized estimates dataset map raw unsmoothed SMR compared Empirical Bayes smoothed SMR…map symbolized using independent quantile categorization. result, notice two things map comparison :general patterns highs lows quite similar, although identicalThe cutpoints legend relatively different.Looking little closely differences illustrated plot can observe several things Empirical Bayes smoothing relation population size degree parameter shrinkage towards mean:counties largest sample size (larger dots plot) fall along diagonal observed smoothed rates similarConversely, counties likely ‘fanned ’ diagonal (indicating different value observed versus smoothed) smallest number events (e.g. small dots)Similarly bluer dots (least shrinkage) also larger less extreme valueThe redder dots (shrinkage) tended smaller.","code":"\nglobal_eb1 <- eBayes(vlbw$VLBW, vlbw$expect)\n# names(global_eb1)  # look at the object returned\n\nnames(global_eb1)## [1] \"RR\"    \"RRmed\" \"beta\"  \"alpha\" \"SMR\"\n# this adds the smoothed relative risk (same as SMR) to the vlbw dataset\nvlbw$ebSMR <- global_eb1$RR\nsmr_map <- tm_shape(vlbw) +\n  tm_fill('SMR',\n          style = 'quantile', palette = '-RdYlBu',\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'Raw SMR of VLBW',\n            inner.margins = c(0.02, 0.02, 0.1, 0.05),\n            legend.format = list(digits = 2))+\n  tm_shape(ga) +\n  tm_borders(lwd = 2, col = 'black')\n\neb_map <- tm_shape(vlbw) +\n  tm_fill('ebSMR',\n          style = 'quantile', \n          palette = '-RdYlBu',\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'EB smoothed SMR of VLBW',\n            inner.margins = c(0.02, 0.02, 0.1, 0.05),\n            legend.format = list(digits = 2))+\n  tm_shape(ga) +\n  tm_borders(lwd = 2, col = 'black')\n\ntmap_arrange(smr_map, eb_map)"},{"path":"disease-mapping-i.html","id":"estimating-bayesian-exceedance-probabilities-from-poisson-gamma-eb-estimates","chapter":"Week 4 Disease Mapping I","heading":"4.3.4.3 Estimating Bayesian exceedance probabilities from Poisson-Gamma EB estimates","text":"estimating parameters Gamma-prior Poisson parameter, \\(\\theta\\), can describe point estimates actually can describe entire posterior distribution estimated smooth rate. posterior distribution way saying Bayesian statistics just one answer, instead probabilistic range answers. Whereas frequentist statistics talk confidence intervals, Bayesian statistics roughly corresponding idea called credible interval, essentially specific thresholds posterior.interpretation credible intervals identical confidence intervals, close enough now. necessary disease mapping, might help illustration visualize posterior estimate two counties. One Dekalb county, large population, Baker county small population. can see, SMR (based observed data) quite different, mean posterior estimate EB-smoothed RR nearly identical. can also see precision certainty , much wider (greater) uncertainty Stewart County compared Dekalb county.describe likely unlikely EB-smoothed relative risk given county different null value 1, can use Bayesian exceedance probabilities. sound similar p-values mapped probmap() function, interpretation different Bayesian framework. Specifically, instead somewhat convoluted way interpret p-values (e.g. “probability observe counts extreme infinite repeated samples, assuming null true”), Bayesian exceedance probabilities straightforward. Specifically simply , “probability true parameter, \\(\\theta\\) greater 1.0, given prior observed data”.function called EBposththreshold() calculation, requires several arguments including conventional observed expected counts, also two parameters ,alpha beta estimated previous step. also need specify threshold beyond interested making inference. relative risk typically 1.0, wanted ask probability exceeding different value (e.g. “probability RR greater 2?”), option.necessary disease mapping, might interested different Bayesian frequentist approach . plot shows dataset.two things note plot comparing two estimates certainty precision:First, apparent inversely related. words frequentist p-value increases, predictive probability Bayesian model gets smaller. simply evaluating inverse parts question. frequentist p-value evaluating probability observing data extreme null true (e.g. small p-values lend support rejection null). contrast Bayesian exceedance probability reporting probability county RR greater 1.0. , higher probability consistent true extremes, rather spurious ones.Second, largely consistent, albeit identical one another. words track along diagonal suggesting county given p-value corresponding proportionate partner exceedance probability. differences reflect smoothing stabilization due EB methods.","code":"\nvlbw$eb2_prob <- EBpostthresh(Y = vlbw$VLBW, \n                              E = vlbw$expect, \n                              alpha = global_eb1$alpha, \n                              beta = global_eb1$beta, \n                              rrthresh = 1)"},{"path":"disease-mapping-i.html","id":"mapping-poisson-gamma-eb-estimates-and-exceedance","chapter":"Week 4 Disease Mapping I","heading":"4.3.4.4 Mapping Poisson-Gamma EB estimates and exceedance","text":"Finally, map smoothed estimates indication high probability different Georgia average rate (e.g. probability exceeding SMR 1.0 95%).Comparing two maps see fewer significant counties using Empirical Bayes approach. surprising, consistent goal trying separate signal random noise. suggest least counties appearing significantly different global rate, fact plausibly outliers small amounts information stably precisely estimated.","code":"\n# Identify counties with p-value < 0.05\npv <- vlbw %>%\n  mutate(pmap.pv = ifelse(SMR > 1 & pmap < 0.05, 1, 0)) %>%\n  group_by(pmap.pv) %>%\n  summarise() %>%\n  filter(pmap.pv == 1)\n\n\nm3<- tm_shape(vlbw) +\n  tm_fill('SMR',\n          style = 'quantile',\n          palette = '-RdYlBu',\n          #breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'SMR of VLBW,\\nGA 2018-2019',\n            inner.margins = c(0.1, 0.02,0.05,0.2),\n            legend.format = list(digits = 2)) +\n  # Add dark borders for significant\n  tm_shape(pv) +\n  tm_borders(lwd = 2, col = 'black') +\n  #tm_shape(ga) + \n  tm_borders(lwd = 1.5, col = 'black') +\n  tm_credits('Counties with higher than expected risk (p<0.05) highlighted with dark borders')+\n  tm_shape(ga) +\n  tm_borders(lwd = 1.5, col = 'black')\n\n# Identify counties with EB exceedance probability > 0.95 (corresponds to p<0.05)\npv2 <- vlbw %>%\n  mutate(eb.pv = ifelse(ebSMR > 1 & eb2_prob > 0.95, 1, 0)) %>%\n  group_by(eb.pv) %>%\n  summarise() %>%\n  filter(eb.pv == 1)\n\nm4 <- tm_shape(vlbw) +\n  tm_fill('ebSMR',\n          style = 'quantile',\n          palette = '-RdYlBu',\n          #breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'Empirical Bayes smoothed\\nSMR of VLBW',\n            inner.margins = c(0.1, 0.02,0.05,0.2),\n            legend.format = list(digits = 2)) +\n  # Add dark borders for significant\n  tm_shape(pv2) +\n  tm_borders(lwd = 2, col = 'black') +\n  tm_credits('Counties with higher than expected risk (p<0.05) highlighted with dark borders')+\n  tm_shape(ga) +\n  tm_borders(lwd = 1.5, col = 'black')\n\ntmap_arrange(m3, m4)"},{"path":"disease-mapping-ii.html","id":"disease-mapping-ii","chapter":"Week 5 Disease Mapping II","heading":"Week 5 Disease Mapping II","text":"","code":""},{"path":"disease-mapping-ii.html","id":"getting-ready-3","chapter":"Week 5 Disease Mapping II","heading":"5.1 Getting Ready","text":"","code":""},{"path":"disease-mapping-ii.html","id":"learning-objectives-4","chapter":"Week 5 Disease Mapping II","heading":"5.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"disease-mapping-ii.html","id":"additional-resources-4","chapter":"Week 5 Disease Mapping II","heading":"5.1.2 Additional Resources","text":"Anselin, L. Spatial Regression Analysis R: workbook. 2007.GeoDa Center Resources: Section distance-based spatial weightsGeoDa Center Resources: Section contiguity-based spatial weights","code":""},{"path":"disease-mapping-ii.html","id":"important-vocabulary-4","chapter":"Week 5 Disease Mapping II","heading":"5.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 5","code":""},{"path":"disease-mapping-ii.html","id":"spatial-thinking-in-epidemiology-3","chapter":"Week 5 Disease Mapping II","heading":"5.2 Spatial Thinking in Epidemiology","text":"first time formally incorporate make explicit spatial means spatial analysis. Although work now represented map (thus spatially contextualized), formally incorporate spatial relationships aspect analysis. Specifically, last week calculated statistical tests heterogeneity, estimated precision statistical significance, produced Empirical Bayes smoothed (stabilized) estimates parameters interest.tasks treated spatial unit spatially geographically independent every spatial unit. assumption units geographically independent referred aspatial analysis.","code":""},{"path":"disease-mapping-ii.html","id":"an-argument-for-the-relevance-of-space","chapter":"Week 5 Disease Mapping II","heading":"5.2.1 An argument for the relevance of space","text":"formally explicitly incorporate spatial relatedness need clear constitutes spatial relationships. two aspects considering spatial relatedness, apply two sides spatial thinking health. discussed recorded lecture, fundamental dimension spatial relations geography distance (relatedly idea proximity), whether euclidean (e.g. crow flies) distance, social distance, network distance.one hand, distance used metric defining aspect local population homogeneity distinct broader regional (e.g. study region-wide) heterogeneity. words, based Tobler’s First Law Geography, near things tend alike (e.g. correlated although necessarily causally linked) distant things (average), implying kind dependence correlation among local units might evident overall.concept – seems hold true many human non-human systems – means faced sparse data, concern uncertainty, can ‘borrow’ statistical information spatial neighbors supplement estimation local disease parameters. exactly spatial Empirical Bayes estimation, instead using overall (global) rate disease prior, use local rate neighbors surrounding entity kind custom, place-specific prior.deeper level, distance also important spatial thinking epidemiology. hypothesize – interested – whether entities geographically socially connected share health-relevant experiences. experiences exposures include microbial space (e.g. person--person transmission infectious agents), social norms (e.g. acceptability smoking body image perceptions), built environments (e.g. lead exposure municipal water systems, food environments), access health resources (e.g. health care, cancer screening), access opportunity structures (e.g. good schools, safe streets, employment opportunities).Distance Cartesian!examples emphasize role distance Cartesian (geographic) space. However, worth emphasizing complex versions distance proximity come play.example air travel makes linear Cartesian distance two places less relevant economic social drivers flows people back forth comes infectious disease transmission Zika Ebola.still distance dimension, defined push pull human mobility migration. possible define spatial neighbors abstract (e.g. non-geographic) ways. example, political scientists created spatial weights matrices connect states geographic boundaries, similarly legislatures act policy decisions. way distance measure ideology rather geography, still meaning spatial analysis health. today focus specific example geographic space rather social, political, economic space.sum, notion explicitly spatial analysis way incorporate theoretical conceptual aspects humans relate one another environment understanding distribution determinants disease.Whether treat spatial dependence relatedness primarily statistical feature exploitation (e.g. spatial disease mapping Empirical Bayes), attribute local ecosystem disease generation, clear neighbors defined influential final numerical results inference take . definition spatial neighbors, corresponding symbolization relatedness creation spatial weights matrices fundamental bridge theory geography meaning spatial epidemiology.","code":""},{"path":"disease-mapping-ii.html","id":"on-making-meaning-from-neighbors","chapter":"Week 5 Disease Mapping II","heading":"5.2.2 On making meaning from neighbors","text":"discussed , primary means spatial epidemiologists can make space explicit incorporating information places geographic units related one another; words distance (proximity, connectivity, contiguity, etc) single unit units begins put pieces puzzle together whole. often done creation numerical weight quantifies distance local unit neighbors (presume proximate, least degree).challenge spatial epidemiologist twofold:Conceptualizing spatial scale extent health-relevant process interest occursTranslating conceptual idea explicit definition neighbors, therefore spatial weightsFor areal analysis (e.g. spatial analysis polygons), two broad classes neighborhood definitions: contiguity-based definitions distance-based definitions. reality abstract expressions ‘distance,’ differ ‘near’ ‘far’ operationalized.primarily focused analysis areal units course, possible create neighbor definitions among point-referenced data using tessalation process creation Thiessen polygons (e.g. see discussion Contiguity-based weights points).table explored explained detail online lectures, lab activity. key takeaway point can define units near units using different definitions, definitions slightly different assumptions results.brief summary several common neighbor definitions:choice neighbor definition use influenced several study-specific factors, can conflict others:Variation size areal units across study area. areal units small (e.g. counties Eastern U.S.) large (e.g. counties Western U.S.), geographic area defined adjacent counties quite different (e.g. think long takes drive across two counties like Dekalb Fulton Atlanta, versus long might take traverse two counties Nevada Utah). contrast, fixed-distance neighbors consistent among linear distance index units neighbors.Assumptions requirements statistical analysis interest. algorithms require/expect features neighbor symmetry spatial weights row standardization account unequal numbers neighbors.assumed meaning space analysis. possible , instance, meaning distance Western counties different travel basic services norm denser areas East.purpose audience map. important make analysis accessible interpretable target audience.Aspects geography including islands presence non-contiguous units (e.g. Hawaii, Alaska, Puerto Rico)","code":""},{"path":"disease-mapping-ii.html","id":"spatial-analysis-in-epidemiology-3","chapter":"Week 5 Disease Mapping II","heading":"5.3 Spatial Analysis in Epidemiology","text":"apply concepts specific spatial analysis, continue use Georgia low birthweight dataset used previous module eBook. reminder, county-level dataset \\(n=159\\) Georgia Counties containing county live births (denominator) well count VLBW births (weight birth < 1500 grams) babies 2018-2019. data derived Georgia OASIS website.section first introduce create examine several different spatial neighbor definitions. never create neighbors just sake. purpose creating spatial neighbors weights matrices always use definition spatial analysis. Later section see use spatial weights producing spatial Empirical Bayes estimates.","code":""},{"path":"disease-mapping-ii.html","id":"creating-contiguity-neighbor-objects","chapter":"Week 5 Disease Mapping II","heading":"5.3.1 Creating contiguity neighbor objects","text":"R, spdep package series functions useful creating spatial weights matrices. general, process going spatial object (e.g. sf class data object) usable spatial weights matrix requires one step, steps vary depending eventual use.Since starting areal (polygon) data, starting point use utility function, poly2nb(), take polygon spatial object (class sf sp) determine specific polygon regions contiguous (touch, share boundaries ) regions. review help documentation, see function takes spatial sf object input, arguments specifying whether use Queen contiguity (default; Rook alternative). function returns something called neighbor list.summary() function objects class nb (neighbor object created spdep) provides useful high-level info, including presence regions zero links (neighbors – problem occur islands, example), distribution number links neighbors.might want look structure queen_nb object also, either using str(queen_nb), perhaps just viewing first elements list (e.g. nb objects class list R, use double-bracket indexing lists like queen_nb[[1]] view neighbors first region).neighbor object essentially list length equal number regions (\\(n=159\\) counties case). elements list correspond order input dataset, first list item first county current sort order. element list vector identifying counties neighbors .One important attribute spatial relationships whether symmetric . context spatial neighbors, spatial symmetry implies \\(region_i\\) neighbor \\(region_j\\), \\(region_j\\) also neighbor \\(region_i\\). Contiguity neighbors symmetric design: definition neighbor shared boundaries, true either partner relationship. see definitions spatial relationships neighbors result symmetric relationships. quick way check whether neighbor object symmetric code:better understand set spatial relationships, can useful visualize neighbor links connections choosing among neighbor definition, simply see relative density pattern connectivity. Note function plot.nb() nb object first argument, must also include matrix centroids second argument. reason nb object defines region connects , say space. centroids tell plot link line begins ends.Notice density neighbors generally lower coast state boundaries. systematic difference neighbors can produce patterns sometimes referred edge effects. edge effects source bias, counties interior state neighbors (thus ‘local information’ average) border counties. especially true absence neighbors artificial case counties bordering Alabama, Tennessee, North South Carolina, Florida. contrast, counties coast ‘real’ absence neighbors.","code":"\n# load the package spdep\nlibrary(spdep)\n\n# Create a queen contiguity neighbor list\nqueen_nb <- poly2nb(vlbw, queen = TRUE)\n\n# Examine the resulting object\nsummary(queen_nb)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 860 \n## Percentage nonzero weights: 3.401764 \n## Average number of links: 5.408805 \n## Link number distribution:\n## \n##  1  2  3  4  5  6  7  8  9 10 \n##  1  4 12 29 36 37 28  9  1  2 \n## 1 least connected region:\n## 64 with 1 link\n## 2 most connected regions:\n## 1 66 with 10 links\nis.symmetric.nb(queen_nb)## [1] TRUE\n# Create a matrix of the x,y coordinates for each county centroid\nga_cent <- st_centroid(st_geometry(vlbw))\n\n# Plot the outlines of counties with light grey boundaries\nplot(st_geometry(vlbw), border = 'grey')\n\n# Add the plot of the Queen contiguity connections\nplot.nb(queen_nb, ga_cent, points = F, add = T)"},{"path":"disease-mapping-ii.html","id":"creating-k-nearest-neighbors","chapter":"Week 5 Disease Mapping II","heading":"5.3.2 Creating k-nearest neighbors","text":"K-nearest neighbors flexible approach assuring balanced number neighbors, can help size density spatial regions varies across study area. instance fixed-distance buffer (e.g. perhaps counties within 50 miles) might work identify relevant neighbors Eastern Midwestern U.S., West, county may 100-200 miles across, zero neighbors definition. K-nearest neighbors, smaller Eastern larger Western counties neighbors (albeit differing spatial scales).k-nearest neighbors depend either arbitrary fixed distance, contiguity, always produce neighbors even islands. example analyses U.S. states, Alaska Hawaii contiguous neighbors. However, k-nearest neighbor approach still assign nearest neighbor regardless far away. instance nearest neighbor Hawaii might California. question must ask whether meaningful say Hawaii California neighbors. interested food environment, seems implausible. However, great deal social, cultural, economic interaction Hawaii California; instances plausible meaningful connection.create k-nearest neighbor object, first must identify relative proximity candidate neighbors. define nearest , convention measure Euclidean distance centroids polygons (literally geometric center), assumption average location describe polygon. requires two steps.First, knearneigh() function takes centroids, calculates pair-wise distances, sorts closest furthest, selects \\(k\\) nearest (smallest distance) units. knn2nb() function takes information creates formal nb neighbor object.Notice summaries interesting force everyone number links! However checking symmetry, important concern rises:K-nearest neighbors almost always produce asymmetric neighbors. Thinking U.S. states perhaps easy way understand . Consider state Hawaii: nearest states probably California, Oregon, Washington. However inverse true. nearest 2 (3 4 5) states California contiguous ‘lower 48’ states; Hawaii certainly among nearest places California.asymmetry problem spatial analytic tasks including spatial Empirical Bayes smoothing week. However, cluster analysis analyses future weeks, neighbor symmetry assumed required. choose k-nearest neighbor definition, also require symmetric spatial relationships, can force symmetry least two ways.First, specify sym  = TRUE knn2nb() call . essentially breaks rigid k-nearest neighbors forces reciprocity ‘neighborliness’. second method appropriate already created asymmetric neighbors, wish retrospectively force symmetry: make.sym.nb(). simply takes asymmetric neighbor object adds links make relationships symmetric. Note, however, alters number links neighbors region: now others.Note now four counties 8 links, rather 5. means counties nearest least 3 others, even though 3 nearest .","code":"\n# First create two sets of neighbors: 2 nearest and 5 nearest\nknn2 <- knearneigh(ga_cent, k = 2)\nknn5 <- knearneigh(ga_cent, k = 5)\n\n# Now take those lists of neighbors and make an nb object\nknn2_nb <- knn2nb(knn2, row.names = vlbw$GEOID)\nknn5_nb <- knn2nb(knn5, row.names = vlbw$GEOID)\n\nsummary(knn5_nb)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 795 \n## Percentage nonzero weights: 3.144654 \n## Average number of links: 5 \n## Non-symmetric neighbours list\n## Link number distribution:\n## \n##   5 \n## 159 \n## 159 least connected regions:\n## 13121 13029 13135 13127 13271 13279 13301 13007 13143 13221 13137 13289 13105 13051 13073 13189 13103 13319 13209 13317 13241 13033 13261 13249 13309 13113 13123 13157 13215 13311 13265 13019 13291 13171 13263 13001 13303 13027 13305 13133 13251 13163 13195 13013 13153 13205 13025 13009 13021 13217 13213 13151 13185 13181 13313 13183 13031 13245 13141 13191 13049 13079 13283 13083 13139 13107 13179 13229 13075 13267 13039 13077 13219 13315 13285 13095 13115 13225 13045 13035 13161 13097 13071 13237 13081 13011 13109 13017 13255 13197 13003 13015 13275 13211 13235 13131 13065 13293 13287 13155 13227 13173 13223 13277 13145 13297 13129 13295 13055 13165 13243 13047 13233 13187 13117 13111 13063 13067 13207 13101 13167 13193 13239 13149 13069 13125 13085 13091 13201 13061 13321 13169 13281 13299 13037 13053 13307 13273 13159 13023 13093 13177 13257 13175 13269 13059 13089 13043 13147 13057 13231 13253 13087 13005 13119 13247 13099 13199 13259 with 5 links\n## 159 most connected regions:\n## 13121 13029 13135 13127 13271 13279 13301 13007 13143 13221 13137 13289 13105 13051 13073 13189 13103 13319 13209 13317 13241 13033 13261 13249 13309 13113 13123 13157 13215 13311 13265 13019 13291 13171 13263 13001 13303 13027 13305 13133 13251 13163 13195 13013 13153 13205 13025 13009 13021 13217 13213 13151 13185 13181 13313 13183 13031 13245 13141 13191 13049 13079 13283 13083 13139 13107 13179 13229 13075 13267 13039 13077 13219 13315 13285 13095 13115 13225 13045 13035 13161 13097 13071 13237 13081 13011 13109 13017 13255 13197 13003 13015 13275 13211 13235 13131 13065 13293 13287 13155 13227 13173 13223 13277 13145 13297 13129 13295 13055 13165 13243 13047 13233 13187 13117 13111 13063 13067 13207 13101 13167 13193 13239 13149 13069 13125 13085 13091 13201 13061 13321 13169 13281 13299 13037 13053 13307 13273 13159 13023 13093 13177 13257 13175 13269 13059 13089 13043 13147 13057 13231 13253 13087 13005 13119 13247 13099 13199 13259 with 5 links\nis.symmetric.nb(knn2_nb)## [1] FALSE\nis.symmetric.nb(knn5_nb)## [1] FALSE\nknn5_symmetric <- make.sym.nb(knn5_nb)\nsummary(knn5_symmetric)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 910 \n## Percentage nonzero weights: 3.599541 \n## Average number of links: 5.72327 \n## Link number distribution:\n## \n##  5  6  7  8 \n## 73 61 21  4 \n## 73 least connected regions:\n## 13121 13135 13127 13289 13105 13051 13103 13319 13317 13241 13033 13249 13113 13157 13215 13171 13027 13133 13251 13013 13009 13021 13181 13031 13245 13049 13079 13083 13107 13179 13267 13039 13219 13315 13285 13095 13225 13081 13017 13275 13235 13155 13145 13295 13055 13165 13243 13047 13233 13111 13207 13101 13167 13239 13149 13091 13201 13061 13281 13053 13159 13093 13177 13257 13175 13059 13147 13057 13231 13253 13087 13005 13099 with 5 links\n## 4 most connected regions:\n## 13007 13223 13129 13187 with 8 links\nis.symmetric.nb(knn5_symmetric)## [1] TRUE"},{"path":"disease-mapping-ii.html","id":"visualizing-differences-between-competing-neighbor-definitions","chapter":"Week 5 Disease Mapping II","heading":"5.3.2.1 Visualizing differences between competing neighbor definitions","text":"spatial analyst, might interested choice neighbors affects results. better understand different one definition next can helpful visualize side--side. visualization likely interest broader audience. words probably publish map. Instead production helps , spatial epidemiologist, better understand options make informed decisions.Using just base-R plotting (create fancier maps desired ggplot tmap), can easily visualize county polygons, lines connecting centroids neighboring counties indication shared influence, contact, interaction.function plot.nb() requires spatial neighbor object (e.g. object class nb), matrix \\(x, y\\) locations polygon centroids, work drawing connecting lines.function code named diffnb() simply utility function compare two nb objects determine different. can plot different values red order quickly see differs one neighbor definition next.surprise lots red lines knn5 compared knn2. Every single county 3 additional neighbors former compared latter. However interesting see many initially asymmetric relationships added links order enforce symmetry (e.g. red lines right-hand plot, compared middle).","code":"\npar(mfrow = c(1, 3),        # set plotting space for 2 side-by-side plots\n    mar = c(.2,.2,1,.2))    # Set margins for plotting\n\n# Plot the knn = 2 neighbor connections\nplot(st_geometry(vlbw), border = 'grey', main = 'knn = 2')\nplot.nb(knn2_nb, ga_cent, point = F, add = T)\n\n# Plot the knn = 5 neighbor connections\nplot(st_geometry(vlbw), border = 'grey', main = 'knn = 5')\nplot.nb(knn5_nb, ga_cent, point = F, add = T, col = 'blue')\nplot.nb(diffnb(knn2_nb, knn5_nb), ga_cent, point = F, add = T, col = 'red')\n\n# Plot the knn = 5 AND the differences (in red) when knn = 5 is made symmetric\nplot(st_geometry(vlbw), border = 'grey', main = 'Symmetric Knn5')\nplot.nb(knn5_nb, ga_cent, point = F, add = T, col = 'blue')\nplot.nb(diffnb(knn5_nb, knn5_symmetric), ga_cent, point = F, add = T, col = 'red')\npar(mfrow = c(1,1))"},{"path":"disease-mapping-ii.html","id":"creating-graph-based-triangle-neighbor-objects","chapter":"Week 5 Disease Mapping II","heading":"5.3.3 Creating Graph-based triangle neighbor objects","text":"contiguity framework takes reasonable approach local implies direct interaction indicated shared borders. However many instances, odd shape polygons means regions quite close one another share border. different approach – one two methods ’ll discuss called graph-based neighbors – defines local neighbors relative proximity using geometry approach.process subdivides space non-overlapping triangles, using centroids region vertices triangle. neighbor therefore region connected edge (link) two vertices (centroids). practically, results neighbor region near(ish) required touching-borders. Graph-based neighbor objects symmetric design.Look back summary queen_nb object created previously. graph-based neighbor definition results slightly connections every county compared Queen contiguity, also reduces variation number links. Queen counties many 10 links 1 link; contrast graph-based definition results counties ranging minimum 3 neighbors maximum 8.visualize triangularized neighbors can plot links, next Queen contiguity compare.Notice graph-based neighbors strange connections along Western border Georgia. Delauney triangle algorithm makes unexpected connections centroids along edges. thought unreasonable spatial relationships (think !), can prune using Sphere Influence graph restrict proximate relationships. instances, carrying pruning produce sphere influence graph neighbors sensible compared using product stage.code little intimidating looking: includes nested functions original triangle neighbor object fed soi.graph() function, fed graph2nb() function. basically looking ties connections defined triangularization algorithm also proximate.see Queen neighbors compares can plot :","code":"\ntri_nb <- tri2nb(ga_cent)\nsummary(tri_nb)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 918 \n## Percentage nonzero weights: 3.631185 \n## Average number of links: 5.773585 \n## Link number distribution:\n## \n##  3  4  5  6  7  8 \n##  1 18 40 63 31  6 \n## 1 least connected region:\n## 15 with 3 links\n## 6 most connected regions:\n## 58 65 82 107 118 120 with 8 links\nis.symmetric.nb(tri_nb)## [1] TRUE\nsoi_nb <- graph2nb(soi.graph(tri_nb, ga_cent))\n\nsummary(soi_nb)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 850 \n## Percentage nonzero weights: 3.362209 \n## Average number of links: 5.345912 \n## Link number distribution:\n## \n##  3  4  5  6  7  8 \n## 11 35 32 53 25  3 \n## 11 least connected regions:\n## 14 15 21 64 71 87 96 120 123 124 152 with 3 links\n## 3 most connected regions:\n## 65 82 107 with 8 links"},{"path":"disease-mapping-ii.html","id":"creating-fixed-distance-neighbors","chapter":"Week 5 Disease Mapping II","heading":"5.3.4 Creating fixed-distance neighbors","text":"concept buffering around locations define exposure probably familiar. uncommon exposures access health services, healthy food stores, exposure toxin emitters quantified using fixed-distance buffers. fixed-distance neighbor definition therefore natural extension, believe definition local near can described () falls within given radius. way different previous approaches neither sharing borders, k-nearest neighbor required.required place (specifically centroid place), falls within designated distance. number units falling within given threshold range zero maximum number units study, certainly vary one location another (e.g. thinking distinctions Eastern Western counties U.S.).Determining appropriate distance can challenging unless clear theory evidence (e.g. distance required avoid exposure radiation fixed point source). Often analysts consider range distances understand whether pattern changes competing scenarios. approach used investigate spatial clusters disease.define fixed-distance neighbors, use function dnearneigh() must define minimum distance (probably always set zero), maximum distance defining buffer.Beware using distance measures unprojected data.Note distance parameters described scale coordinate measures spatial object. case, spatial object projected, units meters. Therefore distance 1000 1 kilometer. However, unprojected data units angular degrees, readily interpretable.calculate neighbors two distance buffers: counties within 25km within 50km centroid county neighbors; counties neighbors. Notice output instance dnearneigh() just distances , actually formal neighbor (nb) object.can compare linkages two distance bands one another:compare one previous definition (e.g. Queen contiguity).","code":"\ndist_25 <- dnearneigh(ga_cent, d1 = 0, d2 = 25000)\ndist_50 <- dnearneigh(ga_cent, d1 = 0, d2 = 50000)\nsummary(dist_50)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 1072 \n## Percentage nonzero weights: 4.240339 \n## Average number of links: 6.742138 \n## Link number distribution:\n## \n##  3  4  5  6  7  8  9 10 11 12 \n## 10 18 20 21 33 21 21 10  4  1 \n## 10 least connected regions:\n## 4 14 22 41 61 71 96 120 152 153 with 3 links\n## 1 most connected region:\n## 156 with 12 links"},{"path":"disease-mapping-ii.html","id":"from-spatial-neighbors-to-spatial-disease-mapping","chapter":"Week 5 Disease Mapping II","heading":"5.3.5 From spatial neighbors to spatial Disease Mapping","text":"main reason struggling preceding ins outs spatial neighbors like define reasonable version local given spatial dataset, use definition advance spatial epidemiologic goals. discussed , primary goal production statistically stable rates, less bouncing around parameters simply due small denominators.state : statistics solve fundamental problems sparse data! However, statistical disease mapping methods, including Empirical Bayes fully Bayesian methods, can use available information recover important underlying geographic trends instances.","code":""},{"path":"disease-mapping-ii.html","id":"empirical-bayes-overview","chapter":"Week 5 Disease Mapping II","heading":"5.3.5.1 Empirical Bayes Overview","text":"introduced last week, Bayesian thinking mathematical operationalization relatively intuitive process engage : often prior information prior beliefs effect size risk rate plausible, informed experiences literature evidence date. look result analysis (e.g. data likelihood), internally (often sub-consciously) combine pieces (prior data) develop new, updated belief, posterior belief.Bayesian process framework moving implicit cognitive process open, stating mathematically prior belief , therefore arrived new updated, posterior, belief.Empirical Bayes disease rate smoothing process take set regions, consider data, question, ‘truest underlying rate disease place?’ compare observed data prior belief expectation rate plausibly (specifically, approximately within range).get prior important potentially impactful; last week aspatial Empirical Bayes smoothing used overall average rate entire study region (e.g. state Georgia) prior. words, sum cases across spatial units (e.g. counties), population risk across units, calculate single reference rate, variance around expectation.reference rate (prior) combined observed data weighted fashion prior weighted higher small-population regions, data weighted higher large-population regions. result weighted calculation posterior smoothed estimate rate.Last week calculated aspatial Empirical Bayes estimate low birthweight. prior information estimation comes size county’s expected count. Specifically mean, \\(\\mu\\), variance, \\(\\sigma^2\\) estimated \\(n=159\\) counties expected count, single, global, overall prior used strategy used last week.Beware output given function providing! One possible place confusion output global aspatial Empirical Bayes estimate using eBayes() excess relative risk county compared global referent (e.g. statewide prevalence VLBW). However function use spatial Empirical Bayes () output rate rather relative excess risk.Luckily two closely related. RR eBayes() represents relative deviation county statewide average. know global average (e.g. calculated r), simply multiplying RR value individual county single global referent, r, gives us aspatial Empirical Bayes smoothed estimate rate county. therefore comparable estimates spatial Empirical Bayes estimators.","code":"\n# Calculate aspatial EB\nglobal_eb1 <- eBayes(vlbw$VLBW, vlbw$expected)\n\n# Add the crude/observed SMR to the data\nvlbw$eb_global <- global_eb1$RR\n\n# Convert the aspatial EB RR to a smoothed aspatial EB rate by multiplying by referent rate, r\nvlbw$EB_global <- r * vlbw$eb_global"},{"path":"disease-mapping-ii.html","id":"spatial-empirical-bayes","chapter":"Week 5 Disease Mapping II","heading":"5.3.6 Spatial Empirical Bayes","text":"using newly-created definitions local neighbors among Georgia counties can extend Empirical Bayes approach changing source prior information.aspatial global EB, total rate Georgia prior reference rate. However another option providing statistical information locally-varying expected rates use average one’s neighbors prior. produces sort borrowing statistical information space, assumption local counties tell us specific place counties far away.Note expectation counties next one another risk rate, instead average local information informative non-local (global) prior information. said, statistical approaches disease mapping believe important spatial dissimilarities exist neighbors, searching boundaries areas high low rates. can implemented package CARBayes introduced upcoming (optional) section fully Bayesian disease mapping.spatial EB, thus follows process global aspatial EB, different prior. prior defined local neighbors, different choices neighbor object likely least influence resulting geographic smoothed patterns.function estimating spatial Empirical Bayes EBlocal() spdep package, requires count events count population risk county, also nb neighbor object. Although highlighted importance neighbor symmetry spatial analysis, symmetric neighbors required spatial Empirical Bayes estimation.NOTE: currently function R estimate spatial EB rates credible/confidence intervals p-values, Poisson-Gamma model aspatial. Fully Bayesian disease mapping (e.g. Disease Mapping IV) best approach spatial methods producing credible/confidence intervals desired.Now can create spatial EB estimate neighbor definitions order understand robust sensitive ultimate results choice neighbors.","code":"\n# Estimate spatial (local) EB under the Queen contiguity neighbor definition\neb_queen <- EBlocal(vlbw$VLBW, vlbw$TOT, nb = queen_nb)\n\n# The output fro EBlocal() is a 2 column data.frame. The second colum is the EB estimate\nvlbw$EB_queen <- eb_queen[,2]\n# Use the sphere of influence-pruned Delauney triangle definition\neb_soi <- EBlocal(vlbw$VLBW, vlbw$TOT, nb = soi_nb)\nvlbw$EB_soi <- eb_soi[,2]\n\n# Use the k-nearest neighbors (k=5) definition\neb_knn5 <- EBlocal(vlbw$VLBW, vlbw$TOT, nb = knn5_nb)\nvlbw$EB_knn5 <- eb_knn5[,2]\n\n# Use the 50-km fixed distance neighbors\neb_dist50 <- EBlocal(vlbw$VLBW, vlbw$TOT, nb = dist_50)\nvlbw$EB_dist50 <- eb_dist50[,2]"},{"path":"disease-mapping-ii.html","id":"visualizing-alternate-smoothing-approaches","chapter":"Week 5 Disease Mapping II","heading":"5.3.6.1 Visualizing alternate smoothing approaches","text":"code simple visual comparison raw/observed, aspatial EB, variety spatially-smoothed EB estimates. review maps, might ask following questions:EB smoothing versus raw/observed estimates differ?spatial EB estimates differ aspatial EB estimate?differences notice among various spatial EB estimates, distinguished unique definitions local?saw last week, differences observed (crude) rates aspatial EB. However can see even dramatic differences four spatial EB rates compared either observed aspatial. Among spatial EB estimates minor differences suggesting – among set neighbor definitions, outcome – relatively consistent patterns VLBW regardless choice neighbors (e.g. answer relatively robust neighbor definition).","code":""},{"path":"disease-mapping-ii.html","id":"final-thoughts-making-choices","chapter":"Week 5 Disease Mapping II","heading":"5.3.7 Final thoughts: Making choices","text":"past two weeks quickly amassed large number analytic tools address one problem spatial epidemiology: reliably characterize spatial heterogeneity presence rate instability uncertainty due data sparsity. analytic strategies include two approaches Empirical Bayes smoothing, also myriad neighbor definitions choose spatial approach.Unfortunately simple rule follow choosing tool use, summary considerations. Ultimately make decisions context epidemiologic question, constraints data, audience end-user results. many things epidemiologic analysis, important role science also need experts can engage art analysis.","code":""},{"path":"disease-mapping-ii.html","id":"choosing-neighborhood-definitions","chapter":"Week 5 Disease Mapping II","heading":"5.3.7.1 Choosing ‘neighborhood’ definitions","text":"can see, lot ways describing local, haven’t even talked inverse-distance weighting. go choosing one definition another? example intersection art science spatial epidemiology., might wonder one decides smooth smooth, smoothing, neighbor definition use? lot written final answers. evidence approximately 6 neighbors provides nice balance informative prior information local units versus biased estimates. beyond broad recommendation, three general approaches selecting neighbor definition:maximizing precision fit - method statistical nature implies best smoother fits data best. possible estimate mean-squared error (MSE) root mean squared error (RMSE) describe far, average, observed rate observed data, idea closest average distance best. discuss extensions idea model fit move fully Bayesian mapping. code estimate RMSE.theory, context, question - emphasis explaining neighbors approach. clearly important analyst bring clarity question hand, local context, decision sensibly local given disease health outcome. instances, one method clearly stands others. However, uncommon moderate support (theoretically) multiple. can see mapped methods , case minor difference definitions data.empirically estimating weights - covering approach . However, briefly, idea use evidence spatial auto correlation inform spatially important one unit another.bottom line recommendation think spatially consider data, question, goals hand. purposes disease mapping, error reduction precision driving goals, comparing RMSE across competing options make sense. However purposes, statistical fit might equate unbiased estimation target parameters (true non-spatial analysis!).one simple approach calculating root mean squared error (RMSE). Recall RMSE square root average squared difference observed value model-predicted (case EB-smoothed) value. first bit code defining simple function calculate RMSE. define function 2 arguments: eb Empirical Bayes estimate \\(region_i\\), o observed rate \\(region_i\\).based , fits best? case, RMSE quite similar, slight advantage given spatial Queen contiguity definition. Although strong evidence preferring one strategy another based statistical fit alone. Therefore factors (including purpose map, audience, theory place underlying analysis) important making decisions.","code":"\nRMSE <- function(eb, o){\n  sqrt(mean((eb - o)^2))\n}\n\nRMSE(vlbw$EB_global, vlbw$rate)## [1] 0.006809047\nRMSE(vlbw$EB_queen , vlbw$rate)## [1] 0.006404844\nRMSE(vlbw$EB_soi , vlbw$rate)## [1] 0.006471757\nRMSE(vlbw$EB_knn5 , vlbw$rate)## [1] 0.006529195\nRMSE(vlbw$EB_dist50 , vlbw$rate)## [1] 0.006433534"},{"path":"disease-mapping-iii.html","id":"disease-mapping-iii","chapter":"Week 6 Disease Mapping III","heading":"Week 6 Disease Mapping III","text":"","code":""},{"path":"disease-mapping-iii.html","id":"getting-ready-4","chapter":"Week 6 Disease Mapping III","heading":"6.1 Getting Ready","text":"","code":""},{"path":"disease-mapping-iii.html","id":"learning-objectives-5","chapter":"Week 6 Disease Mapping III","heading":"6.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"disease-mapping-iii.html","id":"additional-resources-5","chapter":"Week 6 Disease Mapping III","heading":"6.1.2 Additional Resources","text":"Adrian Baddeley tutorial analysis spatial point processesHazelton. Kernel Smoothing Methods. Chapter 10, Handbook Spatial Epidemiology. Posted Canvas","code":""},{"path":"disease-mapping-iii.html","id":"important-vocabulary-5","chapter":"Week 6 Disease Mapping III","heading":"6.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 6","code":""},{"path":"disease-mapping-iii.html","id":"spatial-thinking-in-epidemiology-4","chapter":"Week 6 Disease Mapping III","heading":"6.2 Spatial Thinking in Epidemiology","text":"","code":""},{"path":"disease-mapping-iii.html","id":"revisiting-spatial-point-processes","chapter":"Week 6 Disease Mapping III","heading":"6.2.1 Revisiting spatial point processes","text":"People exist places, uniformly randomly distributed. live cities; fewer live rural area. conditional people actually live work play, null expectation treat occurrence health events (e.g. disease, death, behaviors) random variables, leverage tools statistics characterize occurrence expect versus something unusual.spatial analysis, treat health events random events among individuals located space. Thus, conditional people , might assume (, null hypothesis) occurrence events generated according assumed probability distribution. utility Poisson Point Process becomes apparent see divide region small sub-regions count number events within , assuming count follows Poisson distribution.\nFIGURE 2.1: Poisson point process\nfigure, quantify spatial intensity events calculating \\(\\lambda = \\frac{n}{area}\\). Thus, statistical analysis date premised idea spatial location points can interpreted lens Poisson probability distribution.calculate spatial intensity continuously, without constraint specific parametric distribution, without using possibly arbitrary boundaries zoning schemes areal geographic units census tracts, zip codes, counties?Spatial point process analysis focuses characterizing patterns derived directly location points , without arbitrary aggregation. study point process analysis broad, focus one particularly flexible strategy week: kernel density estimation (KDE). Kernel density estimation several features making useful spatial epidemiology including:non-parametric, meaning rely specific probability distribution (e.g. Poisson, negative binomial)provides alternative means characterizing local neighbors. strategy KDE estimators similar inverse distance weightingIt can used primary tool, can intermediate step creating spatial weights, see geographically weighted regressionWhile really designed analyzing points, can use areal/polygon data well.","code":""},{"path":"disease-mapping-iii.html","id":"what-is-a-kernel-density-estimator","chapter":"Week 6 Disease Mapping III","heading":"6.2.2 What is a kernel density estimator?","text":"kernel function (e.g. mathematically described ‘shape’) iteratively centered point data. example, Gaussian kernel means Gaussian bell-shaped curve centered point; width curve defined parameter, \\(h\\), stands bandwidth.\nFIGURE 2.2: Kernel density estimator\nestimate spatial intensity points, \\(\\hat{\\lambda}\\), can sum area kernels estimate overall kernel density location. kernel density estimate essentially reports spatially continuous summary local intensity events.result can summarize study region spatially-referenced point data using spatially continuous intensity surface. analyst decides smooth bumpy surface increasing decreasing value bandwidth parameter, \\(h\\). see , decision bandwidth made subjectively (e.g. produce visually appealing surface), minimizing error cross-validation.\nFIGURE 2.3: Kernel density smoothing\n","code":""},{"path":"disease-mapping-iii.html","id":"spatial-heterogeneity-versus-spatial-non-stationarity","chapter":"Week 6 Disease Mapping III","heading":"6.2.3 Spatial heterogeneity versus Spatial non-stationarity","text":"now familiar concept spatial heterogeneity phenomenon local estimates intensity, risk, rate different least locations global estimates across entire region. local rates statistically similar global rate say process spatially homogeneous, likely much interest spatial cartographic perspective.vocabulary Disease Mapping 1, first introduced terms ‘stationarity’ versus ‘non-stationarity’ referred closely related idea. stationary process one estimate statistic (e.g. intensity, density, risk, rate, also correlation regression coefficient) dependent location within region. words, estimate statistic invariant choice local area within global study region. contrast, spatial non-stationarity present estimate statistic dependent location measures made.Distinguishing heterogeneity non-stationarityThese two concepts clearly similar. One way unpack subtle differences think two kinds spatial heterogeneity:Areal units (e.g. counties) different values parameter interest (e.g. disease rate), high-rate counties low-rate counties randomly distributed. words conclude heterogeneity even randomly mixed exact location counties.Area units different values, values tend clustered space high-rate counties near one another, low-rate counties near one another. Thus value given county dependent spatial location.discussion spatial heterogeneity spatial non-stationarity relevant discussion spatially varying statistics second part Spatial Analysis module week, especially relevant begin discuss spatial clustering spatial regression.","code":""},{"path":"disease-mapping-iii.html","id":"limitations-in-kernel-density-estimation","chapter":"Week 6 Disease Mapping III","heading":"6.2.4 Limitations in kernel density estimation","text":"several features real data can limit accurate estimation true underlying spatial intensity surface. :Study region edge effects: almost situation, data available given dataset represents subset universe points events interest. can carry KDE available data, boundaries data collected (e.g. within given state boundary), intensity points near boundaries may mis-estimated due missing data. Several statistical smoothers options incorporate adjustments edge effects.Determining bandwidth: choice kernel bandwidth perhaps influential decision driving final results appear. occasionally may theoretical grounds priori specification bandwidth, often decision one subjective analyst choice (typically ideal) statistical optimization. sections discussion fixed versus adaptive bandwidths, well algorithms selecting values least upper lower bounds.","code":""},{"path":"disease-mapping-iii.html","id":"uses-for-kernel-density-estimation-kde-in-spatial-epidemiology","chapter":"Week 6 Disease Mapping III","heading":"6.2.5 Uses for kernel density estimation (KDE) in spatial epidemiology","text":"kernel density estimation method learn semester working point data, clear one major application. However, generally, KDE broad applications. examples might think using kernel density estimates:producing spatially continuous (typically raster) surfaces representing disease risk. application consistent disease mapping, purpose describe spatial heterogeneity disease intensity risk.summarizing point-referenced resources (e.g. retail food outlets; health care clinics; toxic emitters; etc) exposure surface. strategy alternative calculating custom distances event point every resource exposure. Instead, kernel density surface summarizes average exposure resources given point space.smoothing summarizing data measured areal polygon unit. KDE optimized point data, possible extend smoothing data (exposure, covariate, health outcome) measured reported ecologic areal unit.Building extension KDE polygons, can summarize social economic exposure surfaces. useful way extend socio-economic-cultural measures might available census geography represent contained within specific boundaries, explicitly spatially situated.Distinguishing point intensity risk intensityThe material emphasizes interest intensity points per unit area. point represent epidemiologically? Usually represents location observation, automatically clear whether observation event interest (e.g. illness, death, etc), whether person risk (e.g. sampled observed participant may may experience event).epidemiologic purposes care ratio events population risk. can translate idea kernel density surfaces estimating two kernel density estimates: one intensity events (e.g. deaths) per unit area, second intensity persons risk death per unit area. can take ratio two kernel density estimates produce risk surface.illustrated following sections.next section, introduce two different uses kernel density estimation:First intensity estimation spatial point processes consistent description .Second, illustrate use kernel density estimators create weights geographically-weighted summary statistics, including spatially varying mean risk rate. strategy can applied either points polygons.","code":""},{"path":"disease-mapping-iii.html","id":"guide-to-the-rest-of-this-section","chapter":"Week 6 Disease Mapping III","heading":"6.2.6 Guide to the rest of this section…","text":"lot content contained module. worth highlighting broad distinctions help navigate.first section focuses tools using sparr spatstat packages create kernel density estimates point data. includes:Creating ppp objects planar point processesCreating owin objects define study regionsDiscussion several different strategies selecting kernel bandwidth dictates smoothingCreation kernel density surfaces single point processesCreation kernel density relative risk surfaces contrasting ratio numerator denominator intensityVisualizing output several ways.second section introduces seemingly quite distinct strategy incorporating kernel density estimators spatial epidemiology. introduces tools calculating geographically weighted summary statistics characterize spatial heterogeneity. tools use kernel density estimators geographically weight observations, can applied points polygons.NOTE:modules semester, Spatial Analysis section eBook uses examples introduce analytic approaches, -class Lab uses different data practice.‘newness’ kernel density estimators, large volume information, following material eBook closely aligned practice Lab.recommend read content lab, work lab , refer back eBook questions.","code":""},{"path":"disease-mapping-iii.html","id":"spatial-analysis-in-epidemiology-kernel-estimation-of-point-processes","chapter":"Week 6 Disease Mapping III","heading":"6.3 Spatial Analysis in Epidemiology: Kernel estimation of point processes","text":"section three specific objectives:Introduce new spatial data class R, ppp, necessary executing kernel estimation functionsIntroduce kernel density estimation spatial point processes, including selection fixed bandwidths, use adaptive bandwidthsIntroduce spatial relative risk surfaces, including estimation tolerance contours","code":""},{"path":"disease-mapping-iii.html","id":"preparing-packages-and-data","chapter":"Week 6 Disease Mapping III","heading":"6.3.1 Preparing packages and data","text":"several new packages required work:sp Data classIn addition new packages, also need sp package. Recall worked sf class data now, although learned sp older format spatial data R. packages functions (including sparr spatstat) incorporated compatibility sf data, need convert particular objects sf sp moving forward.data used example, lab, concerns exact \\(x,y\\) residential location births Dekalb Fulton county, including indication infants subsequently died within first year life.NOTE: data simulated based approximate patterns. representation actual birth event data.spatial point location births infant deaths two separate files. addition polygon file providing outline Dekalb Fulton counties provided describe study window.code , note us function ('Spatial') object county. conversion step, translating sf object object class sp. necessary particular case polygon file representing study window functions create window expect data class sp. contrast can extract coordinates sf point object, without need convert sp first.","code":"\nlibrary(sparr)         # A package for estimating spatial intensity and relative risk\nlibrary(spatstat)      # A package with tools that underly the sparr package\nlibrary(maptools)      # This has a helper function for working with ppp class data\nlibrary(raster)        # The outputs of these KDE functions will be raster. This package gives us tools for working with rasters\n# This is points for births in Dekalb/Fulton county\nb_point <- st_read('birth_points.gpkg')\n\n# This is points for deaths in Dekalb/Fulton county\nd_point <- st_read('death_points.gpkg') \n\n# This is an outline of Dekalb/Fulton county to be used as a study 'window'\ncounty <- st_read('DekalbFultonWindow.gpkg') %>%\n  as('Spatial')"},{"path":"disease-mapping-iii.html","id":"introducing-a-new-spatial-data-class-ppp","chapter":"Week 6 Disease Mapping III","heading":"6.3.2 Introducing a new spatial data class: ppp","text":"Much statistical methods spatial point process actually developed ecology, methods merging spatial analysis spatial epidemiology fields recent years. One consequence history, early developers methods R defined spatial data classes, case class called ppp point pattern data two-dimensional plane.create ppp data object need, minimum, two things:matrix \\(x,y\\) coordinates event pointsA definition spatial window study region.window necessary nearly data set sub-sample universe possible points, analysis point processes requires appreciation bounds sampling observation.define window formally R object class owin, can rectangular bounding box (e.g. outline available data), customized polygon. use outline Dekalb & Fulton counties customized spatial window observation births infant deaths.Note:\nfunction create owin object works sp class SpatialPolygons. converted polygon file class sp imported ().can see summary plot owin object looks like.","code":"\ncounty_owin <- maptools::as.owin.SpatialPolygons(county)\nsummary(county_owin)## Window: polygonal boundary\n## single connected closed polygon with 698 vertices\n## enclosing rectangle: [1026366.3, 1098719.2] x [1220671, 1302019.3] units\n##                      (72350 x 81350 units)\n## Window area = 2086320000 square units\n## Fraction of frame area: 0.354\nplot(county_owin)"},{"path":"disease-mapping-iii.html","id":"creating-the-ppp-objects","chapter":"Week 6 Disease Mapping III","heading":"6.3.3 Creating the ppp objects","text":"Now use function ppp() create objects class ppp spatial point files, b_point (representing locations births, denominator infant mortality), d_point (representing locations deaths, numerator infant mortality). requires definition study window defined object name county_owin class owin., study window delineates versus study area demarcates edges study region. can look help documentation function ppp() see arguments. Note function requires specification \\(x,y\\) locations two separate vectors, extract coordinate values sf object using st_coordinates().two ppp objects?Recall estimation spatial point process agnostic whether point (person) event (e.g. death) (e.g. live birth lived past 1 year). represent ratio deaths live births (e.g. infant mortality rate), need estimate two kernel density surfaces, take ratio.might expect, built-methods (spatstat package) summarize plot ppp objects.summary includes information overall spatial intensity (e.g. events per unit area), well number points, observational window. plot d_ppp look just like plot d_point contain information. note, repeat code birth events, b_point, plot less readable 94,000 births compared 705 deaths!","code":"\n# Create the birth ppp object\nb_ppp <- ppp(x = st_coordinates(b_point)[, 1], \n             y = st_coordinates(b_point)[, 2],\n             window = county_owin)\n\n# Create the death ppp object\nd_ppp <- ppp(x = st_coordinates(d_point)[, 1], \n             y = st_coordinates(d_point)[, 2],\n             window = county_owin)\nsummary(d_ppp)## Planar point pattern:  701 points\n## Average intensity 3.359978e-07 points per square unit\n## \n## Coordinates are given to 2 decimal places\n## i.e. rounded to the nearest multiple of 0.01 units\n## \n## Window: polygonal boundary\n## single connected closed polygon with 698 vertices\n## enclosing rectangle: [1026366.3, 1098719.2] x [1220671, 1302019.3] units\n##                      (72350 x 81350 units)\n## Window area = 2086320000 square units\n## Fraction of frame area: 0.354\nplot(d_ppp)"},{"path":"disease-mapping-iii.html","id":"bandwidth-selection","chapter":"Week 6 Disease Mapping III","heading":"6.3.4 Bandwidth selection","text":"discussed , lecture, kernel density estimation requires analyst specify kernel function (e.g. Gaussian kernel, quartic bi-weight kernel), kernel bandwidth.two, bandwidth substantially impactful results choice kernel function.reminder, bandwidth (sometimes indicated variable \\(h\\)) describes width radius kernel function, result dictates smooth resulting intensity surface . small bandwidth produce bumpier rougher surface, whereas larger bandwidth result smoothing.Bandwidth measures proximity closenessConceptually might notice similarity bandwidth spatial analysis definition spatial neighbors . case kind ‘local’ inclusion data analysis, within broader global study region.Bandwidth defines sets points considered close .two general kinds bandwidth settings:Fixed bandwidths: single value \\(h\\) designates width kernel (thus resulting smoothness estimated intensity surface) entire study region. Fixed bandwidths commonly used, sensible study region relatively homogeneous population risk. However choosing single value can challenging practice density points varies substantially across study region, case study region includes range urban rural.Adaptive bandwidths: name implies, approach changes adapts size kernel density bandwidth according density points (data) differing sub-areas overall study region. result relatively smoothing (larger bandwidth) areas sparse point data, relatively less smoothing (smaller bandwidth) areas point density.","code":""},{"path":"disease-mapping-iii.html","id":"fixed-bandwidth-methods","chapter":"Week 6 Disease Mapping III","heading":"6.3.4.1 Fixed bandwidth methods","text":"prefer fixed bandwidth, first challenge choosing . One option selecting fixed bandwidth incorporate theory prior knowledge process interest. instance, trying understand whether prevalence diabetes related local food environment urban area, might want bandwidth helps illuminate differences diabetes intensity scale consistent food environment. instance bandwidth 1-mile might reasonable urban areas one 50-miles, latter likely smooth away local variation interest.However, uncommon theory prior knowledge insufficient make clear choice, data sparsity mandates alternate approach driven concern stable estimates.package sparr several functions designed use primarily statistical optimization estimating ‘optimum’ bandwidth. introduce two commonly used statistical bandwidth selection optimizers:Cross-validation: approach divides data subsets, using one subset choose bandwidth, comparing performance subsets. goal find value works ‘best’ (e.g. optimize statistical parameter across multiple iterations). approach computationally intensive large datasets. discussed note , cross-validation can result small bandwidth estimation.-smoothing: alternate approach aims identify maximum amount smoothing necessary minimizing statistical error. definition maximum value rather ideal optimal value, can useful setting bounds.sparr package provides three cross-validation approaches estimation:LCSV.density (least squares cross validated);LIK.density (likelihood cross-validated);SLIK.adapt (described experimental likelihood cross-validation adaptive).‘optimizing’ different thing. LSCV.density minimizes unbiased estimate mean integrated squared error (MISE) whereas LIK.density maximizes cross-validated leave-one-average log-likelihood density estimate.look help documentation see (near bottom) prominent warning message. reports “CV bandwidth selection notoriously unstable practice tendency produced rather small bandwidths…”","code":""},{"path":"disease-mapping-iii.html","id":"cross-validation-with-lik.density","chapter":"Week 6 Disease Mapping III","heading":"6.3.4.2 Cross-validation with LIK.density()","text":"LIK.density() uses likelihood estimation cross-validation optimal bandwidth. death dataset, d_ppp, runs just seconds. However took > 5 minutes produce value much larger births dataset, b_ppp. code lets try see produces:examine object returned (h_LIK_d), ’ll see just single number. value \\(h\\), optimized bandwidth. , words, radius 2-dimensional kernel density function units data, meters case (e.g. original data Albers Equal Area projected). means optimum kernel radius just 1.5 kilometers.","code":"\nh_LIK_d <- LIK.density(d_ppp)## Searching for optimal h in [27.463186212964, 12058.8149770359]...Done.\nprint(h_LIK_d)## [1] 1613.661"},{"path":"disease-mapping-iii.html","id":"oversmoothing-algorithm-with-function-os","chapter":"Week 6 Disease Mapping III","heading":"6.3.4.3 Oversmoothing algorithm with function OS()","text":"approach much less computationally intense, thus feasible spatial point processes (e.g. numerator deaths, denominator births). see , can use value returned OS() pilot value adaptive bandwidth estimation. words provides kind reference starting point adaptation process.Note birth data smaller optimal bandwidth (h_os_b) points. points means information available granular smoothing, whereas relatively sparse death data larger smoothing bandwidth (h_os_d).","code":"\nh_os_d <- OS(d_ppp)\nh_os_b <- OS(b_ppp)\n\nprint(h_os_d)## [1] 4257.798\nprint(h_os_b)## [1] 1897.371"},{"path":"disease-mapping-iii.html","id":"selecting-a-common-bandwidth-for-both-numerator-and-denominator","chapter":"Week 6 Disease Mapping III","heading":"6.3.4.4 Selecting a common bandwidth for both numerator and denominator","text":"One challenge bandwidth selection typically two related spatial point processes (e.g. numerator, death events; denominator, birth events). Therefore don’t want single KDE, instead need consider numerator representing spatial intensity deaths, denominator representing spatial intensity live births risk. raises question whether common bandwidth , whether optimized separately.may minor differences absolute intensity different bandwidths single point process, taking ratio two intensity surfaces can exaggerate small differences quite large. functions estimating single, joint, optimum bandwidth. function LSCV.risk() just LIK.density() , two spatial point processes. code example , like previous example, cross-validation approach birth data set takes excessive amount time (least exercise).","code":"\n#h_LSCV_risk <- LSCV.risk(d_ppp, b_ppp)"},{"path":"disease-mapping-iii.html","id":"adaptive-bandwidth-methods","chapter":"Week 6 Disease Mapping III","heading":"6.3.4.5 Adaptive bandwidth methods","text":"Adaptive methods specified time kernel density estimation. bandwidth constant, instead adaptive, usually still need specify pilot bandwidth, reference point adaptive modification occurs. mentioned , -smoothing approach OS() can used pilot value.","code":""},{"path":"disease-mapping-iii.html","id":"estimating-kernel-density-surfaces","chapter":"Week 6 Disease Mapping III","heading":"6.3.5 Estimating Kernel Density surfaces","text":"now turn actual estimation kernel density approximations underlying spatial intensity disease. approach lab first illustrate estimate separate densities point process (e.g. deaths births), demonstrate two strategies creating spatial relative risk surfaces, generally target output spatial epidemiologists.note, discussion demonstrate use fixed adaptive bandwidths. general adaptive bandwidths may practical approach absence theoretical empirical preference otherwise. However instances fixed bandwidths (either theoretically informed, derived CV -smoothing algorithms) desired, thus seeing action useful.","code":""},{"path":"disease-mapping-iii.html","id":"bivariate.density-for-kde-of-single-point-process","chapter":"Week 6 Disease Mapping III","heading":"6.3.5.1 bivariate.density() for KDE of single point process","text":"actually several R packages accomplish kernel density estimation, one particularly useful spatial epidemiology (kernel density estimator must 2-dimensional frequently take ratio two densities) sparr package, stands Spatial Spatiotemporal Relative Risk.sparr function bivariate.density() flexible useful tool carrying KDE either fixed adaptive bandwidths. many arguments bivariate.density() (see help documentation), several worth specifically highlighting.Intensity versus DensityUp now used words intensity density synonymous point process parameter, exactly accurate.Intensity average number points per unit area. density proportionate intensity, scaled values study region sum 1. words density surface proper probability density function (PDF).map two look identical except scale legend.","code":""},{"path":"disease-mapping-iii.html","id":"fixed-bandwidth-kde-with-bivariate.density","chapter":"Week 6 Disease Mapping III","heading":"6.3.5.2 Fixed bandwidth KDE with bivariate.density()","text":"First, let’s try basic version uses -smooth estimate point process.can explore objects produced function call. instance, list objects, named sub-elements:also plotting functionality built sparr package allows us quickly visualize resulting density plot.","code":"\ndeath_kde <- bivariate.density(pp = d_ppp, h0 = h_os_d, edge = 'diggle')\nbirth_kde <- bivariate.density(pp = b_ppp, h0 = h_os_b, edge = 'diggle')\nsummary(birth_kde)## Bivariate Kernel Density/Intensity Estimate\n## \n## Bandwidth\n##   Fixed smoothing with h0 = 1897.371 units (to 4 d.p.)\n## \n## No. of observations\n##   94373 \n## \n## Spatial bound\n##   Type: polygonal\n##   2D enclosure: [1026366, 1098719] x [1220671, 1302019]\n## \n## Evaluation\n##   128 x 128 rectangular grid\n##   5808 grid cells out of 16384 fall inside study region\n##   Density/intensity range [6.313906e-14, 1.585466e-09]\nnames(birth_kde)## [1] \"z\"         \"h0\"        \"hp\"        \"h\"         \"him\"       \"q\"        \n## [7] \"gamma\"     \"geometric\" \"pp\"\npar(mfrow = c(1, 2)) # Plots in 1 row with 2 columns\nplot(birth_kde, main = 'Birth density')\nplot(death_kde, main = 'Death density')\npar(mfrow = c(1,1))  # Reset plotting space to be 1 row, 1 column"},{"path":"disease-mapping-iii.html","id":"adaptive-bandwidth-kde-with-bivariate.density","chapter":"Week 6 Disease Mapping III","heading":"6.3.5.3 Adaptive bandwidth KDE with bivariate.density()","text":"discussed , alternative single fixed bandwidth, implementation algorithm changes (adapts) bandwidth across study region response density sparseness data. approach still requires specification global bandwidth, adaptation multiplier making global smaller larger needed.code use argument h0 = xxx specify pilot bandwidth. adaptive bandwidth KDE requires adjustment across study region, notice functions take longer fixed bandwidth , especially large birth point process.adaptive maps similar fixed bandwidth?plots intensity adaptive bandwidth point processes look different *fixed bandwidth processes. reason units measurement (intensity) small differences readily apparent.However differences become apparent take ratio two surfaces, .","code":"\ndeath_kde_adapt <- bivariate.density(d_ppp, \n                                     h0 = h_os_d, \n                                     edge = 'diggle', \n                                     adapt = TRUE,\n                                     verbose = FALSE)\nbirth_kde_adapt <- bivariate.density(b_ppp, \n                                     h0 = h_os_b, \n                                     edge = 'diggle',\n                                     adapt = TRUE,\n                                     verbose = FALSE)\npar(mfrow = c(1, 2))\nplot(birth_kde_adapt, main = 'Birth density\\n(adaptive h)')\nplot(death_kde_adapt, main = 'Death density\\n(adaptive h)')\npar(mfrow = c(1,1))"},{"path":"disease-mapping-iii.html","id":"plotting-kde-estimates-with-tmap","chapter":"Week 6 Disease Mapping III","heading":"6.3.5.4 Plotting KDE estimates with tmap","text":"handy sparr package built-plotting functionality quickly visualize results. However may want control plotting, instance tmap even ggplot2.recall named elements list object returned bivariate.density(), first called z, density surface .class object im image. However almost spatial plotting operation outside sparr spatstat, want data raster class rather im format (fundamentally data raster model, data structure R quite data structure class raster). can convert im raster class like :Notice code specification crs = \"+init=epsg:5070\". im object lost information original coordinate reference system (CRS) projection. However, need raster object CRS information plot properly tmap. know original point data projected Albers Equal Area, specifically EPSG code 5070. re-define creating rasters .Now can plot tmap:maps look pixely?two reasons contributing .First, code specified colors quantiles order get range despite possibly skewed values. try re-plotting plots setting style = 'cont' continuous color palette, comment n=9. see produces much smoother looking plot. difference plots style = 'cont' style = 'quantile' gradation color intermediate levels intensity.First, code specified colors quantiles order get range despite possibly skewed values. try re-plotting plots setting style = 'cont' continuous color palette, comment n=9. see produces much smoother looking plot. difference plots style = 'cont' style = 'quantile' gradation color intermediate levels intensity.Another reason pixelation original call bivariate.density() used default output resolution 128 x 128 grid cells. done computational efficiency. However, note want higher-resolution surface (e.g. publication, presentation), can increase specifying resolution = creation KDE surface original call bivariate.density().Another reason pixelation original call bivariate.density() used default output resolution 128 x 128 grid cells. done computational efficiency. However, note want higher-resolution surface (e.g. publication, presentation), can increase specifying resolution = creation KDE surface original call bivariate.density().","code":"\nclass(birth_kde$z)## [1] \"im\"\ndeath_kde_raster <- raster(death_kde_adapt$z,\n                           crs = \"+init=epsg:5070\")\nbirth_kde_raster <- raster(birth_kde_adapt$z,\n                           crs = \"+init=epsg:5070\")\n# Create map of death surface\nm1 <- tm_shape(death_kde_raster) + \n  tm_raster(palette = 'BuPu',\n            style = 'quantile',\n            n = 9,\n            title = 'Death density') +\n  tm_shape(county) +\n    tm_borders() + \n  tm_layout(legend.format = list(scientific = T))\n\n# Create map of birth surface\nm2 <- tm_shape(birth_kde_raster) +\n  tm_raster(palette = 'BuPu',\n            style = 'quantile',\n            n = 9,\n            title = 'Birth density') +\n  tm_shape(county) +\n    tm_borders() + \n  tm_layout(legend.format = list(scientific = T))\n\n# plot 2-panel arrangment\ntmap_arrange(m1, m2)"},{"path":"disease-mapping-iii.html","id":"creating-relative-risk-surface-manually","chapter":"Week 6 Disease Mapping III","heading":"6.3.6 Creating relative risk surface manually","text":"now, created KDE surface death birth points separately. epidemiology, rarely care numerator denominator separately! put two together informative disease map?Raster algebra term arithmetic algebraic manipulation raster grids. Recall raster data set simply array numbers. numbered value grid-point represents mean density intensity points per unit-area, mapped color make plot.simply matrix numbers, can take two rasters resolution study area add, subtract, multiply, log-transform, otherwise operate arithmetically.instance manually create spatial relative risk surface simply take ratio two KDE density surfaces. result relative measure akin SMR: quantifies relative deviation area overall average value. values 1 areas lower average risk, meaning intensity deaths less intensity live births, values 1 higher average risk (intensity deaths greater intensity live births).Intensity vs Density matters taking ratioFor interpreting ratio two kernel surfaces take care distinguish spatial intensity (number point events per unit area; value integrates sums total number points across study region) versus spatial density (probability point event occurring location, conditional total number points; integrates 1 across study region). distinction important.default output bivariate.density() function spatial density surface. ratio two density (probability) surfaces take value 1.0 probability death location proportionate probability birth location.contrast ratio two intensity surfaces interpreted absolute measure (e.g. risk, rate, prevalence) ranging zero 1. choose intensity = TRUE specifying bivariate.density() function get intensity rather (default) density surface.tmap call flipped color ramp using negative sign front name ramp. also specified continuous style rather discrete (e.g. quantile), specified legend breaks.Now can clearly see regions higher risk lower risk infant mortality!","code":"\n# Create risk surface as ratio of death density to birth density\nrisk <- death_kde_raster / birth_kde_raster\n\n# Map it...\ntm_shape(risk) + \n  tm_raster(palette = '-RdYlGn',\n            style = 'cont',\n            breaks = c(0.1, 0.6, 0.9,   1.1, 2, 4.9),\n            title = 'IMR SMR')  +\ntm_shape(county) +\n  tm_borders()"},{"path":"disease-mapping-iii.html","id":"creating-relative-risk-surface-with-risk-function","chapter":"Week 6 Disease Mapping III","heading":"6.3.7 Creating relative risk surface with risk() function","text":"preceding manual approach created two separate kernel density surfaces, manually relied raster algebra create spatial relative risk surface. useful know may use KDE setting work single spatial point process (e.g. imagine instead estimating disease intensity, wish estimate exposure density surface).However, sparr package provides shortcut estimation spatial relative risk surfaces function risk(). takes numerator denominator ppp object arguments calculates spatial relative risk surface automatically.demonstration comparing four pre-defined (possibly theoretically informed, possibly arbitrary), fixed bandwidths purposes data exploration, estimate four distinct spatial relative risk surfaces, well one adaptive KDE. case notice first two arguments numerator denominator ppp object. next argument pre-specified fixed bandwidth (e.g. 1000, 2000, 4000, 8000 meters).function also illustrates another feature allows us quantify statistical precision creating tolerance contours. Tolerance contours simply lines encircle regions statistically significant beyond given threshold. argument tolerate = T tells function estimate asymptotic p-values testing null hypothesis local relative risk death equal across study region.default, function estimates log relative risk, helpful reminder relative risk asymmetric. However, understand ratio measures, careful plot results appropriately. reason, set log = FALSE, although obviously omit keep everything log scale.NOTE: fixed bandwidth risk() functions run quickly, , adaptive bandwidth computationally intensive, take longer.Examine contents one objects. summary show us range estimated risk, resolution evaluation grid, number points evaluated.NOTICEYou can see range estimated risk \\([-8.216873e-14, 11.05946]\\). lower bound practical terms zero (e.g. , small), counter-intuitively also negative! estimated negative risk? answer seems related Diggle edge correction. example substitute edge = 'uniform' anomaly goes away. likely edge correction (big picture valuable strategy) reweights regions may result specific location estimates become negative., can use built-plotting functionality sparr produce maps spatial relative risk surface tolerance contours. (NOTE: default legend works best log relative risk, doesn’t behave well relative risk treats distance 0 1 distance 1 2, 4 5).code plots side--side, might find easier plot one time zoom closer.Things notice plots:Notice contour lines \\(p<0.05\\).Notice risk surface becomes smoother fixed bandwidth transitions \\(1km\\) \\(8km\\).Finally, notice adaptive bandwidth consistent maps, seems balance detail 2000 meter 4000 meter definitions.","code":"\nimr1000 <- risk(d_ppp, b_ppp, h0 = 1000, \n            tolerate = T,\n            verbose = F, \n            log = F,\n            edge = 'diggle')\n\nimr2000 <- risk(d_ppp, b_ppp, h0 = 2000, \n            tolerate = T,\n            log = F,\n            edge = 'diggle',\n            verbose = F)\n\nimr4000 <- risk(d_ppp, b_ppp, h0 = 4000, \n            tolerate = T,\n            log = F,\n            edge = 'diggle',\n            verbose = F)\n\nimr8000 <- risk(d_ppp, b_ppp, h0 = 8000, \n            tolerate = T,\n            log = F,\n            edge = 'diggle',\n            verbose = F)\n\nimradapt <- risk(d_ppp, b_ppp, \n                 h0 = h_os_d, \n                 adapt = T, \n                 tolerate = T,\n                 log = F,\n                 edge = 'diggle',\n                 verbose = F)\nsummary(imr1000)## Log-Relative Risk Function.\n## \n## Estimated risk range [-8.216873e-14, 11.05946]\n## \n## --Numerator (case) density--\n## Bivariate Kernel Density/Intensity Estimate\n## \n## Bandwidth\n##   Fixed smoothing with h0 = 1000 units (to 4 d.p.)\n## \n## No. of observations\n##   701 \n## \n## Spatial bound\n##   Type: polygonal\n##   2D enclosure: [1026366, 1098719] x [1220671, 1302019]\n## \n## Evaluation\n##   128 x 128 rectangular grid\n##   5808 grid cells out of 16384 fall inside study region\n##   Density/intensity range [-1.82875e-25, 2.78467e-09]\n## \n## --Denominator (control) density--\n## Bivariate Kernel Density/Intensity Estimate\n## \n## Bandwidth\n##   Fixed smoothing with h0 = 1000 units (to 4 d.p.)\n## \n## No. of observations\n##   94373 \n## \n## Spatial bound\n##   Type: polygonal\n##   2D enclosure: [1026366, 1098719] x [1220671, 1302019]\n## \n## Evaluation\n##   128 x 128 rectangular grid\n##   5808 grid cells out of 16384 fall inside study region\n##   Density/intensity range [2.143127e-16, 2.383589e-09]\nnames(imr1000)## [1] \"rr\" \"f\"  \"g\"  \"P\"\npar(mar = c(1,1,1,1)) # adjust the plotting margins\npar(mfrow = c(3,2))   # Plot 3 rows and 2 columns\nplot(imr1000)\nplot(imr2000)\nplot(imr4000)\nplot(imr8000)\nplot(imradapt)\npar(mfrow = c(1,1))  # reset the plot space to 1 row, 1 col"},{"path":"disease-mapping-iii.html","id":"using-functions-to-map-rr-with-tmap","chapter":"Week 6 Disease Mapping III","heading":"6.3.8 Using functions to map RR with tmap","text":"begin plotting several maps, conversion im raster code producing map panels can feel cumbersome. following section required, demonstration can write simple custom functions R speed repetitive tasks.function R like macro SAS; simply set instructions accepts arguments (inputs), carries action inputs, returns output. Thus, helps automate cumbersome tasks can repeatedly efficiently.function accepts single argument, labeled simply x . expectation x output one preceding calls risk() function.Notice function first extracts spatial relative risk surface (e.g. x$rr), assigns appropriate projection (got stripped along way).function extracts probability map set pixel-specific p-values. rasterToContour() function takes raster creates contour lines specified levels corresponding 95% tolerance contour. Finally, use return() tells returned function called.write function, need loaded given session; afterwords can call using prepRaster(x = my_risk_object).’re roll, also write function producing tmap panel:result work can easily (compactly), map four fixed-bandwidth spatial relative risk surfaces.","code":"\n### --- prepRaster() function --- ###\nprepRaster <- function(x){\n  rr <- raster(x$rr,\n               crs = \"+init=epsg:5070\")\n\n  p_raster <- raster(x$P,\n               crs = \"+init=epsg:5070\")\n  plines <- rasterToContour(p_raster, levels = c(0.025,  0.975))\n  \n  return(list(rr=rr,plines=plines))\n} ## END prepRaster() ##\n### --- make_map() function to create panel maps --- ###\nmake_map <- function(x, bw){\n  mtitle <- paste('IMR - ', bw, ' smooth', sep = '')\n  tm_shape(x$rr) +\n  tm_raster(palette = '-RdYlGn',\n            style = 'cont',\n            breaks = c(0.1, 0.6, 0.9,   1.1, 2, 4.9),\n            midpoint = NA,\n            title = 'IMR SMR') +\n  tm_shape(x$plines) +\n  tm_lines(col = 'level',\n           legend.col.show = F) + \n  tm_layout(main.title = mtitle,\n            legend.format = list(digits = 1)) \n} ## END make_map() function ##\n# First convert to raster and extract p-value contours\nrr_1000 <- prepRaster(imr1000)\nrr_2000 <- prepRaster(imr2000)\nrr_4000 <- prepRaster(imr4000)\nrr_8000 <- prepRaster(imr8000)\nrr_adapt <- prepRaster(imradapt)\n\n# Then produce map panels\nm1000 <- make_map(rr_1000, '1 km')\nm2000 <- make_map(rr_2000, '2 km')\nm4000 <- make_map(rr_4000, '4 km')\nm8000 <- make_map(rr_8000, '8 km')\nmapadapt <- make_map(rr_adapt, 'adaptive')\n\ntmap_arrange(m1000, m2000, m4000, m8000,mapadapt, ncol = 2)"},{"path":"disease-mapping-iii.html","id":"gwss","chapter":"Week 6 Disease Mapping III","heading":"6.4 Spatial Analysis in Epidemiology: Kernel estimation of areal data","text":"preceding section saw use KDE estimating smooth spatial intensity surface spatial point process. section introduce geographically-weighted statistics extendable areal units points. three key features use KDE preceding section expand :KDE points - whole notion kernel density estimation process indeed connected \\(x,y\\) point location, mean take advantage non-parametric smoothing kinds data, polygons. Typically centroid (geometric center) polygon used stand-point KDE done polygons.KDE binary values - spatial point process definition description location discrete points representing discrete state. instance section , visualized spatial intensity surface infant deaths, separately surface live births. want measure continuous value rather discrete, binary state spatial locations? mechanics KDE can still helpful!KDE kind spatial weighting procedure - true Part well…spatial intensity essentially spatially-weighted number points surrounding index location divided area kernel function. lab primary use kernel function produce weights calculating weighted-statistics including mean, median, etc quantity measured.accomplish extensions use functions package GWmodel (e.g. geographically weighted models) useful exploratory spatial data analysis. main function used , gwss(), stands geographically weighted summary statistics, uses non-parametric spatial weighting kernel density function compute locally varying descriptive statistics mean, median, standard deviation, correlation, . certainly works data represented \\(x,y\\) points, can also work polygon data.means gwss() function can useful exploring spatial heterogeneity form local spatial non-stationarity. Recall spatial stationarity notion statistical parameter global constant across space?previously encountered stationarity opposite spatial heterogeneity. context referring risk prevalence health states. statistic can stationary (constant) non-stationary (spatially varying).objective section extend understanding utility kernel density functions beyond simply computing intensity density surfaces seeing tool creating spatially local weights statistical function. use study region (Fulton Dekalb counties), now looking several socio-contextual covariates derived American Community Survey considered along infant mortality rate produced .section focuses primarily gwss() function accomplish following tasks:Estimate statistically-optimal fixed bandwidth explore adaptive bandwidths use gwss() functionCalculate local, spatially-weighted mean, median, SD, IQR four census-tract level continuous measures using kernel density functionsUsing Monte Carlo simulation produces significance contours estimates local, spatially-weighted summary statisticsCalculate local, spatially-weighted bivariate statistics summarizing correlations (Pearson Spearman) pairs variables varies space","code":""},{"path":"disease-mapping-iii.html","id":"packages-and-data","chapter":"Week 6 Disease Mapping III","heading":"6.4.1 Packages and data","text":"new package introduced GWmodel, several familiar packages also useful:methods introduced section work either spatial points spatial polygons. However example demonstrate use specifically spatial polygons, specifically polygons representing census tracts Fulton Dekalb counties. dataset 345 census tract polygons (4 tracts deleted due missing values), summarizes five summary measures tract:First, read gpkg data sf object, convert sp use GWmodel. sparr package, GWmodel yet fully sf compliant forced use sp data classes. likely change point future.","code":"\nlibrary(tidyverse) # For data piping and manipulationlibrary(GWmodel)   # Has the function gwss() \nlibrary(sf)        # For import of sf data\nlibrary(sp)        # For conversion to sp, required for GWmodel\nlibrary(tmap)      # For mapping\nlibrary(GWmodel)   # Geographically weighted statistics\n# This is Dekalb/Fulton census tracts\natl <- st_read('Fulton-Dekalb-covariates.gpkg') %>%\n  as('Spatial') # convert to sp class"},{"path":"disease-mapping-iii.html","id":"mapping-the-observed-values","chapter":"Week 6 Disease Mapping III","heading":"6.4.1.1 Mapping the observed values","text":"understand new data, consider using summary(), exploratory functions, well producing simple maps see spatial distribution variables. Recall tmap package works sf sp data! means can map object ‘usual manner’ even though converted sp.Index Concentration Extremes (ICE) ranges -1 +1. value \\(-1\\) occurs everyone tract poor; value \\(+1\\) occurs tracts everyone affluent; value \\(0\\) suggests either balance affluence poverty, alternatively everyone ‘middle income.’ Therefore makes sense map separately inevitably need divergent color ramp.","code":"\n# First map the 4 variables that are %\ntm_shape(atl) + \n  tm_fill(c('pctNOHS', 'pctPOV', 'pctMOVE', 'pctOWNER_OCC'),\n          style = 'quantile') +\n  tm_borders()\ntm_shape(atl) +\n  tm_fill('ICE_INCOME_all',\n          style = 'quantile',\n          palette = 'div') +\n  tm_borders()"},{"path":"disease-mapping-iii.html","id":"why-are-we-using-kde-on-these-data","chapter":"Week 6 Disease Mapping III","heading":"6.4.1.2 Why are we using KDE on these data?","text":"least two general reasons might think use spatial smoothing approach KDE continuous data :believe estimates spatial unit (polygons case, points) statistically unstable, believe averaging neighbors produce reliable estimate parameter interest; orYou interested identifying spatial patterns trends possibly larger scale data units . instance might looking regions city apparent clustering poverty, home ownership, residential instability. words suspect people exposed values within boundaries tracts, also nearby environments.","code":""},{"path":"disease-mapping-iii.html","id":"what-bandwidth-for-kernel-density-estimates","chapter":"Week 6 Disease Mapping III","heading":"6.4.2 What bandwidth for kernel density estimates?","text":"Recall decision bandwidth kernel density function one influential using KDE spatial epidemiology. reason bandwidth defines smoothness bumpiness statistical estimation, different choices can produce dramatically different results., might theoretically important criteria selecting bandwidth, use statistical optimization approach. GWmodel function bw.gwss.average() used estimating ‘optimal’ bandwidth estimating spatially varying mean median using cross-validation. specific function statistics (e.g. SD, IQR, correlation coefficients). spatial structure might different variable, can evaluate variables:numbers units map, meters. suggests need pretty large fixed bandwidth (58 70 km!), least minimize error determined cross-validation approach.Adaptive bandwidth GWmodelAdaptive bandwidth GWmodel works little differently sparr package. choose adaptive = TRUE returned distance units map (e.g. meters) instead number nearest neighbors defines large small kernel function adapts.important conceptually adaptation means: GWmodel unit analysis census tract, individual person. census tract 10 people census tract 10,000 assumed amount information., see story CV approach suggests large bandwidth. 345 areal units dataset, suggests nearly included kernel. produce little spatial variation. idea statistical optimization approach appealing, discussed , CV methods known imperfect.now use common-sense approach. seem might balance local information spatial variation including 10% data single kernel density estimation location. choose use \\(n=35\\) neighbors definition adaptive bandwidth. Note alter number see results vary.","code":"\n# Fixed bandwidth selection\nbw.gwss.average(atl, vars = c('pctPOV', 'pctPOV', 'pctMOVE', \n                              'pctOWNER_OCC', 'ICE_INCOME_all'))##                   pctPOV   pctPOV  pctMOVE pctOWNER_OCC ICE_INCOME_all\n## Local Mean bw   58801.33 58801.33 58801.33     65740.07       65740.07\n## Local Median bw 65740.07 65740.07 58801.33     70028.45       65353.39\n# Adapative bandwidth selection\nbw.gwss.average(atl, vars = c('pctPOV', 'pctNOHS', 'pctMOVE', \n                              'pctOWNER_OCC', 'ICE_INCOME_all'),\n                adaptive = T)##                 pctPOV pctNOHS pctMOVE pctOWNER_OCC ICE_INCOME_all\n## Local Mean bw      315     268     297          337            333\n## Local Median bw    297     297     315          326            315"},{"path":"disease-mapping-iii.html","id":"geographically-weighted-summary-statistics-gwss","chapter":"Week 6 Disease Mapping III","heading":"6.4.3 Geographically weighted summary statistics: gwss()","text":"fully describe explore ‘raw’ data, want summarize smoothing extremes, looking broad spatial trends values. Finding local average value can done using either mean median quantify central tendency. Obviously, distribution data within local regions relatively normally distributed (least symmetric), mean median similar. data quite skewed, might prefer median summary measures.Similarly, knowing whether (much) local measures alike different informative. choose statistic works well describe variation normally-distributed (symmetric) data (e.g. standard deviation), one performs well non-normal skewed data (e.g. inter-quartile range). Finding large values either SD IQR suggest substantial local heterogeneity difference target measure, whereas small values suggest local areas relatively similar.gwss() function actually estimates just listed , now focus measures.calculate geographically-weighted summary statistics using gwss() need provide dataset, single variable (vector multiple variables), decision using fixed adaptive bandwidth, finally specification bw bandwidth . , value enter bw depends whether select adaptive = T .fixed bandwidth, value enter number units map (e.g. meters case). requesting adaptive bandwidth, value bw meters, actually number count many nearest neighbors minimally included kernel density estimation location. discussed , use \\(n=35\\) adaptive definition neighbors. result summary estimation including approximately 10% total data. like called robust statistics (e.g. median IQR robust skewed non-normal data), also must specify argument quantile = T.Perhaps unintuitively, way get summary result usual summary(), instead type print(atl.ss). see LOT results. focus moment just results top (local mean; local SD) bottom (local median; local IQR) output.summary gives information range smoothed values statistic, variable. Notice, example names variables. estimates geographically weighted mean end _LM stands local mean. Similarly estimates geographically weighted standard deviation end _LSD local SD.","code":"\natl.ss <- gwss(atl, vars = c('pctPOV', 'pctNOHS', 'pctMOVE', 'pctOWNER_OCC',\n                             'ICE_INCOME_all'),\n               adaptive = T,\n               bw = 35,\n               quantile = T)\nprint(atl.ss)##    ***********************************************************************\n##    *                       Package   GWmodel                             *\n##    ***********************************************************************\n## \n##    ***********************Calibration information*************************\n## \n##    Local summary statistics calculated for variables:\n##     pctPOV pctNOHS pctMOVE pctOWNER_OCC ICE_INCOME_all\n##    Number of summary points: 345\n##    Kernel function: bisquare \n##    Summary points: the same locations as observations are used.\n##    Adaptive bandwidth: 35 (number of nearest neighbours)\n##    Distance metric: Euclidean distance metric is used.\n## \n##    ************************Local Summary Statistics:**********************\n##    Summary information for Local means:\n##                           Min.   1st Qu.    Median   3rd Qu.   Max.\n##    pctPOV_LM          0.050645  0.119765  0.180836  0.270868 0.3947\n##    pctNOHS_LM         0.011338  0.031796  0.051985  0.079042 0.1369\n##    pctMOVE_LM         0.129222  0.166627  0.185688  0.215939 0.3356\n##    pctOWNER_OCC_LM    0.214225  0.421030  0.486894  0.568339 0.7545\n##    ICE_INCOME_all_LM -0.363659 -0.161687 -0.010174  0.180475 0.3921\n##    Summary information for local standard deviation :\n##                           Min.  1st Qu.   Median  3rd Qu.   Max.\n##    pctPOV_LSD         0.030466 0.080964 0.095033 0.122989 0.2000\n##    pctNOHS_LSD        0.013018 0.027740 0.038529 0.055891 0.1512\n##    pctMOVE_LSD        0.043404 0.062143 0.070973 0.085519 0.1594\n##    pctOWNER_OCC_LSD   0.128460 0.171147 0.193042 0.236043 0.3127\n##    ICE_INCOME_all_LSD 0.098726 0.153791 0.186239 0.214184 0.2949\n##    Summary information for local variance :\n##                              Min.    1st Qu.     Median    3rd Qu.   Max.\n##    pctPOV_LVar         0.00092817 0.00655511 0.00903123 0.01512619 0.0400\n##    pctNOHS_LVar        0.00016947 0.00076952 0.00148449 0.00312385 0.0229\n##    pctMOVE_LVar        0.00188388 0.00386175 0.00503717 0.00731354 0.0254\n##    pctOWNER_OCC_LVar   0.01650192 0.02929137 0.03726506 0.05571613 0.0978\n##    ICE_INCOME_all_LVar 0.00974689 0.02365154 0.03468479 0.04587486 0.0870\n##    Summary information for Local skewness:\n##                            Min.  1st Qu.   Median  3rd Qu.   Max.\n##    pctPOV_LSKe         -0.60296  0.40145  0.88760  1.50706 4.5718\n##    pctNOHS_LSKe        -0.26353  0.80588  1.38779  2.41449 7.7612\n##    pctMOVE_LSKe        -0.43164  0.27893  0.58307  0.97433 2.6409\n##    pctOWNER_OCC_LSKe   -1.21089 -0.41080 -0.10531  0.17408 1.1014\n##    ICE_INCOME_all_LSKe -1.03050 -0.18046  0.17660  0.55386 1.8477\n##    Summary information for localized coefficient of variation:\n##                             Min.    1st Qu.     Median    3rd Qu.    Max.\n##    pctPOV_LCV            0.21281    0.41550    0.57740    0.73493  1.1784\n##    pctNOHS_LCV           0.31989    0.49239    0.85060    1.30476  2.2432\n##    pctMOVE_LCV           0.23460    0.35345    0.38736    0.42998  0.6638\n##    pctOWNER_OCC_LCV      0.20111    0.35046    0.42753    0.51556  0.7493\n##    ICE_INCOME_all_LCV -127.78594   -1.02921   -0.35211    1.03900 94.7817\n##    Summary information for localized Covariance and Correlation between these variables:\n##                                                     Min.      1st Qu.\n##    Cov_pctPOV.pctNOHS                       -0.002293092  0.000901241\n##    Cov_pctPOV.pctMOVE                       -0.002989390  0.000907888\n##    Cov_pctPOV.pctOWNER_OCC                  -0.039119689 -0.020228990\n##    Cov_pctPOV.ICE_INCOME_all                -0.054587425 -0.021777955\n##    Cov_pctNOHS.pctMOVE                      -0.006067192  0.000036009\n##    Cov_pctNOHS.pctOWNER_OCC                 -0.028576009 -0.005198261\n##    Cov_pctNOHS.ICE_INCOME_all               -0.027722207 -0.005410855\n##    Cov_pctMOVE.pctOWNER_OCC                 -0.026758619 -0.012236511\n##    Cov_pctMOVE.ICE_INCOME_all               -0.017867269 -0.008467689\n##    Cov_pctOWNER_OCC.ICE_INCOME_all           0.007196400  0.020528232\n##    Corr_pctPOV.pctNOHS                      -0.317000144  0.330201284\n##    Corr_pctPOV.pctMOVE                      -0.331272313  0.157747220\n##    Corr_pctPOV.pctOWNER_OCC                 -0.926032270 -0.768233160\n##    Corr_pctPOV.ICE_INCOME_all               -0.921214422 -0.855191644\n##    Corr_pctNOHS.pctMOVE                     -0.555196633  0.009497939\n##    Corr_pctNOHS.pctOWNER_OCC                -0.742254592 -0.495928778\n##    Corr_pctNOHS.ICE_INCOME_all              -0.793477505 -0.545592647\n##    Corr_pctMOVE.pctOWNER_OCC                -0.891153190 -0.746883164\n##    Corr_pctMOVE.ICE_INCOME_all              -0.817237195 -0.594811585\n##    Corr_pctOWNER_OCC.ICE_INCOME_all          0.476114554  0.736449688\n##    Spearman_rho_pctPOV.pctNOHS              -0.313709999  0.399932352\n##    Spearman_rho_pctPOV.pctMOVE              -0.338963728  0.215510831\n##    Spearman_rho_pctPOV.pctOWNER_OCC         -0.932742660 -0.772363010\n##    Spearman_rho_pctPOV.ICE_INCOME_all       -0.945687048 -0.876406885\n##    Spearman_rho_pctNOHS.pctMOVE             -0.407210551  0.057271829\n##    Spearman_rho_pctNOHS.pctOWNER_OCC        -0.828797842 -0.551819264\n##    Spearman_rho_pctNOHS.ICE_INCOME_all      -0.840537241 -0.647642983\n##    Spearman_rho_pctMOVE.pctOWNER_OCC        -0.885112112 -0.731419517\n##    Spearman_rho_pctMOVE.ICE_INCOME_all      -0.832992937 -0.560002101\n##    Spearman_rho_pctOWNER_OCC.ICE_INCOME_all  0.454870395  0.712341847\n##                                                   Median      3rd Qu.    Max.\n##    Cov_pctPOV.pctNOHS                        0.001807506  0.003304389  0.0180\n##    Cov_pctPOV.pctMOVE                        0.002229805  0.004467243  0.0099\n##    Cov_pctPOV.pctOWNER_OCC                  -0.014516859 -0.009563665 -0.0016\n##    Cov_pctPOV.ICE_INCOME_all                -0.014606879 -0.009577934 -0.0023\n##    Cov_pctNOHS.pctMOVE                       0.000331801  0.000748402  0.0038\n##    Cov_pctNOHS.pctOWNER_OCC                 -0.002496901 -0.001218701  0.0065\n##    Cov_pctNOHS.ICE_INCOME_all               -0.002789993 -0.001573582  0.0039\n##    Cov_pctMOVE.pctOWNER_OCC                 -0.008392739 -0.005565503  0.0056\n##    Cov_pctMOVE.ICE_INCOME_all               -0.005403826 -0.002219461  0.0149\n##    Cov_pctOWNER_OCC.ICE_INCOME_all           0.031021243  0.044464726  0.0689\n##    Corr_pctPOV.pctNOHS                       0.528380318  0.709155971  0.9313\n##    Corr_pctPOV.pctMOVE                       0.336717286  0.519424037  0.7694\n##    Corr_pctPOV.pctOWNER_OCC                 -0.701980159 -0.604820576 -0.3121\n##    Corr_pctPOV.ICE_INCOME_all               -0.805848933 -0.738051250 -0.2913\n##    Corr_pctNOHS.pctMOVE                      0.134766298  0.300808384  0.6833\n##    Corr_pctNOHS.pctOWNER_OCC                -0.380222665 -0.228294821  0.4213\n##    Corr_pctNOHS.ICE_INCOME_all              -0.446961901 -0.323081384  0.3913\n##    Corr_pctMOVE.pctOWNER_OCC                -0.613431373 -0.433099806  0.2776\n##    Corr_pctMOVE.ICE_INCOME_all              -0.458003819 -0.189021343  0.5593\n##    Corr_pctOWNER_OCC.ICE_INCOME_all          0.813864246  0.868133644  0.9618\n##    Spearman_rho_pctPOV.pctNOHS               0.556988603  0.704535569  0.9311\n##    Spearman_rho_pctPOV.pctMOVE               0.374243796  0.529475103  0.7608\n##    Spearman_rho_pctPOV.pctOWNER_OCC         -0.699827624 -0.621225648 -0.3108\n##    Spearman_rho_pctPOV.ICE_INCOME_all       -0.839312766 -0.774693178 -0.3117\n##    Spearman_rho_pctNOHS.pctMOVE              0.174120074  0.303105444  0.5853\n##    Spearman_rho_pctNOHS.pctOWNER_OCC        -0.428396144 -0.245467087  0.2446\n##    Spearman_rho_pctNOHS.ICE_INCOME_all      -0.524973642 -0.381356579  0.1699\n##    Spearman_rho_pctMOVE.pctOWNER_OCC        -0.619497465 -0.450139874  0.3082\n##    Spearman_rho_pctMOVE.ICE_INCOME_all      -0.428406930 -0.173601402  0.4005\n##    Spearman_rho_pctOWNER_OCC.ICE_INCOME_all  0.797280146  0.852406288  0.9477\n##    Summary information for Local median:\n##                               Min.   1st Qu.    Median   3rd Qu.   Max.\n##    pctPOV_Median          0.042468  0.090310  0.161004  0.251916 0.3919\n##    pctNOHS_Median         0.003663  0.014632  0.042506  0.068828 0.1053\n##    pctMOVE_Median         0.095830  0.158633  0.186185  0.201781 0.3108\n##    pctOWNER_OCC_Median    0.191905  0.391960  0.490220  0.602281 0.8389\n##    ICE_INCOME_all_Median -0.384111 -0.195011 -0.023930  0.170068 0.3830\n##    Summary information for Interquartile range:\n##                            Min.   1st Qu.    Median   3rd Qu.   Max.\n##    pctPOV_IQR         0.0272517 0.0846864 0.1150697 0.1709975 0.3370\n##    pctNOHS_IQR        0.0071992 0.0233457 0.0362078 0.0567798 0.1961\n##    pctMOVE_IQR        0.0492332 0.0777156 0.0943680 0.1143242 0.2195\n##    pctOWNER_OCC_IQR   0.0695205 0.2406716 0.2930468 0.3634722 0.6663\n##    ICE_INCOME_all_IQR 0.0741381 0.1853240 0.2307883 0.2932845 0.5597\n##    Summary information for Quantile imbalance:\n##                            Min.    1st Qu.     Median    3rd Qu.   Max.\n##    pctPOV_QI         -0.8423900 -0.3575769 -0.1613870  0.0836692 0.7498\n##    pctNOHS_QI        -0.9376945 -0.5163889 -0.2601448  0.0084147 0.4966\n##    pctMOVE_QI        -0.7152669 -0.2848576 -0.0713806  0.1454639 0.7001\n##    pctOWNER_OCC_QI   -0.7647186 -0.1424103  0.0614975  0.2285407 0.7302\n##    ICE_INCOME_all_QI -0.8670434 -0.2368184 -0.0310687  0.1636269 0.8803\n## \n##    ************************************************************************"},{"path":"disease-mapping-iii.html","id":"mapping-gwss-results","chapter":"Week 6 Disease Mapping III","heading":"6.4.3.1 Mapping gwss results","text":"see results? Try looking result:many sub-objects within main result object. first one, always called SDF class SpatialPolygonsDataFrame. basically sp version polygon spatial file. examine closely (e.g. try summary(atl.ss$SDF)) see happens) see information need make maps (e.g. spatial object attribute data).First, let’s map geographically-weighted median value statistics:can also examine local variation diversity values mapping geographically-weighted IQRRemember, places higher IQR larger local differences values. places high variability similar , different , places high median values?can now repeat code ICE_INCOME_all variable, also repeat variables looking mean SD rather median IQR. evidence local mean local median different?","code":"\nnames(atl.ss)##  [1] \"SDF\"      \"vars\"     \"kernel\"   \"adaptive\" \"bw\"       \"p\"       \n##  [7] \"theta\"    \"longlat\"  \"DM.given\" \"sp.given\" \"quantile\"\nsummary(atl.ss)##          Length Class                    Mode     \n## SDF      345    SpatialPolygonsDataFrame S4       \n## vars       5    -none-                   character\n## kernel     1    -none-                   character\n## adaptive   1    -none-                   logical  \n## bw         1    -none-                   numeric  \n## p          1    -none-                   numeric  \n## theta      1    -none-                   numeric  \n## longlat    1    -none-                   logical  \n## DM.given   1    -none-                   logical  \n## sp.given   1    -none-                   logical  \n## quantile   1    -none-                   logical\n# Map geographically-weighted Median\ntm_shape(atl.ss$SDF) + \n  tm_fill(c('pctPOV_Median', 'pctNOHS_Median', 'pctMOVE_Median', 'pctOWNER_OCC_Median'),\n          style = 'quantile') +\n  tm_borders()\n# Map geographically-weighted IQR\ntm_shape(atl.ss$SDF) + \n  tm_fill(c('pctPOV_IQR', 'pctNOHS_IQR', 'pctMOVE_IQR', 'pctOWNER_OCC_IQR'),\n          style = 'quantile') +\n  tm_borders()## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output"},{"path":"disease-mapping-iii.html","id":"calculating-pseudo-p-values-for-smoothed-estimates","chapter":"Week 6 Disease Mapping III","heading":"6.4.4 Calculating pseudo p-values for smoothed estimates","text":"motivation much disease mapping detection spatial heterogeneity spatial dependence epidemiologic data. Spatial heterogeneity statistical parameter means values truly different locations compared others. Spatial dependence random variable means values one location tend correlated values nearby locations values distant locations.related idea spatial stationarity implies value summary data (e.g. spatially-local mean) location independent. words divided study region 10 equal-sized sub-regions, mean value approximately . contrast, spatial non-stationarity means local summaries location-dependent. example estimate spatially local mean different one sub-region compared another.Note spatial non-stationarity implies heterogeneity parameter interest (values everywhere) spatial dependence observations (near values correlated distant values). heterogeneity, dependence expect, average, local summaries statistics still stationary.can restate definitions form testable hypotheses evaluate disease mapping analysis. First note example multiple candidate random variables (e.g. pctPOV, pctMOVE, ICE_INCOME_all, …), well multiple candidate statistical parameters (e.g. mean, median, SD, IQR, …). hone questions hand, let us assume interested describing mean value random variable pctPOV. use kernel density functions applied spatial data particularly well suited testing spatial stationarity versus spatial non-stationarity statistic parameters.noted , question spatial stationarity hinges largely presence spatial dependence versus spatial independence observed values. Therefore, null hypothesis, \\(H_0:\\), might posit observed values pctPOV independent one another, therefore spatially local mean estimate pctPOV location independent (e.g. summary one location , average, summary another location). alternative hypothesis, \\(H_A:\\), values pctPOV spatially dependent therefore spatially local mean estimate pctPOV location dependent (e.g. equal single global value, every location-specif value).can test hypothesis? seen previously, hypothesis testing spatial data made challenging complex structure data, difficulty making plausible assumptions conventional statistical rules.One effective empirical solution complexity carry Monte Carlo permutation testing null hypothesis. idea permutation testing, can empirically simulate believe data look like null distribution. can compare observed data simulated null distribution describe unusual observations , given expected due chance alone.Permutation testing particularly well-suited questions spatial independence versus spatial dependence, hard conceive means data values independent one another. example, imagine random variable, pctPOV measured 345 units, . null hypothesis spatial independence, geographic location measures irrelevant. example took exact vector \\(n=345\\) values pctPOV randomly changed geographic location, matter assume, null, geographic location irrelevant. randomly reassign vector \\(n=345\\) values pctPOV different locations many, many times begin see distribution arrangements null hypothesis, e.g. spatial independence.permutation testing ?First, measure interest case geographically-weighted average variable, say pctPOV. region spatially weighted average calculated weighted average ’s neighbors (defined kernel).null, assume value individual region’s pctPOV independent value value neighbors. Therefore, permutations empirical way approximate null assumption randomly re-assigning known values different geographic locations.time randomly reassign set locations, repeat process creating geographically weighted average variable, e.g. pctPOV. lot times, distribution geographically weighted pctPOV individual region look like null (spatial independence) true.can compare single observed realization geographically-weighted pctPov region long list hypothetical values (null) see typical unusual observed data . Essentially pseudo p-value just rank-ordered percentile observed data relation range values null.number random permutations guides precision eventual pseudo-p-value. p-value theoretically smaller null permutations. instance compare \\(n=1\\) observed realization data \\(n=99\\) null permutations extreme statement make data extreme \\(p = \\frac{1}{100} = 0.01\\) level. contrast \\(n=1\\) observed realizations \\(n=999\\) random permutations null extreme data \\(p = \\frac{1}{1000} = 0.001\\).function gwss.montecarlo() . specific context geographically weighted summary statistics, function follows 3-step process:First randomly reassign location variables interest \\(n\\) times (\\(n\\) specified user, typically reasonably large)Second, random permutation random variable (e.g. pctPOV), summary statistic (e.g. spatially-weighted local mean pctPOV) calculated.Finally, observed results (e.g. spatially-weighted mean pctPOV calculated using original gwss()) compared null distribution. calculated \\(n=999\\) random permutations, \\(n=1000\\) versions summarized statistic, including observed. pseudo p-value calculated number times spatial unit observed value extreme expected null. example defined extreme something happens fewer 5% time random chance alone, might classify observed value extreme (thus significant) observed value either less lower 2.5% null values, greater upper 97.5% null values.might sound like lot things happening. Mechanically relatively straightforward procedure, can time consuming, particularly lot permutations null. carry Monte Carlo permutation test geographically-weighted statistics single variable, pctPOV. arguments familiar, now must specify many permutations simulations wish using arguments nsim =. computer, took less 1 minute run \\(n=499\\) permutations. Note request \\(n=499\\) simulations, combined observed data, \\(n=500\\) total values spatially-weighted mean value pctPOV compare.can now examine result. First might find returned class matrix. notice columns 5 primary summary statistics estimated gwss_LM local mean_LSD local standard deviation_LVar local variance_LSKe local skewness_LCV local coefficient variationThe numbers column values range 0 1. numbers percentiles reflecting rank location single spatially weighted local mean pctPOV observed data compared \\(n=499\\) versions spatial location randomly assigned.calculate 2-side pseudo p-value conventional 0.05 threshold, interested census tracts observed data either lowest 2.5% highest 2.5% null distribution. words ask census tracts observed spatially-weighted local mean value extreme compared happen chance alone.NOTE:percentile values come specific set randomly distributed simulations. Repeating procedure produce slightly different values printed simply due random variation. based Central Limit Theorem, believe number simulations grows larger, consistent results .However result returned object p.val easy use just . convert something map? , can test census tracts extreme definition, make new spatial object includes significant tracts.Now can use results create map summarizing evidence relation null hypothesis geographically-weighted mean value pctPOV stationary, alternative hypothesis least locations significantly extreme local values expected null.see visual inspection geographically-weighted mean pctPOV suggests great deal spatial heterogeneity apparent spatial non-stationarity. However, permutation test suggests regions far North Fulton West Atlanta values extreme might expect assumption spatial independence.NOTE:important remember hypothesis testing! test whether poverty rate zero, test whether poverty rate different specific census tracts compared others.Instead, specifically test whether spatial dependence data give rise unexpectedly extreme local measures assumptions KDE specified neighbors.","code":"\np.val <- gwss.montecarlo(atl, vars = c('pctPOV', 'pctMOVE'), \n                         adaptive = T,\n                         bw = 35,\n                         nsim = 499)\n\nsummary(p.val)##    pctPOV_LM        pctMOVE_LM       pctPOV_LSD      pctMOVE_LSD    \n##  Min.   :0.0020   Min.   :0.0000   Min.   :0.0020   Min.   :0.0020  \n##  1st Qu.:0.2520   1st Qu.:0.2500   1st Qu.:0.2460   1st Qu.:0.2520  \n##  Median :0.4980   Median :0.4920   Median :0.5060   Median :0.5000  \n##  Mean   :0.5008   Mean   :0.5002   Mean   :0.4992   Mean   :0.4986  \n##  3rd Qu.:0.7540   3rd Qu.:0.7440   3rd Qu.:0.7420   3rd Qu.:0.7400  \n##  Max.   :0.9980   Max.   :0.9980   Max.   :0.9980   Max.   :0.9980  \n##   pctPOV_LVar      pctMOVE_LVar     pctPOV_LSKe      pctMOVE_LSKe   \n##  Min.   :0.0020   Min.   :0.0020   Min.   :0.0020   Min.   :0.0000  \n##  1st Qu.:0.2460   1st Qu.:0.2520   1st Qu.:0.2540   1st Qu.:0.2440  \n##  Median :0.5060   Median :0.5000   Median :0.4940   Median :0.5060  \n##  Mean   :0.4992   Mean   :0.4986   Mean   :0.4993   Mean   :0.5008  \n##  3rd Qu.:0.7420   3rd Qu.:0.7400   3rd Qu.:0.7460   3rd Qu.:0.7520  \n##  Max.   :0.9980   Max.   :0.9980   Max.   :0.9980   Max.   :0.9980  \n##    pctPOV_LCV      pctMOVE_LCV     Cov_pctPOV.pctMOVE Corr_pctPOV.pctMOVE\n##  Min.   :0.0020   Min.   :0.0040   Min.   :0.0020     Min.   :0.0020     \n##  1st Qu.:0.2500   1st Qu.:0.2400   1st Qu.:0.2460     1st Qu.:0.2500     \n##  Median :0.5080   Median :0.4940   Median :0.5140     Median :0.5020     \n##  Mean   :0.5007   Mean   :0.4999   Mean   :0.4997     Mean   :0.4996     \n##  3rd Qu.:0.7460   3rd Qu.:0.7420   3rd Qu.:0.7480     3rd Qu.:0.7400     \n##  Max.   :0.9980   Max.   :0.9980   Max.   :0.9980     Max.   :0.9980     \n##  Spearman_rho_pctPOV.pctMOVE\n##  Min.   :0.0000             \n##  1st Qu.:0.2560             \n##  Median :0.5000             \n##  Mean   :0.4996             \n##  3rd Qu.:0.7560             \n##  Max.   :0.9980\n# First, create TRUE/FALSE vectors testing whether column 1 (pctPOV_LM) is extreme\n# I am using 2 significance levels: 90% and 95%\nsig95 <- p.val[, 1] < 0.025 | p.val[, 1] > 0.975\nsig90 <- p.val[, 1] < 0.05 | p.val[, 1] > 0.95\n\n# Second create a spatial object that ONLY contains significant tracts\natl.sig95 <- atl[sig95, ] %>% \n  aggregate(dissolve = T, FUN = mean) # this is sp code to merge adjacent tracts\n\natl.sig90 <- atl[sig90, ] %>%\n  aggregate(disolve = T, FUN = mean)\ntm_shape(atl.ss$SDF) + \n  tm_fill('pctPOV_LM', \n          style = 'quantile',\n          title = 'Local Average % Poverty') + \n  tm_borders() + \ntm_shape(atl.sig90) +\n  tm_borders(lwd = 2, col = 'blue') + \ntm_shape(atl.sig95) + \n  tm_borders(lwd = 2, col ='red') +\ntm_add_legend(type = 'line',\n              labels = c('p < 0.05', 'p < 0.10'),\n              col = c('red', 'blue'))## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output"},{"path":"disease-mapping-iii.html","id":"estimating-geographically-weighted-bivariate-statistics","chapter":"Week 6 Disease Mapping III","heading":"6.4.5 Estimating geographically-weighted bivariate statistics","text":"final bits information examine geographically-weighted summary statistics function gwss() bivariate correlations covariances. time two variables supplied gwss() function, automatically calculate correlation coefficients (Pearson Spearman), well measures covariance, every pair variables.now, seen KDE function can produce smoothed estimate local mean, median, SD, etc. can also show whether correlation pairs variables spatially stationary (everywhere), spatially non-stationary (varies location).Two things illustrated maps.First, appears magnitude correlation among pairs variables larger areas smaller others.second, spatial patterns correlation pctPOV two variables distinct. words areas correlation relatively stronger weaker .might ask whether differences extreme might expect null hypothesis spatial independence spatial stationarity.Previously conducted Monte Carlo permutation test single variable, pctPOV. provide two variables gwss.montecarlo() function, get pseudo p-values univariate bivariate statistics. NOTE takes time work computer . code took little 1-minute computer.can use summary() dimnames() figure column want. want get permutation p-value Spearman_rho_pctPOV.pctMOVE, 13th column matrix.Now can map correlation pctPOV pctMOVE along significance test.","code":"\ntm_shape(atl.ss$SDF) +\n  tm_fill(c('Spearman_rho_pctPOV.pctNOHS',\n            'Spearman_rho_pctPOV.pctMOVE',\n            'Spearman_rho_pctPOV.ICE_INCOME_all'),\n          style = 'quantile') +\n  tm_borders()## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output## Variable(s) \"Spearman_rho_pctPOV.pctNOHS\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.## Variable(s) \"Spearman_rho_pctPOV.pctMOVE\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\np.val <- gwss.montecarlo(atl, vars = c('pctPOV', 'pctMOVE'), \n                         adaptive = T,\n                         bw = 35,\n                         nsim = 499)## Warning in proj4string(data): CRS object has comment, which is lost in output\n# First, create TRUE/FALSE vectors testing whether column 1 (pctPOV_LM) is extreme\n# I am using 2 significance levels: 90% and 95%\nsig95 <- p.val[, 13] < 0.025 | p.val[, 13] > 0.975\nsig90 <- p.val[, 13] < 0.05 | p.val[, 13] > 0.95\n\n# Second create a spatial object that ONLY contains significant tracts\natl.sig95 <- atl[sig95, ] %>% \n  aggregate(dissolve = T, FUN = mean) \n\natl.sig90 <- atl[sig90, ] %>%\n  aggregate(disolve = T, FUN = mean)\ntm_shape(atl.ss$SDF) + \n  tm_fill('Spearman_rho_pctPOV.pctMOVE', \n          style = 'quantile',\n          title = 'Local correlation Poverty\\n& Residential instability') + \n  tm_borders() + \ntm_shape(atl.sig90) +\n  tm_borders(lwd = 2, col = 'blue') + \ntm_shape(atl.sig95) + \n  tm_borders(lwd = 2, col ='red') +\ntm_add_legend(type = 'line',\n              labels = c('p < 0.05', 'p < 0.10'),\n              col = c('red', 'blue'))## Variable(s) \"Spearman_rho_pctPOV.pctMOVE\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."},{"path":"disease-mapping-iv.html","id":"disease-mapping-iv","chapter":"Week 7 Disease Mapping IV","heading":"Week 7 Disease Mapping IV","text":"","code":""},{"path":"disease-mapping-iv.html","id":"getting-ready-5","chapter":"Week 7 Disease Mapping IV","heading":"7.1 Getting Ready","text":"","code":""},{"path":"disease-mapping-iv.html","id":"learning-objectives-6","chapter":"Week 7 Disease Mapping IV","heading":"7.1.1 Learning objectives","text":"","code":""},{"path":"disease-mapping-iv.html","id":"additional-resources-6","chapter":"Week 7 Disease Mapping IV","heading":"7.1.2 Additional Resources","text":"Waller LA, Carlin BP. Disease mapping. Chapman Hall/CRC handbooks Mod Stat methods. 2010;2010(1979):217–43. (posted Canvas)CARBayes package vignetteCARBayesST spatio-temporal vignette","code":""},{"path":"disease-mapping-iv.html","id":"important-vocabulary-6","chapter":"Week 7 Disease Mapping IV","heading":"7.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 7","code":""},{"path":"disease-mapping-iv.html","id":"spatial-thinking-in-epidemiology-5","chapter":"Week 7 Disease Mapping IV","heading":"7.2 Spatial Thinking in Epidemiology","text":"","code":""},{"path":"disease-mapping-iv.html","id":"what-is-bayesian-inference","chapter":"Week 7 Disease Mapping IV","heading":"7.2.1 What is Bayesian Inference?","text":"Disease Mapping & II introduced global (aspatial) local (spatial) Empirical Bayes estimation. modules, introduced Bayes Theorem, high-level idea importance prior, likelihood, posterior Bayesian inference.However (intentionally) skirted much detail sections. section go little bit deeper; clear lot know learn Bayesian inference presented . hopefully summary helps motivate use fully Bayesian analysis spatial epidemiology.","code":""},{"path":"disease-mapping-iv.html","id":"frequentist-versus-bayesian-inference","chapter":"Week 7 Disease Mapping IV","heading":"7.2.1.1 Frequentist versus Bayesian Inference","text":"Frequentist statistics inference probably learned ‘statistics’ now. words, typical Bayesian inference taught depth, even , many statistics courses. interesting history current dominance frequentist inference much personalities, egos, power utility. ’s another day.core idea frequentist inference centers mental model premised comparing data observed abstract thought experiment expected infinite repetitions. strategy developed agricultural trials survey sampling; words settings meaningful think either repetitively resampling finite subset large population, repetitively conducting experiment order conceive parameter might expected vary simply due random error.Bayesian inference refers alternate philosophical statistical approach analysis inference observed data. Instead assuming frequency often something happen (e.g. abstract empirical thought experiment), Bayesian inference combines mental models.Bayesian’s articulate statement plausible distribution parameter given past experience knowledge (e.g. prior), combine directly data actually suggest. result combination updated statement distribution parameter (e.g. posterior).common critique Bayesian inference priors introduce subjective information compared objective assumptions frequentist inference. Instead, Bayesian priors simply explicit transparent assumptions made; contrast unrealistic unstated assumptions required frequentist inference.\nFIGURE 2.1: Frequentist vs. Bayesian Inference\ncartoon truth universe (e.g. sun exploded: true false) measured neurino detector. measurement almost always reports truth ’s measurement, rolls double-six dice, lies . measure occurs, dice rolled answer “sun exploded.”frequentist statistician left, finds probability telling lie small, given answer “sun exploded,” sun must exploded (e.g. null rejected). Bayesian statistician right skeptical.exaggerated example role prior belief. strictly frequentist interpretation, matters probability observed data hypothetical range possibilities stated null.Obviously silly cartoon. interesting discussion joke might also based incorrect interpretation frequentist statistics link.One nuance brushed aside simplified cartoon frequentist terms, joint hypothesis. Instead implied single hypothesis, “probability sun exploded?” illustrated dual hypothesis, “probability sun exploded neurino detector told lie?” clear way make logical flaw thinking illustrated fit well frequentist well Bayesian framework.","code":""},{"path":"disease-mapping-iv.html","id":"bayesian-inference-in-spatial-epidemiology","chapter":"Week 7 Disease Mapping IV","heading":"7.2.2 Bayesian inference in spatial epidemiology","text":"Bayesian statistics widely incorporated statistical methods across disciplines, Disease Mapping perhaps ubiquitous use Bayesian inference epidemiology. Aside appealing flexibility Bayesian inference generally, two specific reasons Bayesian inference makes sense disease mapping:Borrowing statistical strength spatial (even spatio-temporal) neighbors, efficient way improve reliability precision small area disease rate estimates. already seen spatial Empirical Bayes estimation, different way Kernel Density Estimation. Leveraging notion near things tend alike far thing, incorporation spatial neighbors source prior information can reduce variance estimates, smooth shrink implausible extreme values.Modeling spatial auto correlation explicitly important statistics conventionally rely assumptions independence among observations. Therefore, disease rates two adjacent counties correlated shared environment, demographic structure, interaction, dependence can result biased parameter estimates. Empirical Bayes smoothing explicitly address , fully Bayesian models spatial priors can explicitly model auto correlation, thus allowing estimation likelihood assumptions conditional independence.Bayesian statistics inherently complex frequentists statistics. However, likely substantially opportunity assimilate ideas frequentist thinking, thus Bayesian statistics may feel quite foreign. two concepts incorporate disease mapping Bayesian framework: hierarchical models (conditional autoregressive (CAR)) prior.","code":""},{"path":"disease-mapping-iv.html","id":"bayesian-hierarchical-models","chapter":"Week 7 Disease Mapping IV","heading":"7.2.2.1 Bayesian hierarchical models","text":"Bayesian models hierarchical sense conceive parameters discrete point estimates, instead range plausible values described probability distribution function (PDF). Thus target distribution parameters might lower-level hierarchy, parameters given PDF (e.g. mean variance), assumed arise random probability distribution, representing another hierarchical level.Thus, describe probability random variables lowest level (e.g. perhaps excess relative risk disease \\(region_i\\) compared expected), need specify “second level,” possibly third level hierarchical fashion.Take example interest disease mapping characterizing spatial heterogeneity, specifically value region-specific excess relative risk indicator deviation expectation.previous settings notated likelihood excess relative risk \\(\\theta_i\\); examine \\(\\theta_i\\) log scale, defining related parameter \\(\\psi\\) (spelled psi pronounced like sigh): \\(\\psi_i = log(\\theta_i)\\). Therefore following two statements observed data, \\(Y_i\\) probability model say thing:\\[Y_i | \\beta, \\psi_i \\sim Poisson(E_i exp(x_i \\beta + \\psi_i))\\]\n\\[Y_i| \\theta_i \\sim Poisson(E_i \\theta_i)\\]\ndifference two likelihood statements :) first log scale whereas second ;b) first explicitly incorporates possible covariates resulting parameters, \\(\\beta\\).Importantly, time use one Greek letters, saying represents random variable. Therefore, Bayesian framework, must specify distribution parameters random variable. distributional specifications prior!differ prior Empirical Bayes specify full probability distributions, rather discrete values calculated observed (empirical) data. model becomes hierarchical. first equation two random variables: \\(\\beta_i\\) \\(\\psi_i\\). requires specification prior.\\[\\beta \\sim N(0,100000)\\]\nparticular example, specified prior \\(\\beta\\) (e.g. prior range plausible coefficients possible included covariates model, rural/urban population density) relatively uninformative.words, saying \\(\\beta\\) arises normal distribution mean zero (e.g. average expect association) variance 100,000, saying little specific prior information; therefore data (e.g. likelihood) win cases. common practice fixed effects (e.g. global summary stationary parameters assumed vary space), quite similar frequentist assumption anything possible.However, strategy prior \\(\\psi_i\\) little different. Recall, discussing global local (aspatial spatial) Empirical Bayes, chief distinction specification prior: global/aspatial single prior expectation entire study region, whereas local/spatial EB, unique prior region, defined part values neighboring regions.similar approach taken fully Bayesian disease modeling. One approach define single prior applies region, irrespective spatial relatedness one another. often called spatially unstructured random effect. words random variable defined spatial connectivity, instead arises non-spatial phenomenon. Instead uninformative, incorporate prior knowledge form information plausible range values across study region.\\[\\psi_i \\sim N(0,\\sigma^2)\\]says range possible values \\(\\psi\\) arise normal distribution, centered zero (e.g. expectation region exactly expected), variance \\(\\sigma^2\\).mentioned , Empirical Bayes, value variance term (e.g. specification different think regions can plausibly one another) specified using empirical observed data. However, hierarchical Bayesian setting, can go yet another level say even \\(\\sigma^2\\) random variable prior . example common prior variance term (prior prior called hyperprior!) :\\[\\sigma^2 \\sim inverse-gamma(1, 0.01)\\]\ninverse gamma distribution specified two parameters, \\(\\alpha\\) \\(\\beta\\). Theoretically specify yet another hyper prior two parameters, example –following convention–specify numeric values 1 0.01. example prior distribution looks like (recall \\(\\sigma^2\\) describes variance log scale):summary hierarchical Bayesian models explicitly incorporated spatial relatededness neighbors. next section introduces spatial prior.","code":""},{"path":"disease-mapping-iv.html","id":"conditional-auto-regressive-priors","chapter":"Week 7 Disease Mapping IV","heading":"7.2.2.2 Conditional auto-regressive priors","text":"mentioned previous section, just global local priors Empirical Bayes, fully Bayesian disease mapping. global prior \\(\\psi_i\\) described arising common shared normal distribution regions.helps address concern instability estimates due sparse data single region, shrinking smoothing local regions towards global distribution. However, strategy address ubiquitous presence spatial-dependence. words global strategy neither addresses violation assumption independence among regions (e.g. often actually quite dependent auto-correlated!), take advantage fact provide stronger priors.Conditional Auto Regressive (CAR) model commonly used frequentist Bayesian spatial statistics. particular, informs estimation local log-relative risk parameter, \\(\\psi_i\\) conditioning information neighbors. CAR model setup data assumed distributed normally (e.g. Gaussian), mean variance defined conditional neighbors. fully Bayesian framework, can use CAR conditioning parameterize (e.g. prior ) values local \\(\\psi_i\\).CAR model incorporated multiple different types Bayesian priors disease mapping. basic setup CAR model described :\\[\\psi_i|\\psi_{j \\neq } \\sim N\\left(\\frac{\\sum_{j \\neq } w_{ij}\\psi_i}{\\sum_{j \\neq }w_{ij}}, \\frac{1}{\\tau_{CAR}\\sum_{j \\neq }w_{ij}}\\right)\\]says values \\(\\psi_i\\) (e.g. local log-relative excess risk) normally distributed, conditional values \\(\\psi_i\\) neighbors \\(\\). mean region-specific normal distribution weighted average values \\(\\psi_i\\) neighbors, variance distribution informed \\(\\tau_{CAR}\\), hyperprior denoting conditional variance among neighbors. term \\(w_{ij}\\) binary spatial weights matrix created much spatial Empirical Bayes identifying neighboring adjacent units \\(\\) \\(j\\) \\(w_{ij}=1\\) non-adjacent non-neighbor pairs \\(\\) \\(j\\) \\(w_{ij}=0\\)commonly used spatial prior Bayesian disease mapping called Besag-Yorke-Mollie BYM prior. combines spatially-explicit CAR prior characterize parts spatial heterogeneity spatially structured (e.g. related spatial dependence data), along global spatially unstructured Gaussian prior described previous section.idea sources variation regions fact spatially dependent (e.g. diffusion, selection similar populations, common exposure, etc), whereas sources difference spatially dependent (e.g. abrupt changes rural-suburban-urban, might region-specific exposures shared neighbors). combination types prior sometimes called convolution priors combine two separate random effects together.can describe fully hierarchical Bayesian BYM model like :\\[Y_i|\\beta, \\psi_i \\sim Poisson(E_i, exp(\\beta, \\psi_i))\\]\nnow specifying local values \\(\\psi_i\\) contributed two distinct random components (e.g. one spatially structured one unstructured), can define \\(psi_i\\) sum two parts: \\(\\psi_i = u_i + v_i\\), \\(u_i\\) spatially structured random variable \\(v_i\\) unstructured random variable. therefore needs prior:\\[u_i|u_{j \\neq } \\sim N\\left(\\frac{\\sum_{j \\neq } w_{ij}u_i}{\\sum_{j \\neq }w_{ij}}, \\frac{1}{\\tau_{CAR}\\sum_{j \\neq }w_{ij}}\\right)\\]\n\\[v_i \\sim N(0,\\sigma^2)\\]Finally, need specify hyperpriors two variance terms, \\(\\tau_{CAR}\\), \\(\\sigma^2\\); can defined relatively non-informative manner using Gamma Inverse Gamma distributions allow wide range possibilities.specification Bayesian disease mapping priors using CAR model can seen additional resources cited beginning section.Somewhat confusingly, CARBayes package described uses slightly different Greek letter nomenclature. Specifically, package authors use \\(\\psi\\) indicate spatially-correlated structure random effects, describes set random effects (e.g. convolution model spatially structured unstructured random effects)–\\(\\psi\\) one component–Greek letter \\(\\phi\\) (spelled phi pronounced like figh).models CARBayes package, spatially-correlated spatially unstructured random effects, Leroux, \\(\\psi_i\\) = \\(\\phi_i\\). point model output matrix named phi, might seem confusing calling thing psi (\\(\\psi\\)).","code":""},{"path":"disease-mapping-iv.html","id":"making-inference-from-bayesian-models","chapter":"Week 7 Disease Mapping IV","heading":"7.2.2.3 Making inference from Bayesian Models","text":"basic logic Bayesian inference relatively straightforward, can see Bayesian hierarchical framework looks complex! simple settings possible calculate full posterior distribution (e.g. combination likelihood prior via Bayes Theorem) using closed form strategies.However common closed-form solution exists complex, hierarchical conditional models. Therefore, currently two analytic strategies used make inference simple formula doesn’t work.Markov Chain Monte Carlo (MCMC) simulation used decades Bayesian analysis. ‘brute force’ method takes advantage two statistical tools make inference shape posterior distribution even complex Bayesian models.Markov Chain algorithm drawing (possibly highly dimensional) parameter space. uses stochasticity (randomness) ‘check’ different possible parts parameter space, using comparison well current location fits compared previous. result algorithm can ‘learn’ without getting stuck one location. goal eventually (random sampling plus learning) settle likely answer parameter value.Monte Carlo simulations repetitive sampling drawing posterior. describe exactly shape posterior distribution, take large number samples distribution (e.g. Markov Chain) can create summary shape posterior. instance, mean median large number samples becomes parameter point estimate, \\(2.5^{th}\\) \\(97.5^{th}\\) percentiles samples become Bayesian Credible Intervals.Integrated Nested Laplace Approximation (INLA). much newer strategy aims provide efficient way approximate shape scale posterior. INLA works R especially well suited complex hierarchical, spatial, spatio-temporal models areal point data.","code":""},{"path":"disease-mapping-iv.html","id":"spatial-analysis-in-epidemiology-4","chapter":"Week 7 Disease Mapping IV","heading":"7.3 Spatial Analysis in Epidemiology","text":"Bayesian analysis requires bit care caution part analyst. strongly recommend proceeding project great caution (ideally expert consultant!). However, tools made Bayesian modeling accessible, part pre-programming ‘sensible’ defaults functions.module, discuss MCMC methods implemented single package, CARBayes, package represents reasonable point--entry interested starting Bayesian path. However excellent tutorial resources learning INLA, many Bayesian tools well.CARBayes package functions fitting wide range spatial disease mapping models including:Besag-York-Mollie (BYM) described , spatial heterogeneity modeled sum two random processes: spatially structured process spatial CAR prior; spatially independent unstructured processLeroux CAR model extension CAR single random effect (e.g. two BYM), variable degree spatial autocorrelation parameterized random hyperprior, \\(\\rho\\) describes places might versus less spatial dependence.Dissimilarity model uses CAR prior identify boundaries risk rate abruptly changes. model therefore highlights distinct differences amongst neighbors opposed encouraging similarity; can useful identifying spatial clustering.Localised CAR model another extension similar dissimilarity model aims identify abrupt changes surface highlight clusters.Multilevel CAR models nice alternative access individual level outcomes nested within areal (ecologic) units, opposed relying counts aggregated units.Multivariate Leroux model distinct preceding models univariate, meaning single ‘outcome’ unit observation. multivariate analysis (distinct common multivariable analysis multiple regression), multiple correlated outcomes unit analysis. One example modeling incidence three kinds cancer simultaneously.models can fit Poisson, Binomial, Normal (Gaussian), Multinomial distributed data.CARBayes package vignette provides additional detail specification different models, examples fitting using built-functions. addition, sister package, CARBayesST extensions spatio-temporal data, regions observed multiple times. information package available CARBayesST vignetteThe example uses commonly implemented Besag-York-Mollie (BYM) model.","code":""},{"path":"disease-mapping-iv.html","id":"packages-data","chapter":"Week 7 Disease Mapping IV","heading":"7.3.1 Packages & Data","text":"addition now-familiar packages, also need load CARBayes package.example continue use low birthweight data used previous parts eBook. following code reads sf calculates raw rate VLBW use subsequent comparisons.","code":"\nlibrary(sf)        # sf provides st_read for importing\nlibrary(spdep)     # spdep has functions for creating spatial neighbor objects## Loading required package: spData## To access larger datasets in this package, install the spDataLarge\n## package with: `install.packages('spDataLarge',\n## repos='https://nowosad.github.io/drat/', type='source')`\nlibrary(tmap)      # tmap for plotting results\nlibrary(dplyr)     # dplyr for pipe processing of data\nlibrary(CARBayes)  # CARBayes has functions for fitting a range of CAR models## Loading required package: MASS## \n## Attaching package: 'MASS'## The following object is masked from 'package:dplyr':\n## \n##     select## Loading required package: Rcpp## Registered S3 method overwritten by 'GGally':\n##   method from   \n##   +.gg   ggplot2\nvlbw <- st_read('ga-vlbw.gpkg') %>%\n  mutate(rate = VLBW / TOT )\n\nr <- sum(vlbw$VLBW) / sum(vlbw$TOT)\nvlbw$expected <- r*vlbw$TOT"},{"path":"disease-mapping-iv.html","id":"preparing-for-carbayes","chapter":"Week 7 Disease Mapping IV","heading":"7.3.2 Preparing for CARBayes()","text":"addition usual preparation analytic data set, primary concern fitting Bayesian CAR model creation weights matrix, W, serves identify set neighbors county serve inputs describing shape prior probability distribution. can use tools previous weeks creating range neighbor objects, following caveats:Neighbors (weights) must symmetric, means \\(region_i\\) neighbor \\(region_j\\), \\(region_j\\) also neighbor \\(region_i\\). Contiguity graph-based neighbor objects symmetric design, k-nearest neighbors often asymmetric. Thus, created k-nearest neighbors object may need force symmetry using function make.sym.nb().regions must least one neighbor. formally, sum rows weights matrix must least 1. neighbor approach results unlinked regions (areas zero neighbors, case islands), need excluded, alternate adapted weights matrix created.object use CARBayes function must literal weights matrix (e.g. \\(159 \\times 159\\)) just nb object.create simple Queen contiguity neighbor object, convert object weights matrix. use style = 'B' creation weights matrix says values resulting matrix binary (0 1). default (style = 'W') results row-standardized weights, useful analytic tasks, necessary CAR models, CAR prior inherently adjusts number neighbors.Make sure weights match final data!always important spatial neighbors weights objects made final dataset! changes (additions deletion rows, also re-sorting rearranging) result misalignment spatial weights matrix data.","code":"\nqnb <- poly2nb(vlbw)\nqnb_mat <- nb2mat(qnb, style = 'B')\n\ndim(qnb_mat)  # confirming the dimensions of the matrix## [1] 159 159\nclass(qnb_mat) # confirming its class## [1] \"matrix\" \"array\""},{"path":"disease-mapping-iv.html","id":"how-many-monte-carlo-samples-are-needed","chapter":"Week 7 Disease Mapping IV","heading":"7.3.2.1 How many Monte Carlo Samples are needed?","text":"depends complex model , strong signal data . general concepts worth mentioning.First, tendency (randomly selected) starting location Markov Chain influence early samples. reason common plan discard portion samples burn-period. essentially means hope dependence initial conditions removing first \\(n\\) samples. burn-can quite large, e.g. tens thousands samples!goal model convergence achieved, meaning Markov Chain ‘learned’ enough settle relatively consistent area parameter space. can take many thousands samples, thus convergence diagnostics often used guide decisions many samples required.end day, need \\(n=1000\\) reliable high quality samples posterior accurately describe . may take \\(10,000\\), \\(50,000\\) even \\(100,000\\) samples achieve preceding goals burn-convergence. One option just keep last \\(1,000\\) samples. preferable option use thinning sample every \\(10^{th}\\) every \\(100^{th}\\) sample, burn-period. achives two goals: requires less memory store samples, also reduces residual auto-correlation among sequential samples.","code":""},{"path":"disease-mapping-iv.html","id":"fitting-a-besag-york-mollie-bym-bayesian-model","chapter":"Week 7 Disease Mapping IV","heading":"7.3.3 Fitting a Besag-York-Mollie (BYM) Bayesian model","text":"fitting model, convenient (required) specify fixed-effect component model. specify outcome predictors. CAR Bayesian models can incorporate covariates categorical, continuous, even non-linear (e.g. spline polynomials) likelihood. two reasons might choose include covariates:Covariates strongly predictive outcome improve prediction local fitted rates. One interpretation random effects (e.g. \\(\\psi_i\\)), represent unmeasured causes correlates outcome. Addition relevant covariates explain previously-unmeasured factors.second reason may interest describing geographic trends disease conditional covariate. example used previously local age structure, although covariates might nuisance interpreting geographic patterns disease.now, covariates, thing fixed-effect portion model specification outcome variable (count deaths) offset variable (log denominator risk death), necessary Poisson model counts regions different populations risk. Note creating formula object. anything naming formula.Now three main ingredients:Data, form vlbw objectThe spatial weights matrix, W, represents spatial relationships (qnb_mat)fixed effects portion model.call BYM model specify:Poisson Poisson-Gamma?Notice family = 'poisson', even though previous work observed may extra-Poisson variation data. led us prefer Poisson-Gamma probability model (negative binomial); choose ?part fully Bayesian CAR model actually allowing extra-Poisson variation conditional prior, Gamma prior variance.formula, family, data, W discussed. next three arguments require explanation.burnin = 30000 specifies many MCMC samples discarded. general, discarding large number beginning via burnin argument recommended reduce sensitivity initial conditions ignore time spent early Markov Chain process.n.sample = 60000 specifies total number samples draw. Obviously number must larger burnin else nothing left look ! case take 60,000 samples, discarding first 30,000, leaving 30,000 examination.thin = 30 says keep every 30th sample drawn. reasons thinning reduce auto correlation among consecutive values, save memory, keeping useful number samples describe posterior. keeping 1000 samples, adequate summarizing parameters interest.actually many options analyst can choose specifying BYM model. instance, mentioned appealing feature CARBayes package built-number sensible defaults models, analyst doesn’t make many decisions.However, defaults can changed. example, inverse gamma prior variance, \\(\\tau^2\\), default settings, can modified additional arguments. might able digest now, might useful look briefly help documentation S.CARbym.","code":"\nmod <- VLBW ~ offset(log(TOT))\nbym <- S.CARbym(formula = mod, \n                        family = 'poisson', \n                        data = vlbw, \n                        W = qnb_mat,\n                        burnin = 30000, \n                        n.sample = 60000, \n                        thin = 30,\n                        verbose = FALSE)"},{"path":"disease-mapping-iv.html","id":"summarizing-carbayes-model-output","chapter":"Week 7 Disease Mapping IV","heading":"7.3.4 Summarizing CARBayes model output","text":"summary() function returns list objects returned, nothing useful. print() function provides subset model summary statistics.total parameters estimated summarized . Specifically print() function display fixed-effects (global intercept included specify covariates), well hyper priors, \\(\\tau^2\\) \\(\\sigma^2\\).parameter \\(\\tau^2\\) characterizes conditional variance spatially structured random effects (e.g. \\(u_i\\) BYM convolution prior).parameter \\(\\sigma^2\\) characterizes variance unstructured global random effects.clear variability \\(\\tau^2\\) \\(\\sigma^2\\) suggesting total variability county-specific prevalence VLBW, seems attributable spatially-structured processes compared unstructured processes.note cautionAlthough appealing interpret two variance components BYM model just (e.g. describing proportionate contribution total variation), also known BYM models particular, two clearly identified independent one another. words together can describe variation, safe make inference either separate .Instead can confident sum two reliable prior.Also evident results print(bym) results hyper parameters, fixed-effects, summarized median value posterior samples (e.g. n=1,000 retained posterior samples case), well \\(2.5^{th}\\) \\(97.5^{th}\\) percentiles posterior (e.g. Bayesian Credible Intervals).Deviation Information Criteria (DIC) Bayesian model fit statistic, like fit statistics, smaller better.Finally, Geweke Diagnostic test designed characterize well Markov Chain converged stationary location parameter space. assumption Markov Chain, steps chain move towards optimum (best fitting) values, remain area. Thus Geweke compares mean value sampled posterior end samples, earlier point.model converged, two means similar. resulting test statistic standard Z-score. Values -1.96 1.96 suggestive good convergence, whereas values greater 1.96, less -1.96, may converged.model 60,000 total samples (30,000 burn-), Geweke diagnostic suggests good convergence intercept perhaps poor convergence two variance hyperpriors. model re-fit iterations see improves convergence. approaches include better model-specification (e.g. perhaps important variables missing, making model non-identifiable).","code":"\nsummary(bym)\nprint(bym)## \n## #################\n## #### Model fitted\n## #################\n## Likelihood model - Poisson (log link function) \n## Random effects model - BYM CAR\n## Regression equation - VLBW ~ offset(log(TOT))\n## Number of missing observations - 0\n## \n## ############\n## #### Results\n## ############\n## Posterior quantities and DIC\n## \n##              Median    2.5%   97.5% n.effective Geweke.diag\n## (Intercept) -3.9964 -4.0460 -3.9472      1144.9         1.6\n## tau2         0.1088  0.0451  0.2173       245.4        -1.3\n## sigma2       0.0078  0.0022  0.0314       138.4         1.0\n## \n## DIC =  888.9664       p.d =  54.69138       LMPL =  -457.08"},{"path":"disease-mapping-iv.html","id":"making-inference-from-bayesian-posteriors","chapter":"Week 7 Disease Mapping IV","heading":"7.3.5 Making inference from Bayesian posteriors","text":"output S.CARbym() model function complex. use summary() names() output object, see several components. one called samples contains posterior draws MCMC process. Within object several data matrices, containing posterior samples different parameters. can understand little bit looking shape dimension dim().One matrices inside bym object named fitted.values. interest , instead characterizing heterogeneity \\(\\psi_i\\) (e.g. log-relative risk), wish map model-fitted rates.Fitted values scale observed data. means case Poisson model, fitted values counts VLBW predicted model. see , counts divided known denominator (total birth count) produce model-predicted risk prevalence.However, want know posterior specific random variables, \\(\\beta\\) look bym$samples$beta; want know random effects \\(\\psi_i\\), look bym$samples$psi.One thing might like examine Markov Chain trace plot understand sampled parameter space sequential steps. useful indicator convergence (e.g. trace plot settled common range, likely converged, whereas wanders place, ).Another visualization can useful see shape sampled posterior probability distribution. package coda designed specifically working MCMC samples Bayesian models kinds, functions creating plots. two functions visualizing posterior estimates global intercept, \\(\\beta\\).y-axis sampled values posterior distribution \\(\\beta\\), x-axis 1,000 samples retained (e.g. 60,000 draws - 30,000 burnin, thinning). Notice traceplot() shows Markov Chain moving around test different values. lot variation, bulk samples centered relatively narrow range, \\(-4.02\\) \\(-3.96\\), suggesting good convergence.Also notice chain ‘leaps’ forays away central area parameter space values. another feature Markov Chain: randomly evaluate parts parameter space see might fit better current. fact plot always returns quickly place suggests rejection alternate values.densplot(), can see shape sampled posterior, indicative probability density \\(\\beta\\). instance, clear probability mass median value around -4.0, probability mass lower, higher; words variation certainty true posterior spatial auto correlation value.Recall \\(\\beta\\) represents average log-risk VLBW Georgia. make numbers interpretable, exponentiate get \\(e^{-4.0} = 0.018\\). ‘average’ risk VLBW therefore 1.8%, counties vary around (e.g. ) value.preceding illustrations examine data plot specific parameters extended well beyond \\(\\beta\\) intercept alone! number parameters evaluated, remember \\(n=159\\) different values \\(\\psi_i\\).","code":"\nnames(bym)          # what is inside the model output object?##  [1] \"summary.results\"     \"samples\"             \"fitted.values\"      \n##  [4] \"residuals\"           \"modelfit\"            \"accept\"             \n##  [7] \"localised.structure\" \"formula\"             \"model\"              \n## [10] \"X\"\nnames(bym$samples)  # what is inside the 'samples' sub-object?## [1] \"beta\"   \"psi\"    \"tau2\"   \"sigma2\" \"fitted\" \"Y\"\ndim(bym$samples$beta) # 1000 draws for 1 beta fixed effect (the intercept)## [1] 1000    1\ndim(bym$samples$psi)  # 1000 draws for the psi = ui + vi for each of 159 counties## [1] 1000  159\ncoda::traceplot(bym$samples$beta)\ncoda::densplot(bym$samples$beta)"},{"path":"disease-mapping-iv.html","id":"extracting-summaries-for-mapping-or-analysis","chapter":"Week 7 Disease Mapping IV","heading":"7.3.6 Extracting summaries for mapping or analysis","text":"Finally, want extract summaries data purposes analyzing visualizing. presence 1000 samples posterior every single parameter, makes working data cumbersome. Luckily extractor functions can help.fitted values separate matrix within samples, contain model-predicted value \\(\\hat{Y_i}\\) county. can useful calculating model-smoothed rate risk.random effects, \\(\\psi_i\\) interpreted log-relative risk county. words quantify degree county varies overall average (specifically global intercept, case captured beta matrix within bym$samples).might wish summarize posterior county’s log-relative risk taking median samples \\(\\psi_i\\). function summarise.samples() achieves , lets us specify set quantiles.get median, also get 95% CI use quantiles = c(0.5, 0.025, 0.975). summarise.samples() function returns list two-named elements. want one named quantiles now. exponentiate , relative risks.","code":"\ny_fitted <- fitted(bym)\nvlbw$rate_bym <- y_fitted / vlbw$TOT\npsi <- summarise.samples(bym$samples$psi, quantiles = 0.5)\nnames(psi)  ## [1] \"quantiles\"   \"exceedences\"\nvlbw$RR_bym <- exp(psi$quantiles)[,1]"},{"path":"disease-mapping-iv.html","id":"plot-raw-versus-smoothed","chapter":"Week 7 Disease Mapping IV","heading":"7.3.7 Plot raw versus smoothed","text":"might interested see different Bayesian values raw observed values. can use base-R plot .Just saw Empirical Bayes, Bayesian smoothed rates smoothed towards mean compared raw values.","code":"\nplot(vlbw$rate, vlbw$rate_bym)"},{"path":"disease-mapping-iv.html","id":"mapping-rates","chapter":"Week 7 Disease Mapping IV","heading":"7.3.8 Mapping rates","text":"added modeled parameters vlbw spatial object, ready map., map appears similar spatial Empirical Bayes procedure Disease Mapping II. makes sense Bayesian use definition spatial neighbors. value-added fully Bayesian modeling compared spatial Empirical Bayes smoothing include:CAR-prior Bayesian smoothing borrows strength neighbors (like spatial EB), also models local spatial auto correlation.Bayesian modeling readily accommodates covariates regression, unlike Empirical Bayes procedure last week.Sampling Bayesian posterior permits inference including use 95% credible intervals exceedance probabilities possible spatial EB.","code":"\ntm_shape(vlbw) + \n  tm_fill(c('rate', 'rate_bym'),\n          style = 'quantile',\n          palette = 'BuPu',\n          title = c('Observed rate', 'CAR smoothed')) +\n  tm_borders() + \n  tm_layout(legend.position = c('RIGHT', 'TOP'))"},{"path":"disease-mapping-iv.html","id":"mapping-exceedance-probability","chapter":"Week 7 Disease Mapping IV","heading":"7.3.9 Mapping exceedance probability","text":"mentioned many times challenging can visualize parameter estimate, also measure precision, variance, reliability. nature Bayesian inference lends well characterizing probability posterior consistent threshold. can directly interpret posterior samples probability distribution, need look many MCMC samples exceeded given threshold order make inference confident estimate.example random effect parameters, \\(\\psi_i\\) represent deviation \\(region_i\\) overall mean rate value estimated global intercept. words random effects centered 0 (county \\(\\psi_i\\) = 0 county rate = intercept value). exponentiated, say relative risk centered null value 1. mapped relative risk (like SMR), ’re interested counties different 1, expected value state.calculate probability county greater (less ) 0 log scale (1 RR scale), simply look proportion samples posterior exceed value. proportion exceedence probability. might summarize counties high probability greater threshold (e.g. 0).Similarly, can look counties low probability exceeding zero. means samples zero. suggestive sub-zero deviation. way exceedance probability quantifies exceedingly high exceedingly low counties.calculate exceedence probabilities, use function summarise.samples(), time take advantage argument exceedences. specify threshold. looking random effects, \\(\\psi_i\\), know meaningfully different value one equal zero (threshold different different parameters, e.g. \\(\\sigma^2\\)).specify exceedences = 0, , function calculates proportion samples posterior greater 0, returns proportion. exceedence > 0.975, means >97.5% posterior sample greater zero.new variable vlbw_95 three-level indicator reflecting whether counties significantly lower risk average, significantly higher risk average, whether posterior estimate county consistent state average. worth noting Bayesian’s typically talk significance way frequentist’s. inherently estimating posterior distributions, rather testing discrete null hypotheses. However, ease, used word significance describe posteriors 95% credible interval excluding zero value.several ways incorporate new information map, two simple versions. first, layer three shapes top one another; first county values, second borders counties lower average, third borders counties higher average.alternative approach using tm_symbols() plot colored symbols high low counties.","code":"\n# First calculate probability of exceeding 0 for each county phi\nx <- summarise.samples(bym$samples$psi, exceedences = 0)\n\n# Now create a 1, 0 variable in vlbw indicating P<0.025 or P>0.975\nvlbw$rr_95 <- ifelse(x$exceedences < 0.025, 'Low', \n                     ifelse(x$exceedences > 0.975, 'High', NA))[,1]\ntm_shape(vlbw) +\n  tm_fill('RR_bym',\n          style = 'fixed',\n          palette = 'PRGn',\n          breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Relative Risk') +\n  tm_borders() + \ntm_shape(subset(vlbw, rr_95 == 'Low')) + \n  tm_borders(col = 'purple', lwd = 2) +\ntm_shape(subset(vlbw, rr_95 == 'High')) + \n  tm_borders(col = 'green', lwd = 2) +\n  tm_add_legend(type = 'line', \n                labels = c('Low risk county', 'High risk county'), \n                col = c('purple','green')) +\n  tm_layout(legend.position = c('RIGHT','TOP'))\ntm_shape(vlbw) +\n  tm_fill('RR_bym',\n          style = 'fixed',\n          palette = 'PRGn',\n          breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Relative Risk') +\n  tm_borders() + \ntm_shape(vlbw) + \n  tm_symbols(shape = 'rr_95',\n             col = 'rr_95',\n             palette = 'Dark2',\n             size = .5,\n             shapeNA = NA,\n             showNA = FALSE,\n             legend.shape.show = FALSE,\n            title.col = 'Significance') +\n  tm_layout(legend.outside = TRUE)"},{"path":"reproducibility-and-projects-in-r.html","id":"reproducibility-and-projects-in-r","chapter":"Week 8 Reproducibility and Projects in R","heading":"Week 8 Reproducibility and Projects in R","text":"","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"additional-resources-7","chapter":"Week 8 Reproducibility and Projects in R","heading":"Additional Resources","text":"R Markdown CheatsheetComprehensive guide using R Markdown\nChapter within R Markdown guide specific Notebooks\nChapter within R Markdown guide specific NotebooksEpidemiologist R Handbook - Working projectsEpidemiologist R Handbook - R markdownWorking Projects RR Data Science - Workflow Projects","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"the-benefits-of-code-reproducibility","chapter":"Week 8 Reproducibility and Projects in R","heading":"8.1 The benefits of code reproducibility","text":"Reproducibility refers capacity process create fully independently replicable either future another person. Non-reproducibility scientific findings cited leading problem problem comes ad-hoc thus non-reproducible conduct data preparation analysis.Spatial epidemiology requires intensive data preparation, cleaning, management, often complex sequence analytic steps. words difficult another analyst future version repeat process exactly way unless perfect record done. reason, reproducibility analysis emphasized required course.analysis reproducible sure data stays paired code, (many possible) steps change manipulate data written scripts rather done ‘hand’ (e.g. Excel editor).","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"workflows-to-enhance-reproducibility","chapter":"Week 8 Reproducibility and Projects in R","heading":"8.2 Workflows to enhance reproducibility","text":"R RStudio often used data preparation, analysis, reporting, fundamental importance reproducibility (making analytic processes transparent, interpretable repeatable) built-many features. Appendix introduces several strategies important reproducibility broadly, also important work course.First, brief introduction projects RStudio, slightly -depth description specific file format, rmarkdown can used create Notebooks.","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"using-projects-in-r","chapter":"Week 8 Reproducibility and Projects in R","heading":"8.2.1 Using Projects in R","text":"project R organizes work much might use folders computer sort separate logical scheme. words, place put multiple documents files related one another.instance, might choose single project week class, perhaps separate project assignment. project directory (folder) store data, scripts code, outputs (e.g. saved maps saved objects) specific week assignment.advantage creating formal project RStudio (rather just regular folder, example), RStudio projects certain benefits coding workflow.open project, working directory (e.g. root directory file path R looks files import) automatically set inside project folder. means keep data inside project, never worry broken links incorrect file paths occur data moved.Projects remember environmental settings RStudio, may customize something specific project remembered time open project.ever work version control system Github, projects natural strategy contain repositoryYou avoid using setwd() R! function changes working directory may taught make easier. bad whatever pathname put inside setwd() amost never work another computer. means code fragile specific computer, probably computer specific point time.find relying setwd() strategy hard code file pathnames, please consider learning projects. help make code less fragile robust sharing reproducing.create new project:1.Look upper-right corner RStudio blue-ish R symbol likely say ‘Project.’ Click pull-menu select New Project\n2. see Project Wizard open three options:\n+ yet created folder computer project, choose New Directory\n+ already folder (e.g. perhaps named ‘Week1’), choose Existing Directory\n+ forking checking repository Github, GitLab system, choose Version Control\n3. Navigate location want new folder , else location existing folder already \n4. Name project click Create ProjectOnce project created, can navigate via finder folder. notice new file extension .Rproj. double-click file, project open, including whatever files settings already worked .Get habit opening R double-clicking xxx.Rproj icon project folder. makes sure working directory set helps maintain relative rather absolute file pathnames within project folder.","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"organizing-projects","chapter":"Week 8 Reproducibility and Projects in R","heading":"8.3 Organizing projects","text":"projects analyses simple perhaps involve single script document use built-data. projects complex , involving dataset(s), one files code scripts, possibly output including datasets well images saved figures, markdown files reports. good practice standard strategy organizing .","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"make-scripts-that-do-discrete-tasks","chapter":"Week 8 Reproducibility and Projects in R","heading":"8.3.1 Make scripts that do discrete tasks","text":"may used one file hundreds even thousands lines code every part analysis. isn’t inherently wrong, can make difficult find particular snippets code defined recoded variable, carried descriptive analyses. larger projects, consider creating separate scripts discrete steps. many different R scripts given project, consider storing sub-folder perhaps labeled code/. might break work separate scripts like :script data preparation. allows quickly return process retrieving preparing data make changes.Scripts descriptive analysis. may want revisit descriptives future separate makes easier.Scripts (one ) complex analyses including modeling, figure preparation, simulation.script informative name project-x-data-prep.R project-x-create-final-maps.R.","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"always-store-data-with-code-and-output","chapter":"Week 8 Reproducibility and Projects in R","heading":"8.3.2 Always store data with code and output","text":"creating maps, raw (possibly post-processed, intermediate) data supports maps stored inside project folder. way guarantee can return year recreate map exactly. multiple data files, might consider putting content sub-folder, possibly labeled data/.","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"maintain-all-output-files-figures-cleaned-datasets-etc","chapter":"Week 8 Reproducibility and Projects in R","heading":"8.3.3 Maintain all output files (figures, cleaned datasets, etc)","text":"Just want store code data together, also plan store output content main project folder possibly one sub-folders (e.g. images/ reports/). several kinds outputs might generated including:Images figuresMapsCleaned prepared datasets (either stored .xlsx .csv possibly stored R binary format .rds)Reports (e.g. rendered R-markdown either html pdf)","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"use-the-here-package-to-maintain-robust-relative-pathnames","chapter":"Week 8 Reproducibility and Projects in R","heading":"8.4 Use the here package to maintain robust relative pathnames","text":"many reasons keep work organized, one maintain known constant relationship data code stored. discussed , use setwd() creates rigid absolute pointer file (e.g. data might C:\\MyDocuments\\EPI563\\Week1) stored. changed computers changed file structure current computer, absolute path likely fail making code non-reproducible (code find data)!Instead, please try preference relative pathnames. way describing something relative given starting point. case projects R-studio, starting point always folder containing project. Thus, location dataset stored sub-folder called data : data/mydataset.xlsx; assumed folder data sub-folder parent project folder. long keep project self-contained folder (e.g. copy/paste folder share folder contents), relative location robust.package developed try make bit easier. package named also function named () (know feels bit repetitive!). function, () serves describe hierarchical nesting folders locates file location desire (e.g. import dataset save figure ). examples use ():Importing data: mydata <- read.csv(('data', 'wave1', 'wave1_data.csv)). code, create new object (named mydata) results using function read.csv(). data located within project folder relative path location: data/wave1/wave1_data.csv.Saving output: ggsave(('figures', 'figure1.png')). code, save ggplot() figure computer location within overall project folder: figures/figure1.png.Caution: work Windows OS environment, careful designate file pathnames. R uses notation similar Unix OS, also one adopted Mac OS, define set nested folders forward slash : H:/mkram02/gis-file . Unfortunately opposite Windows describes pathnames (e.g. Windows use back slash like : H:\\mkram02\\gis-file). Using packages avoids confusion.","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"specify-a-relative-location-outside-the-working-directory","chapter":"Week 8 Reproducibility and Projects in R","heading":"8.4.1 Specify a relative location outside the working directory","text":"one folder entire course, inside separate project directory week. working project Week2, might wish load file saved previously Week1. words sub-folder, actually outside current directory. use setwd() function change location, creates possibly fragile absolute pathname can dangerous. Instead create robust relative pathname referring file relation current location.Using two dots pathname tells R go level directory. georgia.csv file referred Week1 directory, currently woring Week2 :means “go level, look data folder, death-data folder, load georgia.csv file.” need go two () levels, simply repeat: ../../data/death-data.georgia.csv","code":"\ndd <- read.csv('../data/death-data/georgia.csv')"},{"path":"introduction-to-r-notebooks.html","id":"introduction-to-r-notebooks","chapter":"Week 9 Introduction to R Notebooks","heading":"Week 9 Introduction to R Notebooks","text":"","code":""},{"path":"introduction-to-r-notebooks.html","id":"why-r-notebooks","chapter":"Week 9 Introduction to R Notebooks","heading":"9.1 Why R Notebooks?","text":"assignments course, least portion deliverable fully-functional, annotated R Notebook. notebooks actually specific case rmarkdown format creating reproducible documents interspersed R code, analytic results text. example eBook, many resources course created using rmarkdown related packages bookdown.said, R Notebooks specific instance case markdown incorporated R Studio nice features applied data analyst.Notebooks allow type text explains happening, interpret findings, note areas need exploration. similar usual commenting might familiar , makes easy narrative expansive comments.Notebooks contain functional R code interspersed narrative comments, code, comments output results can seen one continuous way.Notebooks work interactive mode. means coding working can see results document. save Notebook text, code results saved!reason using Notebooks provide means clear annotation documentation combined ready reproducibility. Reproducibility means someone else (future !) come back get result .benefit advantages , recommend gain familiarity basic (perhaps optional) formatting described . also recommend develop knack rich annotation documentation, just brief (often cryptic) comments used writing SAS code! Document plan . Document . Document results means. Document else needs done.R Notebooks handy serve like ‘lab notebooks’ documenting thinking go. great reports want share others (future self). still ok use regular R-scripts analyses require extensive documentation. example writing functions data-cleaning scripts may appropriate simple scripts extension my_code.R rather my_code.Rmd (e.g. notebook markdown).","code":""},{"path":"introduction-to-r-notebooks.html","id":"important-r-notebook-functions","chapter":"Week 9 Introduction to R Notebooks","heading":"9.2 Important R Notebook functions","text":"","code":""},{"path":"introduction-to-r-notebooks.html","id":"the-yaml","chapter":"Week 9 Introduction to R Notebooks","heading":"9.2.1 The YAML","text":"create new R Notebook R Markdown file within R Studio (e.g. via File > New File > R Notebook), ‘YAML’ automatically created top script delineated three dash lines ---. YAML stands “yet another markup language” set instructions finished notebook look structured. can accept default YAML structure (course modifying title) copy/paste YAML top script. can also read online additional customizations YAML, none necessary course.However, YAML can tricky sometimes. general tips:Keywords (e.g. title, date output) end colon comes ‘argument’ ‘setting’ keyword.‘argument’ ‘setting’ keyword takes multiple lines, can hit , case output:.\nHowever, note sub-arguments (e.g. html_notebook:) parent must indented 2 spaces.\nsub-arguments (e.g. number_sections: yes specific setting html_notebook:) must indented additional 2 spaces. indentations represent organization connect multiple settings correct parent keyword.\nHowever, note sub-arguments (e.g. html_notebook:) parent must indented 2 spaces.sub-arguments (e.g. number_sections: yes specific setting html_notebook:) must indented additional 2 spaces. indentations represent organization connect multiple settings correct parent keyword.","code":"---\ntitle: \"Title of your notebook\"\nauthor: \"Your Name Here\"\ndate: \"Submission date here\"\noutput:\n  html_notebook:\n    number_sections: yes\n    toc: yes\n    toc_float: yes\n---"},{"path":"introduction-to-r-notebooks.html","id":"typing-text","chapter":"Week 9 Introduction to R Notebooks","heading":"9.3 Typing text","text":"utility R Notebooks ability completely document thinking process carry analyses. necessary wordy just sake taking space, opportunity clearly delineate goals, steps, data sources, interpretations, etc.can just start typing text script serve purpose. text formatting functions summarized later document, Cheat sheets online resources linked elsewhere.","code":""},{"path":"introduction-to-r-notebooks.html","id":"adding-r-code","chapter":"Week 9 Introduction to R Notebooks","heading":"9.4 Adding R Code","text":"R Notebooks let write R code within Markdown file, run code, seeing results appear right code (rather Console, usually appear).2 ways add new chunk R code:Click green C-Insert button top editor panel R Studio. top option R code.Use keyboard short cut:\nMac Command + Shift + \nWindows Ctrl + Alt + \nMac Command + Shift + IWindows Ctrl + Alt + INotice R code chunks delineated three back-ticks (sort like apostrophes)…back-ticks typically key tilde (~) upper left keyboards. space sets 3 back-ticks R code goes called code chunk.see syntax color change things type inside R chunk (e.g. delineated ```), versus outside. Everything inside follows syntax rules R. Everything outside printed final report, run R code.want run code inside code chunks, can either:Place cursor line click Ctrl+enter (Windows) CMD+Return (Mac), can click Run button top editor pane R Studio.run code within chunk click green Run Current Chunk button upper-right code chunk.code corresponding results.way can iterate analytic process…switching running code, viewing output, documenting free text.want see current work--progress looks like HTML, can click Preview button top panel. save document, open Viewer panel.","code":"\nhead(mtcars)##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nplot(cars)"},{"path":"introduction-to-r-notebooks.html","id":"workflow","chapter":"Week 9 Introduction to R Notebooks","heading":"9.5 Workflow","text":"benefit Notebooks (slightly different regular Markdown) can work interactively code, seeing results immediately just regular script. contrast ‘regular’ Markdown file doesn’t run code click ‘Knit.’recommend workflow:Click File>New File>R Notebook create new file. Edit YAML (stuff top) correct title, author, etc. template created example code. Delete generic code YAML. Save file project folder.Use space YAML type objective purpose analysis, introduction background useful.Carry analysis, inserting code chunks, running , documenting free text go.wish, can see results look HTML clicking Preview button.Sometimes go back re-run code different order, else delete code without re-running entire script. means code reproducible objects created longer code support . final check reproducibility (assurance code self-contained dependent steps outside script) recommend always end clicking RUN button top panel. Specifically, choose Restart R Run Chunks. runs sure look results! step erases data objects memory starts running script top. error , something missing code. Try figure make changes code script everything expect.","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"formatting-markdown-and-notebooks","chapter":"Week 10 Formatting Markdown and Notebooks","heading":"Week 10 Formatting Markdown and Notebooks","text":"","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"optional-functions","chapter":"Week 10 Formatting Markdown and Notebooks","heading":"10.1 Optional functions","text":"list formatting functions long. include couple find useful (mandatory) :","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"customizing-your-yaml","chapter":"Week 10 Formatting Markdown and Notebooks","heading":"10.2 Customizing your YAML","text":"default YAML perfectly fine, YAML top script includes added functions including:Specify table contents - works use headersSpecify section numberingSpecify table contents ‘floating’ means html visible even scroll. PDF rendering, ‘float’ option.","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"simple-formatting-of-your-notebook","chapter":"Week 10 Formatting Markdown and Notebooks","heading":"10.3 Simple formatting of your Notebook","text":"generally helpful organize document using headers separate tasks steps code. can easily create headers using hashtag/pound sign #. Specifically…# beginning line denotes top-level (level-1) header large bold.## beginning line denotes level-2 header### unsurprisingly level-3 header!Make sure space # textAlways leave blank line (return/enter) header text ‘regular’ text.can also make numbered bulleted lists helpful. line begins either asterisk (*) number begin bulleted numbered list.Headers populated table contents, specified.","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"text-formatting","chapter":"Week 10 Formatting Markdown and Notebooks","heading":"10.4 Text formatting","text":"R Markdown Cheatsheets lots examples formatting. Three things use frequently bold, italics, numbered bulleted lists.Numbered lists start number, line must end 2 space (blank line ).Instead numbers can use lettersBulleted lists can initiated asterisk +, also must 2 spaces (blank carriage return) end item.","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"making-tables","chapter":"Week 10 Formatting Markdown and Notebooks","heading":"10.5 Making tables","text":"required, may want summarize data table R Markdown. packages devoted creating tables, can create quick--dirty table just using keyboard symbols.First start making header row. Separate column name ‘pipe’ symbol, |Put continuous line dashes (-----) column name, separating columns pipe symbol (|)Now type text corresponding row column. Separate columns pipe (|) separate rows carriage return/EnterSo following text typed directly Markdown file (e.g. inside code chunk):produce following output:limited useful additional customizations table. instance can alter width column changing relative number dashes pipes. can also specify whether contents columns left right justified, whether centered using colons (:) inside line dashes.","code":"Column 1  | Column 2 | Column 3\n----------|----------|-----------\nText 1    | Text 2   | Text 3\nNext line | Next line 2 | Next line 3Column 1 | Left justified | Centered | Right justified |\n---------|:-------------|:---------:|-----------:|\nRow 1  | 1,024,477 | Johnson & Johnson | Dekalb County |\nRow 2 | 4,321 | Frederick | Mercer  |"},{"path":"formatting-markdown-and-notebooks.html","id":"final-note","chapter":"Week 10 Formatting Markdown and Notebooks","heading":"10.6 Final Note","text":"Remember final step think done project, Click Restart R Run Chunks, save/preview Notebook sure expect.","code":""},{"path":"sf-overview.html","id":"sf-overview","chapter":"Week 11 Tips for working with sf data class","heading":"Week 11 Tips for working with sf data class","text":"Simple Features (sf) cheat sheet","code":""},{"path":"sf-overview.html","id":"st_set_geom","chapter":"Week 11 Tips for working with sf data class","heading":"11.1 st_set_geom()","text":"feature sf class data special column containing geometry information (often labeled geom) different variables. Specifically sticky. Stickiness variable means manipulate sf data object, geom column almost always sticks rest data even try remove .Imagine happen regular data.frame typed code console mvc[1, 1:2]. Typically kind numerical indexing cause R return row 1 columns 1 2. However, try R sf object happens:Notice get first row, first second column also got geom column even though didn’t request . stickiness generally desirable, important keep geographic/geometry data connected attribute data. However times want drop information. several ways , explicit way:literally erases sets NULL geometry column. retrieved without going back original data.","code":"\nlibrary(sf)\nlibrary(tidyverse)\n\nmvc <- st_read('../DATA/GA_MVC/ga_mvc.gpkg')## Reading layer `ga_mvc' from data source \n##   `C:\\Users\\mkram02\\OneDrive - Emory University\\EPI563-Spatial Epi\\SpatialEpi-2021\\DATA\\GA_MVC\\ga_mvc.gpkg' \n##   using driver `GPKG'\n## Simple feature collection with 159 features and 17 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -85.60516 ymin: 30.35785 xmax: -80.83973 ymax: 35.00066\n## Geodetic CRS:  WGS 84\nmvc[1, 1:2]## Simple feature collection with 1 feature and 2 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -82.55071 ymin: 31.46925 xmax: -82.04858 ymax: 31.96618\n## Geodetic CRS:  WGS 84\n##   GEOID                    NAME                           geom\n## 1 13001 Appling County, Georgia MULTIPOLYGON (((-82.55069 3...\nmvc2 <- st_set_geometry(mvc, NULL)\n# look at the class of the original and the modified object\nclass(mvc)## [1] \"sf\"         \"data.frame\"\nclass(mvc2)## [1] \"data.frame\"\n# look at the first row and 1-2nd column after NULLing geom\nmvc2[1, 1:2]##   GEOID                    NAME\n## 1 13001 Appling County, Georgia"},{"path":"sf-overview.html","id":"st-as-sf","chapter":"Week 11 Tips for working with sf data class","heading":"11.2 st_as_sf()","text":"also times , inextricably, data set seems like sf object gets rejected function geometry information sf. Sometimes data manipulation steps strip away sf data class even though geom column still exists. happens can reinstate class status calling st_as_sf(). Essentially formal way declaring object sf explicitly defining spatial component.","code":""},{"path":"sf-overview.html","id":"st_crs","chapter":"Week 11 Tips for working with sf data class","heading":"11.3 st_crs()","text":"Spatial coordinate reference systems (CRS) projections critically important managing visualizing spatial data. spatial information sf object determined values coordinates contained geom geometry column, values assume known defined coordinate system. instance unprojected data typically measured degrees latitude longitude, even units can vary depending geodetic system datum used.know ’re working ? function st_crs() return whatever information stored object CRS/projection.recent version sf, returned st_crs() two pieces information:first piece labeled User input: case reads WGS 84, suggesting object based datum CRS.second piece information labeled wkt: stands Well-Known Text. standardized structured format describing annotating coordinate/projection information. detail probably want structure WKT CRS . short includes features:base datum underlying ellipsoid, case WGS 84Specific parameters including prime meridian, coordinate systemThe ID, often represented EPSG code.fact object mvc EPSG code 4326 suggests simple, unprojected, WGS-84 CRS (e.g. see ).Occasionally WKT complex, perhaps previous transformations stored metadata encoded WKT. case, closer examination WKT may needed identify CRS/projection. instance TARGETCRS mentioned? may current CRS.","code":"\nst_crs(mvc)## Coordinate Reference System:\n##   User input: WGS 84 \n##   wkt:\n## GEOGCRS[\"WGS 84\",\n##     DATUM[\"World Geodetic System 1984\",\n##         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n##             LENGTHUNIT[\"metre\",1]]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"geodetic latitude (Lat)\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"geodetic longitude (Lon)\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     USAGE[\n##         SCOPE[\"Horizontal component of 3D system.\"],\n##         AREA[\"World.\"],\n##         BBOX[-90,-180,90,180]],\n##     ID[\"EPSG\",4326]]"},{"path":"dplyr.html","id":"dplyr","chapter":"Week 12 Tips for using dplyr","heading":"Week 12 Tips for using dplyr","text":"handy resources detail wrangling dplyr tidyr:dplyr overviewData transformation dplyr cheat sheetEpidemiologists R Handbook - tidy dataAs case many software packages, always one way get something done R! Base-R tools can accomplish kinds tasks, sometimes cumbersome inefficient.use R epidemiologists focused tools data science, might find diverse continually evolving tidyverse great toolbox explore. Originated Hadley Wickham (founder RStudio), packages constituting tidyverse now contributed lots different people. common interest handling data tidy ways. R Data Science authoritative guide tidy data, many tools constituting tidyverse including ggplot2, dplyr .appendix brief introduction dplyr package set data manipulation functions. words epidemiologists’ go-package data manipulation, recoding, preparation R.two high-level observations use dplyr semester:dplyr functions can thought verbs. means one tool act data, producing change. question “want change?”Functions dplyr (many parts tidyverse matter) can stand alone code. alternatively can chained together sequence. chaining (called piping tool connect chain steps called pipe looks like : %>%) can make code easier humans read, also helps run sequence steps efficiently.examples , use Georgia motor vehicle crash mortality dataset unit observation (e.g. content one row data) Georgia county, columns variable names. dataset also explicitly spatial meaning includes geography information regarding boundaries county, contained geom column, typical sf class data R.first rows dataset looks like (minus geom column):","code":"## Reading layer `ga_mvc' from data source \n##   `C:\\Users\\mkram02\\OneDrive - Emory University\\EPI563-Spatial Epi\\SpatialEpi-2021\\DATA\\GA_MVC\\ga_mvc.gpkg' \n##   using driver `GPKG'\n## Simple feature collection with 159 features and 17 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -85.60516 ymin: 30.35785 xmax: -80.83973 ymax: 35.00066\n## Geodetic CRS:  WGS 84##   GEOID                     NAME   variable estimate          County\n## 1 13001  Appling County, Georgia B00001_001     1504  Appling County\n## 2 13003 Atkinson County, Georgia B00001_001      875 Atkinson County\n## 3 13005    Bacon County, Georgia B00001_001      945    Bacon County\n## 4 13007    Baker County, Georgia B00001_001      390    Baker County\n## 5 13009  Baldwin County, Georgia B00001_001     2943  Baldwin County\n## 6 13011    Banks County, Georgia B00001_001     1767    Banks County\n##   MVCDEATHS_05 MVCDEATHS_14 MVCDEATH_17 TPOP_05 TPOP_14 TPOP_17\n## 1            4            4          10   17769   18540   18521\n## 2            5            1           3    8096    8223    8342\n## 3            7            5           0   10552   11281   11319\n## 4            1            1           1    3967    3255    3200\n## 5            6            8          13   46304   45909   44906\n## 6            4            8           6   16683   18295   18634\n##   NCHS_RURAL_CODE_2013    nchs_code     rural MVCRATE_05 MVCRATE_14 MVCRATE_17\n## 1                    6     Non-core     Rural   22.51111   21.57497   53.99276\n## 2                    6     Non-core     Rural   61.75889   12.16101   35.96260\n## 3                    6     Non-core     Rural   66.33813   44.32231    0.00000\n## 4                    4  Small metro non-Rural   25.20797   30.72197   31.25000\n## 5                    5 Micropolitan non-Rural   12.95784   17.42578   28.94936\n## 6                    6     Non-core     Rural   23.97650   43.72779   32.19921"},{"path":"dplyr.html","id":"select","chapter":"Week 12 Tips for using dplyr","heading":"12.1 select()","text":"first verb dplyr called select() useful want remove select specific columns/variables. instance, mentioned dataset 17 attribute columns plus geom column. perhaps need three variables, decided easier exclude unneeded variables? can select() want (inversely can select don’t want).three useful tips using select() spatial data:select variables keep simply list (e.g. select(data, var1, var2, var3))easier omit specific variables (e.g. perhaps 100 variables want drop 3), place negative sign name (e.g. select(data, -var5, -var6)).Finally, something specific working sf spatial data geometry column (typically named geom geometry) sticky. means ’s hard get rid . ’s actually good thing. usually want geometry stick attribute data. occasionally might want convert spatial sf data object aspatial data.frame. must first set geometry null like : aspatial.df <- st_set_geometry(spatial.df, NULL). See additional info .Let’s motor vehicle crash data.","code":"\n# First we read in the dataset, which is stored as a geopackage\nmvc <- st_read('GA_MVC/ga_mvc.gpkg')\n\n# Look at column names\nnames(mvc)\n\n# For this example we do not want the geom column because it is too big to view\nmvc2 <- st_set_geometry(mvc, NULL)\n\n# Creating a new object with only 4 attributes\nmvc2 <- select(mvc2, GEOID, NAME, rural, MVCRATE_05, MVCRATE_17)\n\n# look at column names\nnames(mvc2)##  [1] \"GEOID\"                \"NAME\"                 \"variable\"            \n##  [4] \"estimate\"             \"County\"               \"MVCDEATHS_05\"        \n##  [7] \"MVCDEATHS_14\"         \"MVCDEATH_17\"          \"TPOP_05\"             \n## [10] \"TPOP_14\"              \"TPOP_17\"              \"NCHS_RURAL_CODE_2013\"\n## [13] \"nchs_code\"            \"rural\"                \"MVCRATE_05\"          \n## [16] \"MVCRATE_14\"           \"MVCRATE_17\"           \"geom\"## [1] \"GEOID\"      \"NAME\"       \"rural\"      \"MVCRATE_05\" \"MVCRATE_17\""},{"path":"dplyr.html","id":"mutate","chapter":"Week 12 Tips for using dplyr","heading":"12.2 mutate()","text":"Another frequently needed verb called mutate() might guess changes data. Specifically mutate() function creating new variable, possibly recode older variable. mvc data object 159 rows (one n=159 counties).Let’s imagine wanted create map illustrated magnitude change rate death motor vehicle crashes 2005 2017. want create two new variables name delta_mr_abs (absolute difference rates) delta_mr_rel (relative diference rates).look help documentation mutate() ’ll see first argument input dataset, case mvc. anywhere one zillion different ‘recode’ steps can included inside parentheses, separated comma. , created two new variables, one representing absolute representing relative difference rates two years.can look first rows selected columns see new variables:","code":"\n# Now we make a new object called mvc2\nmvc3 <- mutate(mvc2, \n               delta_mr_abs = MVCRATE_05 - MVCRATE_17,\n               delta_mr_rel = MVCRATE_05 / MVCRATE_17)\nhead(mvc3)##   GEOID                     NAME     rural MVCRATE_05 MVCRATE_17 delta_mr_abs\n## 1 13001  Appling County, Georgia     Rural   22.51111   53.99276   -31.481650\n## 2 13003 Atkinson County, Georgia     Rural   61.75889   35.96260    25.796294\n## 3 13005    Bacon County, Georgia     Rural   66.33813    0.00000    66.338135\n## 4 13007    Baker County, Georgia non-Rural   25.20797   31.25000    -6.042034\n## 5 13009  Baldwin County, Georgia non-Rural   12.95784   28.94936   -15.991517\n## 6 13011    Banks County, Georgia     Rural   23.97650   32.19921    -8.222703\n##   delta_mr_rel\n## 1    0.4169284\n## 2    1.7173090\n## 3          Inf\n## 4    0.8066549\n## 5    0.4476038\n## 6    0.7446303"},{"path":"dplyr.html","id":"filter","chapter":"Week 12 Tips for using dplyr","heading":"12.3 filter()","text":"select() choosing columns keep drop, filter() choosing rows keep drop. familiar SAS, filter() statement might .Imagine wanted map urban counties, omit rural counties. defining filtering rule. rule logical statement (e.g. relationship can tested data return TRUE FALSE).create new dataset, mvc4 created mvc3 restricted non-Rural counties:can see original object (mvc3) 159 rows, filtered object (mvc4) 102, reflecting number non-Rural counties Georgia.Although example used one filtering rule (e.g. keep rows rural == 'non-Rural'), construct complex filter including several different logical tests within filter() function, separated comma. instance filter non-rural counties population 100,000 specified region state, assuming variables indicating values.","code":"\nmvc4 <- filter(mvc3, rural == 'non-Rural')\n\n\ndim(mvc3) # dimensions (rows, columns) of the mvc3 object## [1] 159   7\ndim(mvc4) # dimensions (rows, columns) of the restricted mvc4 object## [1] 102   7"},{"path":"dplyr.html","id":"arrange","chapter":"Week 12 Tips for using dplyr","heading":"12.4 arrange()","text":"Occasionally might want sort dataset, perhaps find lowest highest values variable, group like values together. Sorting dplyr uses arrange() verb. default, data arranged ascending order (either numerical alphabetical character variables), can also choose descending order :","code":"\nmvc5 <- arrange(mvc3, desc(MVCRATE_17))\n\nhead(mvc5)##   GEOID                     NAME     rural MVCRATE_05 MVCRATE_17 delta_mr_abs\n## 1 13307  Webster County, Georgia     Rural   38.81988  115.16315    -76.34327\n## 2 13269   Taylor County, Georgia     Rural   22.57336   73.69197    -51.11860\n## 3 13165  Jenkins County, Georgia     Rural   47.00353   57.03205    -10.02853\n## 4 13001  Appling County, Georgia     Rural   22.51111   53.99276    -31.48165\n## 5 13087  Decatur County, Georgia non-Rural   18.17455   52.40305    -34.22851\n## 6 13191 McIntosh County, Georgia non-Rural   16.11863   49.62427    -33.50564\n##   delta_mr_rel\n## 1    0.3370859\n## 2    0.3063205\n## 3    0.8241598\n## 4    0.4169284\n## 5    0.3468223\n## 6    0.3248135"},{"path":"dplyr.html","id":"pipe-operator","chapter":"Week 12 Tips for using dplyr","heading":"12.5 %>% Pipe operator","text":"Everything ’ve done now one step time, created five different datasets avoid overwriting original. one source coding efficiency R comes careful chaining piping together multiple steps.every verb required input dataset first argument, chain steps, functions take output previous step input current step. example code chunk everything one step:practice, takes experience write whole chain steps want. often go iteratively, adding one step time checking step expected.","code":"\nmvc6 <- mvc %>%\n  st_set_geometry(NULL) %>%                             # remove geom column\n  select(GEOID, NAME, rural, MVCRATE_05, MVCRATE_17) %>%# select target variables\n  mutate(delta_mr_abs = MVCRATE_05 - MVCRATE_17,        # recode variables\n        delta_mr_rel = MVCRATE_05 / MVCRATE_17) %>%\n  filter(rural == 'non-Rural') %>%                      # filter (restrict) rows\n  arrange(desc(MVCRATE_17))                             # sort by MVCRATE_17\n\ndim(mvc6)## [1] 102   7\nhead(mvc6)##   GEOID                      NAME     rural MVCRATE_05 MVCRATE_17 delta_mr_abs\n## 1 13087   Decatur County, Georgia non-Rural   18.17455   52.40305   -34.228506\n## 2 13191  McIntosh County, Georgia non-Rural   16.11863   49.62427   -33.505640\n## 3 13033     Burke County, Georgia non-Rural   34.87510   39.96093    -5.085824\n## 4 13189  McDuffie County, Georgia non-Rural   18.67501   37.21276   -18.537756\n## 5 13055 Chattooga County, Georgia non-Rural   11.76194   36.33428   -24.572337\n## 6 13227   Pickens County, Georgia non-Rural   29.32874   34.82335    -5.494612\n##   delta_mr_rel\n## 1    0.3468223\n## 2    0.3248135\n## 3    0.8727301\n## 4    0.5018442\n## 5    0.3237147\n## 6    0.8422147"},{"path":"dplyr.html","id":"group_by-and-summarise","chapter":"Week 12 Tips for using dplyr","heading":"12.6 group_by() and summarise()","text":"dplyr verb can incredibly important spatial epidemiology combination group_by() summarise(). two used aggregate summarize data. instance data arranged individual persons unit analysis (e.g. 1 person = 1 row data), wanted aggregate got counts per census tract, use group_by() arrange rows groups defined census tract, use summarise() calculation (e.g. count, mean, sum, etc) separately group.important feature sf data objects operated dplyr verbs, built functionality handle geography/geometry data. instance, imagine wanted create map aggregated rural counties separately non-rural counties.can see (might predicted), aggregation changed dataset 159 rows 2 rows: one row rural one non-rural. Let’s see spatial data first mapping original data, mapping aggregated data. Read qtm()) tmap functions.dplyr verbs (e.g. mutate(), select(), filter()), constrained using group_by() single variable. Returning example individual observations nested within census tracts, use new_data <- individ_data %>% group_by(gender, year, tract) create file row data unique stratum gender * year * census tract.may see message R console run group_by() followed summarise() says something like summarise() ungrouping output override .groups argument. telling R automatically ungrouped data. specifically removed grouping last group_by() variable, avoid unintended consequences persistent grouping. group_by() two variables drops last grouping*, grouping may persist. grouping can removed adding ungroup() summarise().","code":"\nmvc7 <- mvc %>%\n  group_by(rural) %>%\n  summarise(avg_mr_17 = mean(MVCRATE_17))\n\nmvc7 %>% st_set_geometry(NULL)## # A tibble: 2 x 2\n##   rural     avg_mr_17\n## * <chr>         <dbl>\n## 1 non-Rural      18.8\n## 2 Rural          29.2\n# Using the qtm() function from tmap to create map of original data\nm1 <- qtm(mvc, 'MVCRATE_17')\n\n# Using the qtm() function from tmap package to create a map\nm2 <- qtm(mvc7, 'avg_mr_17')\n\ntmap_arrange(m1, m2)"},{"path":"dplyr.html","id":"join","chapter":"Week 12 Tips for using dplyr","heading":"12.7 join()","text":"Merging data common epidemiology, also prone many unintended consequences important pay attention options successful merges ‘table joins’ called parlance relational databases.challenges getting desired output merge join(), dplyr set verbs including: left_join(), right_join(), inner_join(), full_join() . documentation many ways join!join simply way merge two tables common key ID variable. purposes class want focus several key features joining important spatial analysis.explanation work two separate files: aspatial data.frame motor vehicle crash data called mvc.df; sf object U.S. counties, called us. unit analysis county, common ID key variable county FIPS code (named GEOID), different number rows:expected, mvc.df \\(n=159\\) counties makeup Georgia. However spatial/geography information \\(n=3220\\) rows, corresponding number U.S. counties territories.First, difference xxxx_join() two tables relate one another. purposes left_join() right_join() trick. difference: left_join() starts first object joins second. contrast right_join() starts second object joins first. mean?learn?number rows output dataset dictated two things:order objects written (e.g. case mvc.df always first us always second, contained within join())direction join. left_join() merge begins mvc.df, limits output 159 rows. contrast right_join() merge begins us, limits output 3220 rows.class output data depends object first. Notice left_join(), started aspatial data.frame, output also aspatial data.frame (although geom column now incorporated!). contrast right_join() put sf object us first, class sf.means merging joining think whether want rows data go output, ; think whether () can make sf object first.scenario , want \\(n=159\\) rows, thus want exclude non-Georgia counties. means must mvc.df first. Therefore, force object class sf like (also see info ):Joining Key/ID variables different namesSometimes common variable, county FIPS code, variable names different. example code , column storing unique county ID mvc.df named GEOID. However column sf object us stores unique county ID named FIPS. still possible use join() verbs relating (inside c() concatenation) order datasets introduced.","code":"\ndim(mvc.df)  # this is dimensions for the aspatial attribute data## [1] 159  17\ndim(us)      # this is dimensions for the spatial county polygon sf data## [1] 3220   10\n# left join, starting with mvc.df as the first object, us as the second\ntest.left <- mvc.df %>%\n  left_join(us, by = 'GEOID')\n\ndim(test.left)## [1] 159  26\nclass(test.left)## [1] \"data.frame\"\n# right join, starting with mvc.df as the first object\ntest.right <- us %>%\n  left_join(mvc.df, by = 'GEOID')\n\ndim(test.right)## [1] 3220   26\nclass(test.right)## [1] \"sf\"         \"data.frame\"\ntest.left <- mvc.df %>%\n  left_join(us, by = 'GEOID') %>%\n  st_as_sf()\n\ndim(test.left)## [1] 159  26\nclass(test.left)## [1] \"sf\"         \"data.frame\"\n# if us had the FIPS code stored in the column named 'FIPS'\ntest.left <- mvc.df %>%\n  left_join(us, by = c('GEOID' = 'FIPS'))"},{"path":"dplyr.html","id":"pivot_","chapter":"Week 12 Tips for using dplyr","heading":"12.8 Reshaping (transposing) data","text":"numerous intermediate advanced data manipulation options available dplyr tidyverse, outside scope course. One final verb represents sophisticated kind data change, however useful preparing spatial data. tools transpose reshape rectangular dataset wide long vice versa. Transposing useful , example, column disease rate several years (data wide), want dataset single column contains rate separate column indicates year (data long). article introduces notion pivoting data; can also review section R Data ScienceTwo related verbs help pivot tidy data one direction :","code":""},{"path":"dplyr.html","id":"pivot_longer-for-going-from-wide-to-long","chapter":"Week 12 Tips for using dplyr","heading":"12.8.1 pivot_longer() for going from wide to long","text":"Reviewing article linked previous paragraph (searching help documentation) give detail. example look take current mvc dataset, contains motor vehicle crash mortality rate county three different years (2005, 2014, 2017) separate columns (e.g. wide):mapping time-series often beneficial data long, say want data single column mvc_rate separate column year, can choose create map subset (defined year) data.First let’s look results, ’ll walk steps code chunk :can see, now 3 rows Appling County (GEOID 13001): one three years, different MVCRATE . long dataset. code work? step--step code chunk :first step create new object, mvc_long outcome steps piped together. input pipe original dataset, mvc.use as_tibble() current work around annoying ‘feature’ pivot_* functions don’t play well sf data classes. use as_tibble() essentially removing class designation, (making tibble tidy object); importantly different st_set_geometry(NULL) actually omits geometry column (e.g. see additional detail ).used select() pull variables interest, although leave variables desired.pivot_longer() can called several ways. way call , first specified columns pivot defining cols = argument variables start phrase MVCRATE. starts_with() another utility function dplyr. step told R columns wanted changed three called MVCRATE_05, MVCRATE_12 MVCRATE_17The names_to = argument defines column name new dataset delineate three variables (e.g. MVCRATE_05, etc). case wanted value year word MVCRATE_12. accomplish extra work:First, note used option names_sep = '_'. another utility function says want break string parts wherever designated separated (e.g. underscore, _) occurs. take column name MVCRATE_05 break underscore return two parts: MVCRATE 05.breaking two produce two answers, make two variable names names_to = hold . Thus names_to = c(\".value\", \"year\"). words column labeled .variable hold value MVCRATE column year hold value 05'.value' actually special value instance. way designating first part essentially junk. automatically discarded.values_to = 'mvcrate'. define name new dataset hold actual value (e.g. MVC mortality rate .)mutate() step just way take year fragment (e.g. 05, 12, 17) make calendar years first making numeric, simply adding 2000.final step, st_as_sf() manipulations actually removed objects designation class sf. Importantly, remove geom column, object recognized (e.g. tmap) spatial object. st_as_sf() simply declares fact sf.best way wrap head around start trying reshape transpose data hand. may need look additional help examples online, time become intuitive.see might gone work, use tm_facets() (read abouttmap_facets() ).","code":"\n# this code shows the first 6 rows (the head) of the relevant variables\nmvc %>% \n  st_set_geometry(NULL) %>%\n  select(GEOID, NAME, MVCRATE_05, MVCRATE_14, MVCRATE_17) %>%\n  head()##   GEOID                     NAME MVCRATE_05 MVCRATE_14 MVCRATE_17\n## 1 13001  Appling County, Georgia   22.51111   21.57497   53.99276\n## 2 13003 Atkinson County, Georgia   61.75889   12.16101   35.96260\n## 3 13005    Bacon County, Georgia   66.33813   44.32231    0.00000\n## 4 13007    Baker County, Georgia   25.20797   30.72197   31.25000\n## 5 13009  Baldwin County, Georgia   12.95784   17.42578   28.94936\n## 6 13011    Banks County, Georgia   23.97650   43.72779   32.19921\nmvc_long <- mvc %>%\n  select(GEOID, NAME, MVCRATE_05, MVCRATE_14, MVCRATE_17) %>%\n  as_tibble() %>%\n  pivot_longer(cols = starts_with(\"MVCRATE\"),\n               names_to = c(\".value\", \"year\"),\n               values_to = \"mvc_rate\",\n               names_sep = \"_\") %>%\n  mutate(year = 2000 + as.numeric(year)) %>%\n  st_as_sf()\nmvc_long %>%\n  st_set_geometry(NULL) %>%\n  head()## # A tibble: 6 x 4\n##   GEOID NAME                      year MVCRATE\n##   <chr> <chr>                    <dbl>   <dbl>\n## 1 13001 Appling County, Georgia   2005    22.5\n## 2 13001 Appling County, Georgia   2014    21.6\n## 3 13001 Appling County, Georgia   2017    54.0\n## 4 13003 Atkinson County, Georgia  2005    61.8\n## 5 13003 Atkinson County, Georgia  2014    12.2\n## 6 13003 Atkinson County, Georgia  2017    36.0\ntm_shape(mvc_long) +\n  \n  tm_fill('MVCRATE') + \n  tm_borders() +\ntm_facets(by = 'year')"},{"path":"dplyr.html","id":"pivot_wider","chapter":"Week 12 Tips for using dplyr","heading":"12.8.2 pivot_wider()","text":"course also possible go way, long wide. often easier. code return original shape:Take look output:appears returned 1 row per county. steps?, start removing class designation sf calling as_tibble()mutate() called re-create variable become column names.longer need old year variable omit select(-year)Finally pivot_wider() call arguments defining current variable contains informatino new column name (names_from =) current variable contains information population cells within column (values_from =).","code":"\nmvc_wide <- mvc_long %>%\n  as_tibble() %>%\n  mutate(my_var = paste0('MVCRATE ', year)) %>%\n  select(-year) %>%\n  pivot_wider(names_from = my_var,\n              values_from = MVCRATE) %>%\n  st_as_sf()\nmvc_wide %>%\n  st_set_geometry(NULL) %>%\n  head()## # A tibble: 6 x 5\n##   GEOID NAME                     `MVCRATE 2005` `MVCRATE 2014` `MVCRATE 2017`\n##   <chr> <chr>                             <dbl>          <dbl>          <dbl>\n## 1 13001 Appling County, Georgia            22.5           21.6           54.0\n## 2 13003 Atkinson County, Georgia           61.8           12.2           36.0\n## 3 13005 Bacon County, Georgia              66.3           44.3            0  \n## 4 13007 Baker County, Georgia              25.2           30.7           31.2\n## 5 13009 Baldwin County, Georgia            13.0           17.4           28.9\n## 6 13011 Banks County, Georgia              24.0           43.7           32.2"},{"path":"intro-tmap.html","id":"intro-tmap","chapter":"Week 13 Tips for using tmap","heading":"Week 13 Tips for using tmap","text":"Base-R capable data visualization plotting capabilities, fall short anything simple maps spatial data. Many packages including sp ggplot2 also functionality specifically optimized data visualization needs spatial epidemiologist. brief introductions packages.semester workhorse mapping/cartography tool tmap (thematic mapping) package. package builds grammar graphics logic built ggplot2 data visualizations conceived series layers information (e.g. axes, plot space, data points, lines, fill, legends, titles, etc) systematically stacked one top another. tmap start spatial object (e.g. data object either sf sp class) build visualization similarly combining adding together sequential layers., use data motor vehicle crash mortality dataset Georgia counties (vector polygon spatial data file), along information highways (vector line data file) trauma centers (vector point data).First load package, tmap browse help index:seeing range functions within tmap, import three datasets stored geopackage format begin visualizing:","code":"\n# load the tmap and sf packages\nlibrary(tmap)\nlibrary(sf)\n\nhelp('tmap')\n# import (read) three spatial datasets stored in geopackage format\nmvc <- st_read('GA_MVC/ga_mvc.gpkg')\nhwy <- st_read('GA_MVC/ga_hwy.gpkg')\ntrauma <- st_read('GA_MVC/trauma_centers.gpkg')"},{"path":"intro-tmap.html","id":"tmap-mode","chapter":"Week 13 Tips for using tmap","heading":"13.1 tmap mode","text":"One nice feature tmap two modes plotting maps. may develop general preference one another, although opinion serve slightly different purposes.plot mode produces conventional static maps viewed plot pane R-Studio, can saved file. main maps dissemination papers, posters, many presentations.view mode interactive plot html browser-like window. mode allows user interact map including panning, zooming, clicking spatial objects view underlying data. great data exploration, extensions web-served maps. However useful non-web-based dissemination want control map.select mode function tmap_mode() either 'plot' 'view' parentheses. Note set mode, subsequent maps mode…must re-submit tmap_mode() call switch back . default, tmap_mode() 'plot', means produces static maps. plot static maps, switch ’view' mode compare.","code":""},{"path":"intro-tmap.html","id":"qtm","chapter":"Week 13 Tips for using tmap","heading":"13.2 Quick maps: qtm()","text":"function qtm() stands Quick Thematic Maps, provides step simple plot() functions quickly plotting spatial objects. fundamental argument submitting qtm() name object plotted.produces geometry information (note unlike plot(), plot map every variable!).produce choropleth map (e.g. one objects shaded represent underlying statistic value), simply add name variable.Can tell legend cut-points determined? ’ll talk matters change later.Now try switching tmap_mode():Try things view mode:default visible R Studio Viewer pane; icon screen arrow allows show new window…biggerZoom outPanHover counties (see hovering?)Click counties (see click?)Underneath zoom + / - icon like stack papers. changes background map (background information change zoom /?)Click icon looks like stack pages. lets change background map (assuming currently connected internet)change back (like) :","code":"\nqtm(mvc)\nqtm(mvc, 'MVCRATE_05')\ntmap_mode('view')\ntmap_mode('plot')"},{"path":"intro-tmap.html","id":"customizing-qtm-for-polygons","chapter":"Week 13 Tips for using tmap","heading":"13.2.1 Customizing qtm() for polygons","text":"polygon data, might like control several features including title, color palette, style continuous variables categorized legend.syntax customizes original plot several ways:changing fill.style (style continuous variables categorized order plot sequential ramp choropleth map) default (fixed equal intervals) quantile style (default quantiles \\(n=5\\) quintiles although schemes including tertiles quartiles possible also)choosing custom color palette, case Yellow-Green-Blue (YlGnBu) palette, one several built-options.Providing informative title legend, rather default variable name.Notice label legend code “/n” inserted middle line. use forward slash creates called escape character. case “/n” inside character string inserts line break. “(2017)” line “MVC Mortality.”","code":"\nqtm(mvc,\n    fill = 'MVCRATE_17', \n    fill.style = 'quantile', \n    fill.palette = 'YlGnBu',\n    fill.title = 'MVC Mortality \\n(2017)')"},{"path":"intro-tmap.html","id":"customizing-qtm-for-lines","chapter":"Week 13 Tips for using tmap","heading":"13.2.2 Customizing qtm() for lines","text":"qtm() (tmap generally) can also handle types spatial data including line shape objects, can provide customization results. Try highway dataset:basic plot highways uses default colors sizes, plot uses lines.lwd= argument specify line width thickness. lines.col= sets color.","code":"\nqtm(hwy, \n    lines.lwd = 2, \n    lines.col = 'red')"},{"path":"intro-tmap.html","id":"customizing-qtm-for-points","chapter":"Week 13 Tips for using tmap","heading":"13.2.3 Customizing qtm() for points","text":"surprisingly, similar control point spatial objects, case locations trauma centers.symbols.size symbols.shape specified, symbolized variables modifying size shape. also settings color. study help documentation, notice arguments require numbers (thus use LEVEL_number integer) allow character/factors (thus use LEVEL).","code":"\nqtm(trauma,\n    symbols.size = 'LEVEL_number', \n    symbols.shape = 'LEVEL')"},{"path":"intro-tmap.html","id":"finding-valid-color-options","chapter":"Week 13 Tips for using tmap","heading":"13.2.4 Finding valid color options","text":"base R many ways specify colors including using standardized character strings, well HEX codes complicated alphanumeric labels used across industries identify unique colors. one many lists base-R color names: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdfHowever mapping often want just single colors, reasonable sets colors symbolizing sequential values, categorical values, diverging values. coming weeks talk choose color style symbolizing maps. several sources finding effective color palettes spatial mapping. One best resources choosing color palette iw Color Brewer website.Another source color palettes actually built right tmap package (actually part add-package installed called tmaptools). tool actually directly derived Color Brewer site , helps make clear name palettes tmap.Occasionally next step caused session R crash. Therefore usually open second instance R Studio just next thing. simply go Session R Studio menu click New Session. creates another completely independent instance R Studio (e.g. none packages data loaded current session present new session unless specify ).use package name (tmaptools) followed double colon (::)? shortcut R lets call single function package without loading package. Basically says “go look package called tmaptools load specified function”. use shortcut (general, tmaptools) one two situations:function name two packages, specifying package identifies one mean. instance soon learn package dplyr function select() package also name function another package handling spatial data called raster. often use dplyr::select() disambiguate.situations like tmaptools::palette_explorer() really need one function currently need anything else package.may discover experimentation, tmaptools::palette_explorer() function actually small interactive app opens new window lets see array color palettes. can see divided sequential, divergent, categorical color ramps can move slider change many categories see color ranges. thing want explorer abbreviated names left color ramp.","code":"\ntmaptools::palette_explorer()"},{"path":"intro-tmap.html","id":"building-maps-with-tmap","chapter":"Week 13 Tips for using tmap","heading":"13.3 Building maps with tmap","text":"qtm() great quickly making map, want control map, want shift full functions tmap.","code":""},{"path":"intro-tmap.html","id":"building-blocks-in-tmap","chapter":"Week 13 Tips for using tmap","heading":"13.3.1 Building blocks in tmap","text":"tmap produces maps using grammar graphics approach means building final product ‘sum’ several fundamental components, plus possible options layers. three fundamental components maps tmap:Specify spatial object map using tm_shape().Following call tm_shape() generally specify layers wish symbolize map. words specifying shape doesn’t plot anything…just starting point. layers actual things object/shape plot. case polygons usually use tm_fill() specify layer fill polygon, although layers available (e.g. see base derived layers listed look help('tmap')).Finally, many instances want customize map layout features, title legend, might like add elements North arrow scale bar.given map, various layers steps connected together R code plus sign (+); highlights map sum many parts.NOTE: use pipe (%>%) plus (+) seemingly connect steps together ! perhaps unfortunate ggplot2 tmap use pipe dplyr. Beware choose correct connector function hand!pipe (%>%) links together separate functions. contrast plus (+) tmap ggplot2 add sub-parts instructions main function called.Note steps 1 2 can repeated many spatial objects wish layer. wanted put points lines top polygon shape, specify tm_shape() corresponding layers spatial object turn.code replicates first map qtm(), basically says, \"Start object mvc symbolize two layers: first fills polygons represent MVCRATE_17 second adds polygon borders:Look help documentation tm_fill() see myriad ways can customize map! ’s little overwhelming, ’d suggest looking style palette arguments, using -mentioned palette_explorer() try different colors different styles cut-points.Reverse order color paletteBy default, color palettes – sequential, divergent, categorical – arranged particular order. want colors go opposite direction? can specify putting hyphen ‘negative sign’ inside quotes name selected color palette. example:","code":"\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17') +\n  tm_borders()\nm1<- tm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          palette = 'BuPu',\n          style = 'quantile') +\n  tm_layout(main.title = \"BuPu color in default order\",\n            inner.margins = c(0.01, 0.01, 0.05, 0.2)) +\n  tm_borders()\nm2<- tm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          palette = '-BuPu',\n          style = 'quantile') +\n  tm_layout(main.title = \"BuPu color in reverse order\",\n            inner.margins = c(0.01, 0.01, 0.05, 0.2)) +\n  tm_borders()\ntmap_arrange(m1, m2)"},{"path":"intro-tmap.html","id":"customizing-text-on-maps","chapter":"Week 13 Tips for using tmap","heading":"13.3.2 Customizing text on maps","text":"several ways may wish customize text maps. example may want provide name legend, new labels categories, title, subtitle caption whole map.give title legend map use title = 'xxx' tm_fill() (layer function) call.change labels legendTo add source credits annotationThe tm_fill() option creates 5 bins categories plotting default. reason unnecessary put n = 5 specify many categories. However explicit number categories provide vector 5 labels correspond categories. course one choose non-default number categories (e.g. n = 3 n = 7), custom labels provided many labels categories.","code":"\n  # First, I create a vector of my custom legend labels\n  # (note, there must be same number of labels as there are categories in map)\nmyLabels <- c('Low (Q1)', 'Q2', 'Q3', 'Q4', 'Hi (Q5)')\n\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          style = 'quantile',\n          title = 'MVC Rate in 2017',\n          n = 5, \n          labels = myLabels) +\n  tm_borders() +\ntm_layout(title = 'Motor Vehicle Crashes per capita in Georgia',\n          legend.outside = T) +\ntm_credits('Source: Georgia OASIS, retrieved 2019')"},{"path":"intro-tmap.html","id":"adding-two-or-more-spatial-objects-in-one-map","chapter":"Week 13 Tips for using tmap","heading":"13.3.3 Adding two or more spatial objects in one map","text":"Just like ArcGIS, additional spatial layers can added produce informative map. instance interested highways trauma centers related motor vehicle mortality rates add layers.Several things note code:three separate spatial objects plotted, called starting tm_shape() followed additional function specific layer. See help documentation, Tenekes article Canvas table layers available kinds shapes (e.g. polygons, points, lines).step added feature (e.g. call parentheses) connected together plus signsWithin step (e.g. within parentheses), arguments separated commasI organize code vertically think makes readable one line.Try changing arguments try substituting different options!","code":"\ntm_shape(mvc) + \n  tm_fill('MVCRATE_17',\n          style = 'quantile',\n          palette = 'Purples') +\n  tm_borders() +\n\ntm_shape(hwy) + \n  tm_lines(lwd = 2, col = 'red') +\n  \ntm_shape(trauma) + \n  tm_bubbles(shape = 'LEVEL',\n             col = 'pink')"},{"path":"intro-tmap.html","id":"controlling-layout-and-map-elements","chapter":"Week 13 Tips for using tmap","heading":"13.4 Controlling layout and map elements","text":"audience map , making look ‘just right’ may critical. However, creating map share colleagues, stakeholders, public, cartographic design important effective visual communication.tmap wide range tools customize way single map (even set maps) looks. fact many can feel overwhelming first. best advice use help documentation often, experiment lot! repeat help documentation completely, provide guidance several common layout needs options.Note focus examples static maps presented tmap_mode('plot'). Many options behave using interactive tmap_mode('view'), interactive html plots dynamically resize, formatting may differ. See ?tm_view() information options specific interactive mode.","code":""},{"path":"intro-tmap.html","id":"understanding-the-graphic-space-in-tmap","chapter":"Week 13 Tips for using tmap","heading":"13.4.1 Understanding the graphic space in tmap","text":"see discussion , many tools adjust size position elements fit way want, accomplish desired graphic layout. one recurring source frustration understanding parameters move parts graphical space. apparent, see plot really set nested plot spaces border margin width control.see can ‘turn ’ global option called design.mode using function tmap_options(). colorize different parts plot space, messaging names space. can help figure whether need control inner.margins, outer.margins move things panel overall plot space.example uses two-map plot illustrate information returned:see text output (interprets colors), actually several different plot spaces.device (yellow) means full extent output device, whether screen, .png .pdfouter.margins (indicated green) shows far edges plot area edge graphic devicemaster shape (indicated red) actual plotted map. see instructions adjusted inner.margins; margins distance red area blue area. want map smaller inside frame, use inner.margins shrink size red area","code":"\n# Turn 'on' the design.mode option\ntmap_design_mode(T)## design.mode: ON\n# Plot a map of two rates, side by side (e.g. see small multiples below)\ntm_shape(mvc) + \n  tm_fill(c('MVCRATE_05', 'MVCRATE_17'),\n          palette = 'Purples',\n          style = 'quantile') +\ntm_borders()## ----------------aspect ratios------------------------## | specified (asp argument of tm_layout)          NA |## | device (yellow)                         1.4000000 |## | device without outer margins (green)    1.4000000 |## | facets region (brown)                   1.7278757 |## | frame (blue)                            0.8639379 |## | master shape, mvc, (red)                0.8639379 |## -----------------------------------------------------\n# turn off the design.mode unless you want to see it on the next map you plot\ntmap_design_mode(F)## design.mode: OFF"},{"path":"intro-tmap.html","id":"controlling-map-layout","chapter":"Week 13 Tips for using tmap","heading":"13.4.2 Controlling map layout","text":"function tm_layout() controls title, margins, aspect ratio, colors, frame, legend, among many things. Type ?tm_layout() review help documentation see long list arguments can modify. Arguments via tm_layout() function incorporated map ‘adding’ (e.g. using + sign) tmap object, just add tm_fill() tm_borders().Explaining code :Aspects layout specified different steps:tm_fill() permits specification title legend. Notice inclusion \\n within title. noted , called escape character particular one forces line break/carriage return, let wrap title legend onto two linestm_borders() familiar, use alpha argument specify transparency borders, resulting lighter color. alpha parameter ranging 0 (fully transparent, invisible) 1 (transparency). can use alpha many different settings; useful maps many units (e.g. map U.S. counties) diminish visual impact boundaries using transparency.tm_layout() function many purposes, two things: add overall title map, adjust spacing inside frame line things fit:inner.margins controls big mapped figure relation overall frame; using argument way squish things around legend elements fit without bumping .\nargument expects vector four number going bottom, left, top, right\nvalues vector four numbers can range 0 1, representing relative amount space map object frame. use inner.margins = c(0.02, 0.02, 0.1, 0.2) means little extra space bottom left (0.02 ), extra space top (0.1) even right (0.2). arrived values trial error necessary keep legend bumping map.\nargument expects vector four number going bottom, left, top, rightThe values vector four numbers can range 0 1, representing relative amount space map object frame. use inner.margins = c(0.02, 0.02, 0.1, 0.2) means little extra space bottom left (0.02 ), extra space top (0.1) even right (0.2). arrived values trial error necessary keep legend bumping map.Explaining code :code differed three ways:used option request histogram legend, using legend.hist = T tm_fill() functionI moved entire legend outside frame specifying legend.outside = T tm_layout() function. Note addition shifting legend outside, can also control location using legend.position (changing location inside frame) legend.outside.position (controlling position outside frame).","code":"\n# Using tm_fill and tm_layout to control layout and text\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          style = 'quantile',\n          palette = 'BuPu',\n          title = 'Deaths per 100,000, \\n2017') +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = 'Car crash mortality in Georgia',\n            inner.margins = c(0.02, 0.02, 0.1, 0.2))\n# Adding a histogram to the legend and moving the legend outside of the frame\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          style = 'quantile',\n          palette = 'BuPu',\n          title = 'Deaths per 100,000, \\n2017',\n          legend.hist = T) +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = 'Car crash mortality in Georgia',\n            legend.outside = T)"},{"path":"intro-tmap.html","id":"adding-map-elements","chapter":"Week 13 Tips for using tmap","heading":"13.4.3 Adding map elements","text":"Finally, may wonder add map elements like north arrows, scale bars, captions, etc.extremely busy map many elements, illustrating features:","code":"\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          style = 'quantile',\n          palette = 'BuPu',\n          title = 'Deaths per 100,000, \\n2017',\n          legend.hist = T) +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = 'Car crash mortality in Georgia',\n            legend.outside = T, \n            inner.margins = c(0.1, 0.02, 0.02, 0.1)) +\n  tm_compass(type = '4star', \n             size = 2,\n             position = c('right', 'top')) +\n  tm_scale_bar(position = c('left', 'bottom')) +\n  tm_credits('Source: Georgia OASIS') +\n  tm_grid(alpha = 0.2)"},{"path":"intro-tmap.html","id":"change-the-global-style-of-a-map","chapter":"Week 13 Tips for using tmap","heading":"13.4.4 Change the global style of a map","text":"tmap several pre-defined ‘styles’ ‘themes’ maps. may strategy chose epidemiologic maps, quick easy way achieve certain ‘feel’ map. style simply means set options preset (user can still modify individual elements) produce particular look. see examples global map produced using ten different styles, type tmap_style_catalog() console. computer took approximately 60-90 seconds produce ten separate .png files sub-folder project. can browse see styles differ. Two examples shown :\nFIGURE 13.1: tmap style: Natural\n\nFIGURE 4.1: tmap style: Classic\n","code":""},{"path":"intro-tmap.html","id":"making-small-multiple-maps","chapter":"Week 13 Tips for using tmap","heading":"13.5 Making small-multiple maps","text":"Small multiples refers production multiple maps presented set. often desire small multiples way visually compare two features easy put map.three ways prepare small multiples tmap. look , notice differ respect number legends produced, range legends, content flexibility customization within map panels.","code":""},{"path":"intro-tmap.html","id":"small-multiples-as-a-vector-of-variables","chapter":"Week 13 Tips for using tmap","heading":"13.5.1 Small multiples as a vector of variables","text":"plot side--side maps two variables spatial object, simply call vector variable names specifying layer symbolization.strategy produced single map variable listed vector, map unique legend, determined breaks data variable.","code":"\ntm_shape(mvc) + \n  tm_fill(c('MVCRATE_05', 'MVCRATE_17'),\n          palette = 'Purples',\n          style = 'quantile',\n          title = c('Mortality, 2005', 'Mortality, 2017')) +\n  tm_borders() +\n  tm_layout(inner.margins = c(0.02, 0.02, 0.1, 0.2),\n            legend.position = c('right', 'top'))"},{"path":"intro-tmap.html","id":"tmap-facet","chapter":"Week 13 Tips for using tmap","heading":"13.5.2 Small multiples with facets","text":"Facet plotting something common package ggplot2. refers production two plot figures stratified ‘grouping’ variable. Typically facet plots ggplot2, scale \\(x\\) \\(y\\) axis held constant across set plots values plotted readily comparable.tmap, facet plotting means creating multiple map plots distinguished slicing stratifying spatial units along group. Faceting can useful highlighting patterns among different sub-groups spatial data. Unlike ggplot2, scale legend bounds x, y coordinate extent enforced across panel maps default. Instead min/max x, y coordinates can vary according scope content panel (e.g. default, free.coords = T).default range cut-points legend held constant across maps (e.g. single legend produced represent data maps).like force consistency panels (e.g. either better contextualization comparability), can specified. Argument free.coords = FALSE (e.g. map min/max x, y coordinate range) free.scale=FALSE (e.g. map spatial scale ‘zoom’ appropriate contents panel).strange facet map produced stratifying NCHS urban/rural six-level categorization scheme. First code happens default, setting free.coords free.scales FALSE. can see default, map frame zooms maximize selected object, scale different . contrast forced maintain constant scale easier see relative size locations subset.","code":"\n# Basic facet map with defaults\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17') +\n  tm_borders() +\n  tm_facets(by = 'nchs_code')\n# With facet parameters set to FALSE\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17') +\n  tm_borders() +\n  tm_facets(by = 'nchs_code', free.coords = FALSE, free.scales = FALSE)"},{"path":"intro-tmap.html","id":"facets-for-time-series","chapter":"Week 13 Tips for using tmap","heading":"13.5.3 Facets for time-series","text":"small multiples vector--variables facets differ:One point, might obvious first, distinguishes first two methods small multiple map productions use data separate maps. Notice first option (supplying vector variables plot using c() call within tm_fill() example) good mapping things wide data. words maps separate columns different maps.contrast tm_facets() creates separate maps stratifying rows data. words good mapping things long data. used idea long versus wide data might seem confusing, relatively common distinction data handling.extension idea wanted map time-series (e.g. maps disease rates year series years), create long dataset year. Imagine dataset row data every county Year 1; separate dataset row data every county Year 2; . stacking datasets dataset becomes long number geographic units \\(\\times\\) number years. easily ArcGIS, perfectly allowable sf class spatial objects. plotting, simply use tm_facets() = YEAR produce series.example taking current ‘wide’ dataset (e.g. currently 3 years separate columns), making long dataset (e.g. single column MVCRATE, separate column year distinguish year-rate talking ). produce time-series faceted maps. case use tidy functionality pivot_* verbs (e.g. read use pivot verbs )Now, plot long sf object ignoring fact three rows data every county. Can tell happens?Notice maps ? Try changing YEAR == 2017 different year. can see ignored long format, tmap essentially plotted Georgia counties 3 times, last layer (e.g. 2017) top thus one see. beware…Now let’s take advantage long format dataset facet sub-divide dataset separate maps delineated year variable:","code":"\nnrow(mvc) # N = 159 rows corresponds to N=159 Georgia counties## [1] 159\nmvc_long <- mvc %>%\n  select(GEOID, NAME, MVCRATE_05, MVCRATE_14, MVCRATE_17) %>%\n  as_tibble() %>%\n  pivot_longer(cols = starts_with(\"MVCRATE\"),\n               names_to = c(\".value\", \"year\"),\n               values_to = \"mvc_rate\",\n               names_sep = \"_\") %>%\n  mutate(year = 2000 + as.numeric(year)) %>%\n  st_as_sf()\nnrow(mvc_long) # N =477 rows corresponds to 3 years each for N =159 counties  ## [1] 477\n# This is the WRONG way to plot a long dataset!\ntm_shape(mvc_long) +\n  tm_fill('MVCRATE') +\n  tm_borders()\n# If you want a single map from a long dataset, use the subset() function ...\ntm_shape(subset(mvc_long, year == 2017)) +\n  tm_fill('MVCRATE') +\n  tm_borders()\ntm_shape(mvc_long) +\n  tm_fill('MVCRATE') + \n  tm_borders() +\ntm_facets(by = 'year', ncol = 1)"},{"path":"intro-tmap.html","id":"small-multiples-with-tmap_arrange","chapter":"Week 13 Tips for using tmap","heading":"13.6 Small multiples with tmap_arrange()","text":"third way make small multiples, one gives maximum control separate panel, create one time, combining panel using function tmap_arrange(). notable difference name map object create , provide list names tmap_arrange().example used two totally different shape objects illustrate point tmap_arrange() particularly good combining things simply wide long subsets single dataset. approach also good taking totally different approach symbolizing two variables dataset, doesn’t assume trying keep anything .","code":"\nm1 <- tm_shape(mvc) +\n  tm_fill('MVCRATE_05') +\n  tm_borders()\n\nm2 <- tm_shape(trauma) +\n  tm_symbols(shape = 'LEVEL',\n             col = 'LEVEL')\n\ntmap_arrange(m1, m2)"},{"path":"intro-tmap.html","id":"summarizing-small-multiples","chapter":"Week 13 Tips for using tmap","heading":"13.7 Summarizing small multiples","text":"Small multiples common visualization GIS software like ArcGIS. small multiples need create multiple data frames manipulate Layout view; often difficult get consistent scales, legends, coordinates.R, idea faceting quite common much potential spatial epidemiology, emphasized . summarize overarching differences among three approaches future reference.","code":""},{"path":"intro-tmap.html","id":"saving-maps","chapter":"Week 13 Tips for using tmap","heading":"13.8 Saving maps","text":"Saving maps use programs applications important. Images can saved output formats available R image functions. words can save files .png, .pdf, .jpg, .tiff, etc.quick way use export button plot pane R studio.Recall way graphic R looks shaped part active graphic device. screen plot pane default graphic device things arranged look good screen. However save different graphic device (e.g. jpg device), things might look different. sometimes trial--error troubleshooting width, height, dpi options.specify save via code, rather export button (good idea terms reproducible code!) use tmap_save(). save final two-panel map created previous step :now skills make wide variety maps R. fine-tune tmap works customize desired purpose, likely spend lot time looking help documentation online resources. sometimes tedious, process figuring make just map want valuable. time able create sophisticated maps quickly efficiently.","code":"\n# First make it an object by giving it a name, m3\nm3 <- tmap_arrange(m1, m2)\n\ntmap_save(m3, filename = 'mvc_maps.png')"}]
