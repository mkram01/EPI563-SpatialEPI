[{"path":"index.html","id":"how-to-use-this-ebook","chapter":"How to use this eBook","heading":"How to use this eBook","text":"Welcome Concepts & Applications Spatial Epidemiology (EPI 563) Fall 2023 Semester! eBook one several sources information support progress semester. overview course, expectations, learning objectives, assignments, grading, please review full course syllabus Canvas. eBook serves provide ‘jumping point’ content covered week. Specifically, content herein introduce key themes, new vocabulary, provide additional detail complementary asynchronous (pre-recorded) video lectures, foundational synchronous (class) work.","code":""},{"path":"index.html","id":"strategy-for-using-this-ebook","chapter":"How to use this eBook","heading":"Strategy for using this eBook","text":"main body eBook, separate module chapter week. content read preparation week’s material. end eBook (e.g. Appendices), section week’s Lab Handout.general, content within week’s main section divided two parts focusing spatial thinking spatial analysis. dichotomy always hold, broad terms can expect sections specific content class Tuesday versus Thursday respectively.Spatial thinking epidemiology: section introduces vocabulary, concepts, themes important incorporation spatialized geo-referenced data epidemiologic work. minimum, plan read content prior class Tuesday, although likely benefit reading sections Tuesday.Spatial analysis epidemiology: section focused data management, visualization, spatial statistics, interpretation. content relevant work together Tuesday’s, essential successful work Thursday lab activities. expected actually execute code eBook lab thursday!. code primer activities class.Weekly Lab Handout: new addition eBook 2022. Note lot overlap spatial analysis section main portion eBook, lab handout. overlap design! overlap, Lab Handout often contain specificity detail code analysis easier absorb interactively working .Throughout book concepts ideas may highlighted call-blocks.block denotes potential pitfall area caution.block denotes additional bit information additional idea note topic hand.block denotes tip advice best practices efficiency.Please note continually updating eBook throughout semester, choose download, please double-check Last updated date (colored bar bottom screen) sure recent version.\neBook licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"download-ebook.html","id":"download-ebook","chapter":"Download eBook","heading":"Download eBook","text":"download PDF full version eBook click . Note eBook may intermittently updated semester may (may ) care re-download updated version. Note date last updated footer page see versions changed.","code":""},{"path":"software-installation.html","id":"software-installation","chapter":"Software installation","heading":"Software installation","text":"information module follows pre-class video setting R RStudio computer.","code":""},{"path":"software-installation.html","id":"installing-r-on-your-computer","chapter":"Software installation","heading":"Installing R on your computer","text":"August 2023, --date version R 4.3.1. R Project Statistical Computing continually working update improve R, result new versions 1-2 times per year.already R installed, can open console check current version typing (copy/pasting) : R.Version()$version.stringIf R, older version listed , can install update R going R repository: https://www.r-project.org/. Note many ‘mirrors’ servers software stored. Generally wise select one geographically close , although work theory. One mirror relatively close Atlanta : http://archive.linux.duke.edu/cran/","code":""},{"path":"software-installation.html","id":"r-installation-notes-for-mac-os-users","chapter":"Software installation","heading":"0.0.1 R installation notes for Mac OS users","text":"click Mac OS cran webpage, see several different options installation. Specifically, versions Mac Intel processors, different version Mac OS Apple Silicon (e.g. M1 chip).know chip Mac?know whether Intel Apple processors, click Apple icon choose “Mac”. resulting window look either :Chip Apple M1 Pro means Apple Silicon. download arm64 name. description say native Apple silicon.Processor xxx xxx Intel chip. means older Intel chip, download R installer Intel-based Macs.","code":""},{"path":"software-installation.html","id":"installing-xcode-mac-only","chapter":"Software installation","heading":"0.0.1.1 Installing Xcode (Mac only)","text":"packages require compiling done outside R. may need utility package Xcode . R package, instead app available Apple Store. Open Apple Store, search Xcode install.Xcode BIG! don’t much storage hard drive might problem. two options:Don’t install Xcode see happens. Depending configurations machine, might necessary. know problem packages install.Try install “slimmer” version Xcode following instructions.","code":""},{"path":"software-installation.html","id":"r-installation-for-windows-os-users","chapter":"Software installation","heading":"0.0.2 R installation for Windows OS users","text":"using computer running Microsoft Windows OS, click Windows option CRAN website. Choose option reads base click recent downloader.","code":""},{"path":"software-installation.html","id":"installing-rtools43-windows-only","chapter":"Software installation","heading":"0.0.2.1 Installing Rtools43 (Windows Only)","text":"work R packages (e.g. installing new ones) can done base-R tools, instances installation new package requires complex ‘unpacking’ code installation github. Windows machine may require additional tools work. Luckily package ! called Rtools43 (assuming already downloaded installed R 4.2), install install packages .Note Rtools43 package within R, instead Windows utility installed outside R .running Windows, navigate back CRAN website, choose Windows, – instead clicking base – click Rtools follow instructions. Specifically, sure version Rtool aligns version R. example, R 4.3.1, choose Rtools43.","code":""},{"path":"software-installation.html","id":"installing-rstudio-on-your-computer","chapter":"Software installation","heading":"Installing RStudio on your computer","text":"R-Studio one several integrated development environments (IDE) working R. means wrapper around core R functionality makes coding project work R much easier without. develop projects analyses using R within IDE R-Studio. Using R-Studio lets us robust code-editing debugging, code syntax highlighting (e.g. coloring different words according use, identifying possible errors), assistance file management, working larger projects, outputting results.current version RStudio 2023.06.1+524. RStudio --date version, please install update .INSTALL: go https://posit.co/download/rstudio-desktop/UPDATE: Open RStudio go Help Menu choose ‘Check Updates’R-Studio Cheat sheet provides quick reference guide many ways R-Studio makes work R easier.fact, “cheat sheets” lots packages utilities R; browse .","code":""},{"path":"installing-packages-for-this-course.html","id":"installing-packages-for-this-course","chapter":"Installing packages for this course","heading":"Installing packages for this course","text":"base R great deal essential functionality, power R comes rapidly growing list user-created contributed ‘packages’. package simply bundle functions tools, sometimes also including example datasets, basic documentation, even tutorial ‘vignettes’. can see official R packages going : https://cran.r-project.org/web/packages/.common way install package R install.packages() command. instance install package ggplot2 :install.packages(\"ggplot2\")Remember need install package (although may update packages occasionally – see green Update button Packages tab R Studio). want actually use package (example ggplot2) call like :library(ggplot2)call library() working, nothing visible happens. However see errors, might package date (thus needs updated/reinstalled), important dependencies missing. Dependencies packages package depends. Typically installed default, sometimes something missing. , simply install missing package try calling library(ggplot2) .Notice function install.packages('yourPackage') must use quotes around package name. contrast function library(yourPackage) use quotes.submit installation request, note output R console. get warning says installation possible missing package ‘yourPackage’, suggests missing dependency (e.g. something main package needs work correctly). Try installing package mentioned error. trouble, reach TA’s!","code":""},{"path":"installing-packages-for-this-course.html","id":"installing-packages-used-for-general-data-science","chapter":"Installing packages for this course","heading":"Installing packages used for general data science","text":"rest page, copy paste provided code order install packages necessary course. Notice hover right code-chunk html version eBook, see copy icon quick copying pasting.Although copying pasting code, take moment look output. get error messages package install? Sometimes see warning message announcing something, perhaps package installed ok. error message, package probably install.see package installed, try loading typing library(yourPackage). nothing happens (errors) good!packages support general work R:rmarkdown allows creation mixed output documents combine code, documentation results single, readable format.packages tinytex knitr necessary creating R documents including PDF output required submitting assignments.use many data manipulation tools tidyverse. can learn tidyverse : https://tidyverse.tidyverse.org/, can see applications tidyverse packages R Epidemiologists Handbook. tidyverse actually collection data science tools including visualization/plotting package ggplot2 data manipulation package dplyr. reason, install.packages('tidyverse') , actually installing multiple packages!packages pacman utilities help simplify file pathnames package loading behavior.","code":"\ninstall.packages('tidyverse')   \ninstall.packages(c('pacman', 'here'))\ninstall.packages(c('tinytex', 'rmarkdown', 'knitr')) \ntinytex::install_tinytex()  \n# this function installs the tinytex LaTex on your\n#  computer which is necessary for rendering (creating) PDF's "},{"path":"installing-packages-for-this-course.html","id":"installing-packages-use-for-geographic-data","chapter":"Installing packages for this course","heading":"Installing packages use for geographic data","text":"many ways get data want spatial epidemiology R. often (don’t always) use census geographies aggregating units, census populations denominators, following packages useful. designed quickly extract geographic boundary files (e.g. ‘shapefiles’) well attribute data US Census website via API.NOTE: able interact Census bureau API R, need personalized “API key”. enter second line code (e.g. help() function), see information :Request key Census bureauEnter cutoms key machine available needed.need Census API key couple weeks, good start now ask help trouble!","code":"\ninstall.packages(c('tidycensus','tigris')) \n\nhelp('census_api_key','tidycensus')"},{"path":"installing-packages-for-this-course.html","id":"installing-packages-used-for-spatial-data-manipulation-visualization","chapter":"Installing packages for this course","heading":"Installing packages used for spatial data manipulation & visualization","text":"section installs set tools specific goals importing, exporting, manipulating, visualizing, analyzing spatial data.first line packages functions defining, importing, exporting, manipulating spatial data.second line tools use visualizing spatial data (e.g. making maps!).BEWAREThere many large shifts currently underway R architecture spatial analysis. learn, years shifting away older data class defined package sp newer one called sf.addition shift, end 2023 several packages helped link older R functions standard GIS libraries outside R retired. include maptools, rgeos rgdal. Newer packages rely , older packages updated. Note sp, rgeos, rgdal, maptools included list . year uncover new opportunities new bugs related unanticipated package dependencies caused packages aging . ’ll see bumps run year!","code":"\ninstall.packages(c('sp', 'sf', 'raster', 'RColorBrewer', 'OpenStreetMap'))  \ninstall.packages(c('tmap', 'tmaptools')) "},{"path":"installing-packages-for-this-course.html","id":"installing-packages-used-for-spatial-analysis","chapter":"Installing packages for this course","heading":"Installing packages used for spatial analysis","text":"Finally packages specifically spatial analysis tasks carry .","code":"\ninstall.packages(c('spdep', 'CARBayes', 'sparr', 'spatialreg',  'DCluster', 'SpatialEpi', 'smerc'))\ninstall.packages(c('GWmodel', 'spgwr') )"},{"path":"locating-spatial-epidemiology.html","id":"locating-spatial-epidemiology","chapter":"Week 1 Locating Spatial Epidemiology","heading":"Week 1 Locating Spatial Epidemiology","text":"","code":""},{"path":"locating-spatial-epidemiology.html","id":"getting-ready","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.1 Getting Ready","text":"","code":""},{"path":"locating-spatial-epidemiology.html","id":"learning-objectives","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"locating-spatial-epidemiology.html","id":"additional-resources","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.1.2 Additional Resources","text":"Geocomputation R Robin Lovelace. recurring ‘additional resource’ provides lots useful insight strategy working spatial data R. encourage browse quickly now, return often questions handle geographic data (especially class sf) R.introduction ggplot2 package. just one dozens great online resources introducing grammar graphics approach plotting R.basic introduction tmap package also one many introductions tmap mapping package. tmap builds grammar graphics philosophy ggplot2, brings lot tools useful thematic mapping!R SAS users cheat sheet","code":""},{"path":"locating-spatial-epidemiology.html","id":"important-vocabulary","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 1","code":""},{"path":"locating-spatial-epidemiology.html","id":"spatial-thinking-in-epidemiology","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.2 Spatial Thinking in Epidemiology","text":"first learning epidemiology, can difficult distinguish concepts, theories, purpose epidemiology versus skills, tools, methods use implement epidemiology. distinctions foundational collective professional identity, way go work.instance, think epidemiologists data analysts, scientists, data scientists, technicians something else? questions bigger can address class, importance becomes especially apparent learning area spatial epidemiology. tendency discourse spatial epidemiology focus primarily data methods without understanding relate scientific questions health population ultimately responsible. Distinguishing threads overarching goal course, even learn data science spatial analytic tools.One quite simplistic important example questions methods inter-related apparent think data. Data central quantitative analysis, including epidemiologic analysis. data different spatial epidemiology?","code":""},{"path":"locating-spatial-epidemiology.html","id":"unit-of-analysis","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.2.1 Unit of analysis","text":"first thing might come mind course related future spatial epi work, explicitly geographic spatial measures contained within data. content spatial data distinct: addition geographic spatial location may illuminate otherwise aspatial attributes. even fundamental content thinking unit analysis.likely many examples epidemiology coursework, explicit (sometimes implicit) unit analysis individual person. Spatial epidemiology can definitely align individual-level analysis. see, common units observe measure spatial epidemiology – therefore units compose much data – individuals instead geographic units (e.g. census tract, county, state, etc) extension collection aggregation individuals therein.distinction unit analysis important implications epidemiologic concerns including precision, bias, ultimately inference (e.g. meaning can make analysis), ’ll discuss throughout semester.One concrete implication discussion always able answer basic question dataset wish analyze: “one row data represent?” row data one way think unit analysis, often (always) spatial epidemiology row data summary population contained geographic unit boundary.Said another way ecologic summary population. stated , simplistic example important learn spatial statistics methods, also maintain perspective epidemiology population health science. advance public health need good methods also need critical understanding populations support, data analyze, conclusions can reliably draw work.move semester, encourage dig deep methods work, also step back ask questions like “choose method?” “question epidemiology useful ?”","code":""},{"path":"locating-spatial-epidemiology.html","id":"spatial-analysis-in-epidemiology","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3 Spatial Analysis in Epidemiology","text":"","code":""},{"path":"locating-spatial-epidemiology.html","id":"spatial-data-storage-formats","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.1 Spatial data storage formats","text":"worked spatial GIS data using ESRI’s ArcMap, familiar called shapefiles. one common format storing geographic data computers. ESRI shapefiles actually single file, anywhere four eight different files file name different extensions (e.g. .shp, .prj, .shx, etc). different file (corresponding extension) contains different portion data ranging geometry data, attribute data, projection data, index connecting together, etc.may know shapefiles (opinion definitely best) way store geographic data. class recommend storing data format called geopackages indicated .gpkg extension.Geopackages open source format developed functional portable across devices, including mobile devices. useful storing individual files efficient compact way.clear, many formats make claim geopackages ultimate format; just happen meet needs course, much work spatial epidemiologists. worth noting many GIS programs including ArcMap QGIS can read write geopackage format; constraint limitation terms software data stored .gpkg format.","code":""},{"path":"locating-spatial-epidemiology.html","id":"representing-spatial-data-in-r","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.2 Representing spatial data in R","text":"work course assumes basic R user; need expert, assume understand data objects (e.g. data.frame, list, vector), basic operations including sub-setting index (e.g. using square brackets extract modify information: []), base-R plotting, simple regression modeling. familiar R, need quick self-directed learning.good online resources R skills, instructor TA’s can point additional resources needed:Epidemiologist R HandbookR Data Science, particularly introductory chaptersR TutorialJust conceptualization , thinking , data spatial epidemiology requires reflection, actual structure representation data computer tool R also requires attention.Specifically, spatial data R automatically conventional aspatial epidemiologic data often arranged rectangular data.frame (e.g. like spreadsheet rows observations columns variables).spatial data complex just spreadsheet, need complex spatial data software platforms like ESRI’s ArcMap.spatial, dataset must representation geography, spatial location, spatial relatedness, commonly done either vector raster data model (see description vocabulary). spatial geographic representations must stored computer /held computer memory, hopefully means relating associating individual locations corresponding attributes. example, want know attribute (e.g. count deaths given place), location place, ideally want two connected together.past 10+ years, R increasingly used analyze visualize spatial data. Early , investigators tackling complexities spatial data analysis R developed number ad hoc, one-approaches data. worked short term specific applications, created new problems users needed generalize method new situation, chain together steps. settings uncommon convert dataset multiple different formats accomplish task sequence; resulted convoluted error-prone coding, lack transparency analysis.eventual response early tumult thoughtful systematic approach defining class data tackled unique challenges spatial data R. Roger Bivand, Edzer Pebesma others developed sp package defined spatial data classes, provided functional tools interact .sp package defined specific data classes contain represent points, lines, polygons, well raster/grid data. data classes can contain geometry (names like SpatialPoints SpatialPolygons) contain geometry plus related data attributes (names like SpatialPointsDataFrame SpatialPolygonsDataFrame).spatial object can contain information spatial data might include: spatial extent (min/max x, y values), coordinate system spatial projection, geometry information, attribute information, etc.flexibility power sp* class objects, became standard last years. Interestingly, perhaps sophistication sp* class began undermine . sp* class data well-designed programming point view, still little cumbersome (frankly confusing) applied analysts new users.Analysis spatial epidemiology primarily computer programming, producing transparent reliable data pipelines conduct valid, reliable, reproducible analysis. Thus, epidemiologists data scientists desired spatial tools incorporated growing toolbox data science tools R.calls user-friendly intuitive approach spatial data led team (e.g. Bivand, Pebesma, others) develop Simple Features set spatial data classes R. Loaded sf – simple features – package, data format quickly become standard handling spatial data R.power sf class, discussed , makes spatial data behave like rectangular data thus makes amenable manipulation using tool works data.frame tibble objects. Recognizing many users functions prefer older sp* objects, sf package includes number utility functions easily converting back forth.class use sf* class objects preferred data class, tools ’ll learn updated recently thus still require sp*, occasionally go back forth.sf* data classes designed hold essential spatial information (projection, extent, geometry), easy evaluate data.frame format integrates attribute information geometry information together. result intuitive sorting, selecting, aggregating, visualizing.","code":""},{"path":"locating-spatial-epidemiology.html","id":"benefits-of-sf-data-classes","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.3 Benefits of sf data classes","text":"Robin Lovelace writes online eBook, Gecomputation R, sf data classes offer approach spatial data compatible QGIS PostGIS, important non-ESRI open source GIS platforms, sf functionality compared sp provides:Fast reading writing dataEnhanced plotting performancesf objects can treated data frames operationssf functions can combined using %>% pipe operator works well tidyverse collection R packages (see Tips using dplyr examples)sf function names relatively consistent intuitive (begin st_)","code":""},{"path":"locating-spatial-epidemiology.html","id":"working-with-spatial-data-in-r","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.4 Working with spatial data in R","text":"lab, one example dataset use, called ga.mvc quantifies counts rates death motor vehicle crashes Georgia’s \\(n=159\\) counties. dataset vector represents counties polygons associated attributes (e.g. mortality information, county names, etc).","code":""},{"path":"locating-spatial-epidemiology.html","id":"importing-spatial-data-into-r","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.4.1 Importing spatial data into R","text":"important distinguish two kinds data formats. way data stored computer hard drive, way data organized managed inside program like R.shapefiles (.shp) popularized ESRI/ArcMap example format storing spatial data hard drive. contrast, discussion sf* sp* data classes refer data organized inside R.Luckily, regardless data stored computer, possible import almost format R, inside R possible make either sp* sf* data class. means receive data .shp shapefile, .gpkg geopackage, .tif raster file, can easily imported.sf functions act spatial objects begin prefix st_. Therefore import (read) data use st_read(). function determines import data based extension file name specify.Look help documentation st_read(). Notice first argument dsn=, might complete file name (e.g. myData.shp), might folder name (e.g. mygeodatabase.gdb). motor vehicle crash data saved shapefile (mvc.shp, actually six different files computer), geopackage (mvc.gpkg) can read like :can take look defined data class imported objects within R:Notice two objects class (e.g. type data stored within R), even though two different kinds files stored computer: one shapefile one geopackage. st_read() can automatically detect storage format based extension, use appropriate interpreter import data. nice means can bring many types spatial data R!also notice examined class() object, classified sf data.frame class. incredibly important, speaks elegant simplicity sf* data classes!objects classified sf perhaps obvious spatial object; fact object also classified data.frame means can treat object purposes data management, manipulation analysis relatively simple-seeming object: rectangular data.frame.work? explore lab essentially dataset rows (observations) columns (variables). can see variable/column names like :can see dataset attribute variables (e.g. GEOID, NAME, MVCRATE_17), final column called geometry one called geom another.geometry columns different usual run---mill column variables don’t hold single value. Instead, ‘cell’ columns actually contains embedded list \\(x,y\\) coordinates defining vertices polygons Georgia’s counties. spatial location information row contained single variable called geom (alternately, geometry).Another way learn sf object use head() function. addition displaying top six rows data (typical behavior head() function), sf objects head() also print important metadata file.summarize, sf within R powerful :limited data arrive us. acquire data (web, colleague, etc) shapefile, geopackage, raster formats, can imported R.inside R (stored sf data objects), can treat datasets almost aspatial, rectangular datasets. means use sub-setting, filtering, recoding, merging, aggregating without losing spatial information!","code":"\n# this reads in the shapefile\nmvc.a <- st_read('GA_MVC/ga_mvc.shp')\n\n# this reads in the geopackage\nmvc.b <- st_read('GA_MVC/ga_mvc.gpkg')\nclass(mvc.a)## [1] \"sf\"         \"data.frame\"\nclass(mvc.b)## [1] \"sf\"         \"data.frame\"\nnames(mvc.a)## [1] \"GEOID\"      \"NAME\"       \"MVCRATE_17\" \"geometry\"\nnames(mvc.b)## [1] \"GEOID\"      \"NAME\"       \"MVCRATE_17\" \"geom\"\nhead(mvc.a)## Simple feature collection with 6 features and 3 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -84.64195 ymin: 31.0784 xmax: -82.04858 ymax: 34.49172\n## Geodetic CRS:  WGS 84\n##   GEOID                     NAME MVCRATE_17                       geometry\n## 1 13001  Appling County, Georgia   53.99276 MULTIPOLYGON (((-82.55069 3...\n## 2 13003 Atkinson County, Georgia   35.96260 MULTIPOLYGON (((-83.141 31....\n## 3 13005    Bacon County, Georgia    0.00000 MULTIPOLYGON (((-82.62819 3...\n## 4 13007    Baker County, Georgia   31.25000 MULTIPOLYGON (((-84.64166 3...\n## 5 13009  Baldwin County, Georgia   28.94936 MULTIPOLYGON (((-83.42674 3...\n## 6 13011    Banks County, Georgia   32.19921 MULTIPOLYGON (((-83.66862 3..."},{"path":"locating-spatial-epidemiology.html","id":"exporting-spatial-data-from-r","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.4.2 Exporting spatial data from R","text":"importing often primary challenge spatial data R, uncommon might modify alter spatial dataset wish save future use, write disk share colleague.Luckily sf package functionality write sf spatial object disk wide variety formats including shapefiles (.shp) geopackages (.gpkg). , R uses extension specify filename determine target format.write two files, navigate computer folder look written. particular notice .shp file actually many files, .gpkg single file.","code":"\n# Write the file mvc to disk as a shapefile format\nst_write(mvc, 'GA_MVC/ga_mvc_v2.shp')\n\n# Write the file mvc to disk as a geopackage format\nst_write(mvc, 'GA_MVC/ga_mvc_v2.gpkg')"},{"path":"locating-spatial-epidemiology.html","id":"basic-visual-inspectionplots","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.5 Basic visual inspection/plots","text":"want see spatial data? base-R powerful function called plot() can used create easy incredibly complex visualizations graphical representation data.package sf, functionality plot() extended handle uniqueness spatial data. means call plot() spatial object without loaded sf, results different plot() called loading sf.plot() sf, default try make map every variable data frame! Try . want (usually ), can force plot variables providing vector variable names.Sometimes want know something spatial size, extent, shape data. can easily plot geometry spatial object (e.g. attributes). two approaches quickly plot geometry:","code":"\nplot(mvc) # this plots a panel for every column - or actually the first 10 columns\nplot(mvc['MVCRATE_05']) # this plots only a single variable, the MVC mortality rate for 2005\nplot(mvc[c('MVCRATE_05', 'MVCRATE_17')]) # this plots two variables: MVC rate in 2005 & 2017\nplot(st_geometry(mvc)) # st_geometry() returns the geom information to plot\nplot(mvc$geom)  # this is an alternative approach...directly plot the 'geom' column"},{"path":"locating-spatial-epidemiology.html","id":"working-with-crs-and-projection","chapter":"Week 1 Locating Spatial Epidemiology","heading":"1.3.6 Working with CRS and projection","text":"Maps used describe geographical spatial location particular objects representation things planet Earth. maps printed paper screens. words, maps identify locations somewhere planet earth represent flat planar medium.world latitude longitude lines painted ground, earth flat! Instead earth nearly spherical (really geoid) universal reference start measuring.two reasons, maps require minimum coordinate reference system (CRS) define numbers coordinates relate actual places. addition maps best interpreted formally projecting data account artifact induced pretending earth flat.unambiguous way describe CRS /projection using EPSG code, stands European Petroleum Survey Group. consortium standardized hundreds projection definitions manner adopted several R packages including rgdal sf.given dataset already CRS (possibly projection). CRS projection information contained original file imported, usually maintained use st_read(). However sometimes missing must first find . known, might choose change transform CRS projection specific purpose. discuss class.CRS information imported critical find CRS information data source owner.course GIS course (e.g. assumed already exposure geographic information systems generally), learning theory application coordinate reference systems projections primary purpose semester. However, basic knowledge necessary successfully working spatial epidemiologic data. several resources peruse learn CRS, projections, EPSG codes:useful overview/review coordinate reference systems RRobin Lovelace’s Geocompuation R projections sfEPSG website: link searchable database valid ESPG codesHere useful EPSG codes\nFIGURE 1.1: Comparing CRS\nchoice CRS /projection substantial impact rendered map looks, evident figure (source image).already saw CRS/projection information mvc object used head() function ; top read WGS 84.Recall two main types CRS:Geographic say coordinate locations represented latitude longitude degrees;Projected means coordinate values transformed representation spherical geoid onto planar (Euclidean) coordinate system.WGS 84 ubiquitous geographic coordinate system common boundary files retrieved U.S. Census bureau.important question work spatial dataset understand whether primarily geographic projected CRS, one.quick logical test returns TRUE FALSE answer question “sf object simply longitude/latitude geographic CRS?”. answer case TRUE WGS 84 geographic (longlat) coordinate system, additional information projection. FALSE wanted know CRS/projection?somewhat complicated looking output summary CRS stored spatial object. two things note output:top, User input WGS 84At bottom section labeled GEOGCRS says ID[\"EPSG\",4326\"]literally hundreds distinct EPSG codes describing different geographic projected coordinate systems, semester three worth remembering:EPSG: 4326 common geographic (unprojected long-lat) CRSEPSG: 3857 also called WGS 84/Web Mercator, dominant projection used Google MapsEPSG: 5070 code projected CRS called Albers Equal Area benefit representing visual area maps equal manner.One rule thumb determine data degrees lat/long (thus geographic) versus linear units meters miles (thus projected) look xmin, ymin, xmax, ymax printed top output whenever use head(xxx).Degrees latitude (y-axis values) range \\(-90^\\circ\\) \\(+90^\\circ\\), degrees longitude (x-axis values) range \\(0^\\circ\\) \\(180^\\circ\\).contrast projected data cartesian linear units (rather degrees), usually numbers much higher 180.CRS/projection clearly defined, may choose transform project data different system. sf package another handy function called st_transform() takes spatial object (dataaset) one CRS outputs object transformed new CRS.see difference three? Although EPSG 4326 unprojected EPSG 3857 projected (e.g. Mercator conical projection), appear similar, although identical.Mercator projection known increased distortion equator. general prefer use ‘projected’ rather ‘unprojected’ (long/lat ) data visualization analysis, specifically almost always prefer equal area projections choropleth maps, coloring area represented communicates something intensity measure.Whenever bring new dataset need check CRS project transform needed.Important: important distinguish defining current projection data act projecting transforming data one known system new CRS/projection.transform data correctly define current original CRS/projection status. function tells us current status . cases data associated CRS information might completely blank (instance read numerical \\(x,y\\) points geocoding GPS process).cases can set underlying CRS using st_set_crs() attach user-known definition data object, assumes know .two arguments function: first x = objectName, second value = xxx ‘xxx’ valid EPSG code.","code":"\nst_is_longlat(mvc)## [1] TRUE\n# Retrieve CRS metadata from an sf object\nst_crs(mvc)## Coordinate Reference System:\n##   User input: WGS 84 \n##   wkt:\n## GEOGCRS[\"WGS 84\",\n##     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n##         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n##         MEMBER[\"World Geodetic System 1984 (G730)\"],\n##         MEMBER[\"World Geodetic System 1984 (G873)\"],\n##         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n##         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n##         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n##         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n##         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n##             LENGTHUNIT[\"metre\",1]],\n##         ENSEMBLEACCURACY[2.0]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"geodetic latitude (Lat)\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"geodetic longitude (Lon)\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     USAGE[\n##         SCOPE[\"Horizontal component of 3D system.\"],\n##         AREA[\"World.\"],\n##         BBOX[-90,-180,90,180]],\n##     ID[\"EPSG\",4326]]\n# This uses the Albers equal area USA, \nmvc.aea <- st_transform(mvc, 5070)\n\n# This uses the Web Mercator CRS (EPSG 3857) which is just barely different from EPSG 4326\nmvc.wm <- st_transform(mvc, 3857)\n\n# Now let's look at them side-by-side\nplot(st_geometry(mvc), main = 'EPSG 4326')\nplot(st_geometry(mvc.wm), main = 'Web Mercator (3857)')\nplot(st_geometry(mvc.aea), main = 'Albers Equal Area (5070)')"},{"path":"cartography-for-epidemiology-i.html","id":"cartography-for-epidemiology-i","chapter":"Week 2 Cartography for Epidemiology I","heading":"Week 2 Cartography for Epidemiology I","text":"","code":""},{"path":"cartography-for-epidemiology-i.html","id":"getting-ready-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.1 Getting Ready","text":"","code":""},{"path":"cartography-for-epidemiology-i.html","id":"learning-objectives-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"cartography-for-epidemiology-i.html","id":"additional-resources-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.1.2 Additional Resources","text":"CDC Guidance Cartography Public Health Data (complements required reading)Maps LieColor Brewer Website color guidance choropleth mapsAnalyzing US Census Data: Methods, Maps, Models R","code":""},{"path":"cartography-for-epidemiology-i.html","id":"important-vocabulary-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 2","code":""},{"path":"cartography-for-epidemiology-i.html","id":"spatial-thinking-in-epidemiology-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2 Spatial Thinking in Epidemiology","text":"Making pretty maps full extent spatial epidemiology. However, epidemiologic cartography can sometimes beginning end spatial epidemiology given purpose. even epidemiologic analysis goes well beyond mapping (perhaps incorporate aspatial analysis, incorporate sophisticated spatial analysis), ability produce clear, concise, interpretable map important skill.Robb, et al1 write:Disease mapping can used provide visual cues disease etiology, particularly relates environmental exposures….Mapping things allows visualization baseline pattern spatial structure disease, potential detection disease clusters, initial investigation exposure-disease relationship.aspects cartography map design general thematic maps quantitative data. issues seem especially pertinent us epidemiologists quantitative population health scientists. include decisions make color choice process categorizing numerical data visual representation map.especially important epidemiology? primary purpose map visually represent something meaningful spatial geographic variation health health-relevant feature (e.g. exposure resource). Communicating meaningful representing variation matters solely technical GIS task; requires epidemiologic insight.instance approach representing ratio measures odds ratio risk ratio different represent risk rate data, understand scale units distinct case. Similarly, understand characterizing variation heterogeneity normal Gaussian (bell-shaped curve) distribution different uniform highly skewed distribution long right tail. insight scales values differently interpreted epidemiologically must translated sensible choices mapping.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"color-choices","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.1 Color choices","text":"thematic maps, color flexible important tool communication. Color, hue, contrast can accentuate map elements themes minimize others. result can completely change story map tells seemingly small changes use color. means clear explicit choose given color sequence colors, beware unintentionally misrepresenting data color choices.producing choropleth maps, often talk collections colors color ramps color palettes, single color interesting. quick scan either tmaptools::palette_explorer() utility, Color Brewer website demonstrate many colors choose , just matter preference? Perhaps, guidelines keep mind.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"sequential-palettes","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.1.1 Sequential palettes","text":"color palettes use color hue, value, saturation represent symbolize values underlying statistical parameter interest. parameter statistic naturally ordered, sequential monotonic, makes sense choose colors range light dark. Conventionally lighter neutral tones represent lower smaller numbers darker colors intense tones represent higher larger numbers. dark colors jump viewer readily, occasionally inverse used emphasize small values, done caution can counter intuitive.\nFIGURE 2.1: Sequential color palettes\nSequential palettes useful epidemiologic parameters prevalence, risk, rates, continuous exposure values emphasis distinguishing higher values lower values.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"diverging-palettes","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.1.2 Diverging palettes","text":"less common choice, one especially important epidemiologic parameters, diverging palette. pattern, neutral color center sequence, two different color hues become darker intense go center.\nFIGURE 2.2: Diverging color palettes\nmight choose color sequence one two reasons:wish show units vary around overall mean median, highlighting larger versus smaller overall mean/median. instance diverging palettes might emphasize areas particularly high burden disease (therefore need additional attention), well unexpectedly low burden disease (therefore worthy understanding protective factors).mapping epidemiologic measure effect (e.g. ratio difference measure) values null ratio \\(1.0\\) (ratio) \\(0\\) (difference). example map Standardized Mortality/Morbidity Ratios, risk odds ratios, prevalence ratios, potentially diverging data. exception ratio values side null (e.g. \\(>>1\\) \\(<<1\\)).\nFIGURE 2.3: Mapping ratio measure divergent palette\nmap , SMR (ratio county-specific prevalence low birth weight infants overall statewide live birth prevalence) varies \\(0.13\\) \\(2.30\\). words, county SMR \\(1.0\\) average prevalence low birthweight. range data sequential way risk prevalence. Instead neutral color assigned counties range \\(0.90-1.10\\), around null. way indicating counties average typical. contrast, counties increasing excess morbidity darker green, substantially lower morbidity darker purple.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"qualitative-palettes","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.1.3 Qualitative palettes","text":"Qualitative refers categories naturally ordered sequential. instance counties assigned values single leading cause death county, might choose qualitative palette, sequential diverging palette might mislead viewer thinking natural ordering causes less intense color.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"choropleth-binning-styles","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.2 Choropleth binning styles","text":"second topic relevant intersection cartography epidemiologic thinking means choose cut-points visualizing data. words map visually represent underlying statistical value, assign map numerical values colors. depends greatly intended message story map needs tell. interested distinguish units rank higher lower values? primarily focused finding extreme outliers, variation ‘middle’ distribution less interest? distinct purposes give rise different decisions assign colors numerical values data.\nFIGURE 2.4: Comparing binning styles data\ndiscussed lecture, numerous methods styles categorizing continuous data choropleth mapping (e.g. identical data summarized four different styles figure ). Cynthia Brewer (ColorBrewer fame) Linda Pickle (2002) sought evaluate styles effective communicating spatial patterns epidemiologic data.cartographers, Brewer & Pickle critical epidemiologists’ -reliance quantile cut-points, given many strategies seemed cartographic advantages. However, randomizing map ‘readers’ interpret maps underlying epidemiologic data using seven different styles, determined readers accurately reliably interpret disease patterns maps using quantile cut-points. benefits styles purposes, common use communicating spatial areas rank higher lower terms disease burden, quantiles straightforward.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"mapping-time-series","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.2.2.1 Mapping time series","text":"common spatial epidemiology want map spatial patterns disease several different snapshots time series observe evolution disease burden time. changing patterns time raises additional questions make cuts data. several options determining cut-points time series:Pool years data together calculating cut-points (e.g. using quantiles). Use pooled cut-points years.Create custom year-specific cut-points reflect distribution data year separately.Create cut-points based single year apply years.\nFIGURE 2.5: Georgia MVC deaths year common scale\nmap Georgia motor vehicle crash mortality data three different years (2005, 2014, 2017), created tmap using tm_facet() option = year. result, quantile cut-points represent breaks pooling observations across three years. words cut-points come 159 counties times three years: 477 values.common legend applies three maps, strategy useful comparing differences absolute rates across years.\nFIGURE 2.6: U.S. heart disease mortality year-specific scales\nmap heart disease mortality rates county two years (1973-4; 2009-10) uses quantile breaks calculated separately time period. done part heart disease mortality rate declined much years scale distinguished highs lows one map distinguish anything map. case compared absolute rates relative ranking counties two years.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"spatial-analysis-in-epidemiology-1","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3 Spatial Analysis in Epidemiology","text":"Every spatial epidemiology project must include attention data acquisition, cleaning, integration, visualization. specific workflow driven largely overarching epidemiologic question, purpose, goal. section use specific question illustrate key steps data preparation epidemiologic cartography.Case Example Objective: Create choropleth map visualizing geographic variation -cause mortality rate U.S. counties 2016-2018. Compare choropleth map % uninsured U.S. counties.objective directly relevant lab week well Visualizing US Mortality, Visual Portfolio, assignment due later semester.Although specific question dictates specific data needs, four types data frequently needed produce map health outcome state:Numerator data, case representing count deaths per county target yearDenominator data, case representing population risk death county target yearContextual covariate data, case prevalence uninsured U.S. countyGeographic (geometry) data representing shapes boundaries U.S. counties","code":""},{"path":"cartography-for-epidemiology-i.html","id":"obtaining-and-preparing-numerator-data","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.1 Obtaining and preparing numerator data","text":"event interest (e.g. numerator risk, rate, prevalence) can come many sources. conducting primary data collection, arises study design measurement. using secondary data, common use surveillance data (e.g. vital records, notifiable diseases, registries, etc) administrative data source health events.using secondary data sources owned managed another entity, one challenge can occur suppression data protect privacy. example National Center Health Statistics mortality data available CDC WONDER suppresses count deaths, well crude mortality rate, whenever numerator count less ten events. can also instances local state public health agency fails report data NCHS, producing missing values.Depending data format, possible either missing suppressed data inadvertently imported R zero-count rather missing. therefore critically important understand data source guidelines. decision manage zero, missing, suppressed data epidemiologic choice, one must addressed creating map.deal data suppression. many reasons target data may fall thresholds suppression. Perhaps outcome event quite rare, stratifying multiple demographic factors, perhaps counting small geographic unit. suppression problematic mapping, consider pooling multiple years, reducing demographic stratification, using larger geographic areas increase event count reduce number suppressed cells.example, downloaded -cause mortality counts county CDC WONDER 2016-2018 (pooling three years reduce suppression). Lab discuss procedure acquiring data web. importing data appears.","code":"\nhead(death)##    FIPS             County Deaths Population     crude\n## 1 01001 Autauga County, AL    536      55601  964.0114\n## 2 01003 Baldwin County, AL   2357     218022 1081.0836\n## 3 01005 Barbour County, AL    312      24881 1253.9689\n## 4 01007    Bibb County, AL    276      22400 1232.1429\n## 5 01009  Blount County, AL    689      57840 1191.2172\n## 6 01011 Bullock County, AL    112      10138 1104.7544"},{"path":"cartography-for-epidemiology-i.html","id":"obtaining-and-preparing-denominator-or-contextual-data","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.2 Obtaining and preparing denominator or contextual data","text":"mortality data accessed CDC included numerator (count deaths) denominator (population risk). However instances may one dataset provides health event data (numerator), need link population denominator order calculate risk, rate, prevalence.U.S. Census Bureau maintains reliable population count data U.S., available aggregates Census Block Group, Census Tract, Zip code tabulation area, City Place, County, State, Region.Census data can aggregated total population stratified age, gender, race/ethnicity, many variables. census data also contains measures social, economic, housing attributes may relevant measure context exposures spatial epidemiologic analyses. two broad types demographic socioeconomic data released Census Bureau.Decennial Census tables (theoretically) count 100% population every 10 years. can cross-classified age, race/ethnicity, sex, householder status (e.g. whether head house owns rents many people live house)American Community Survey (ACS) tables provide much larger number measures based samples rather complete counts. ACS began early 2000’s continually sampled survey. Despite collected every year, many small areas (e.g. census tracts even counties) enough responses single year make reliable estimates. Therefore ACS data pooled 5-year moving-window datasets. instance 2015-2019 ACS reports estimates responses collected time period, available Census Block Group .may accessed Census ACS data directly Census Bureau website classes tasks past. interest reproducibility efficiency, introduce tidycensus package R. excellent tool acquiring either Decennial Census ACS data directly within R. advantage twofold:can quicker learn ;\n2, makes data acquisition fully reproducible without unrecorded steps happening web browsers. words actual code details downloaded (rather un-documented steps clicking downloading browser).practice code next sections lab. included primer. sections walk one way download prepare data quantify county-level prevalence population uninsured, might covariate interest examining spatial variation mortality.selected code relatively efficient, although may find complex confusing. include like explore data-manipulation functions R. Please note need learn functions Census data acquisitions section course, although might find related approaches useful. Note also many ways accomplish anything R, achieve ends different strategies.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"setting-up-census-api","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.2.1 Setting up Census API","text":"access Census products (e.g. attribute tables geographic boundary files) using tidycensus package, need register declaring API key. haven’t already done , go register key.","code":"\n# Only do this if you haven't already done it; it should only need to be done once.\n\ntidycensus::census_api_key('YourKeyHere', install = T) "},{"path":"cartography-for-epidemiology-i.html","id":"choosing-variables","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.2.2 Choosing Variables","text":"far biggest challenge requesting data Census Bureau knowing want, stored. Census data distributed aggregated counts contained specific tables (unique ID), made specific variables (also unique ID composed table ID plus unique ID). two ways find variables:go Census website browse around. instance Census Data Explorer website one way browse topics variablesYou download variables given year R, use filters search .code queries Census website (assuming internet connection) requests list variables ACS 5-year pooled dataset (e.g. acs5) window time ending 2018 (e.g. 2014-2018). also specify cache = T just means save results quicker loading ask future.may easiest look dataset using View() function. , see three variables, option click Filter button (upper left View pane; looks like funnel). Filter option one way search key words either label concept column.interested capturing prevalence uninsured county. Try :Go View mode variables (e.g. View(all_vars))Click Filter buttonType insurance concept fieldType B27001 name field\nFIGURE 2.7: Screenshot RStudio View() ACS variables\nwant list specific tables variable ID’s extract Census. lab use detailed code accomplish goal.may noticed full list ACS variables nearly 27,000 variables! code use tricks filter huge list variables get names want. relies tidyverse package stringr great manipulating character variables (great many data science tasks; read stringr ). case using filter just table want (e.g. B27001), get names variables contain string ‘health insurance’.list variables want acquire; one represents count uninsured multiple age groups. sum get total population uninsured prevalence.","code":"\nlibrary(tidycensus)\n\nall_vars <- load_variables(year = 2018, dataset = 'acs5', cache = T)\n\nhead(all_vars)\na <- all_vars %>% \n  filter(stringr::str_detect(name, 'B27001')) %>%  # this limits to rows for the B27001 table\n  filter(stringr::str_detect(label, 'No health insurance'))  # this limits to rows with this text\n\nmyVars <- c('B27001_001', a$name)##  [1] \"B27001_001\" \"B27001_005\" \"B27001_008\" \"B27001_011\" \"B27001_014\"\n##  [6] \"B27001_017\" \"B27001_020\" \"B27001_023\" \"B27001_026\" \"B27001_029\"\n## [11] \"B27001_033\" \"B27001_036\" \"B27001_039\" \"B27001_042\" \"B27001_045\"\n## [16] \"B27001_048\" \"B27001_051\" \"B27001_054\" \"B27001_057\""},{"path":"cartography-for-epidemiology-i.html","id":"retrieving-data-from-census","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.2.3 Retrieving data from Census","text":"actually retrieve data Census use function get_acs() (getting decennial data function get_decennial()). request data must specify geography (e.g. want counts states, counties, census tracts, census block groups?), variables, year, dataset. Look ?get_acs read options.following code chunks use dplyr tidyverse verbs %>% (pipe) connect data steps together. complex first, worth carefully examining step works. familiar syntax, probably helpful review Appendix section dplyr.Looking first rows data object insure_tidy , might surprised column labeled variable, cells within column actually thought variable names! data structured tidy format, happens long wide. Read transposing data . following steps reshape data useful.code :define geography = county.Specify vector (previously created named myVars) variables downloadSpecify year interest. Note 2018 references 2014-2018 5-year windowspecify survey, often acs5The code necessary variables age-specific counts number uninsured people. one variable, B27001_001 count included table. words, denominator calculating prevalence uninsured. Therefore following code :filter() restricts rows data variable denominator count (B27001_001). Filter like SASrename() way rename variables likingselect() drops variable called variableThe code addresses issue common census tables: may constructed way want . discussed , case values counts age group, want single count entire population county. Therefore, necessary sum across aggregate counts age groups get single count (numerator number uninsured) county.strategy used specific data long format, happens tidy data case. Read changing long wide .code achieves steps:filter() using != mean “equal ”; simply removes denominator variable, summing numerator countsgroup_by() useful dplyr verb; similar using class SAS, tells R something separately group (e.g. GEOID county case)summarise() verb works hand--hand group_by(). grouping declares groups, summarise() tells . case just want count uninsured across age gruops.simple merge, worth mentioning steps:left_join() one famiy merging verbs. left left_join() simply means start first table (one left) merge second table. implications whether rows rows left right (first second) table retained. case left first table insure_denom right second table insure_num)mutate() calculates uninsured prevalenceselect() excludes unnecessary variablesThe code process complex. specific exact scenario, scenario might require different steps. challenge , new spatial analyst, think mind data looks beginning want look end. create sequence steps progresses beginning end. takes practice, worthwhile spatial epidemiology, also data science processing generally.","code":"\n# First, request the data from ACS\ninsure_tidy <- get_acs(geography = 'county',\n                     variables = myVars,\n                     year = 2018, \n                     survey = 'acs5') %>%\n  select(-moe)\n\n# Look at the resulting object\nhead(insure_tidy)\n# Now I pull out the denominator\ninsure_denom <- insure_tidy %>%\n  filter(variable == 'B27001_001') %>%\n  rename(TOTPOP = estimate) %>%\n  select(-variable)\n\n# Look at the resulting object\nhead(insure_denom)## # A tibble: 6 × 3\n##   GEOID NAME                    TOTPOP\n##   <chr> <chr>                    <dbl>\n## 1 01001 Autauga County, Alabama  54277\n## 2 01003 Baldwin County, Alabama 205452\n## 3 01005 Barbour County, Alabama  22882\n## 4 01007 Bibb County, Alabama     20468\n## 5 01009 Blount County, Alabama   57169\n## 6 01011 Bullock County, Alabama   9978\n# Now I sum up all the variables for the numerator\ninsure_num <- insure_tidy %>%\n  filter(variable != 'B27001_001') %>%\n  group_by(GEOID) %>%\n  summarise(no_insure = sum(estimate))\n\nhead(insure_num)## # A tibble: 6 × 2\n##   GEOID no_insure\n##   <chr>     <dbl>\n## 1 01001      3875\n## 2 01003     20864\n## 3 01005      2558\n## 4 01007      1619\n## 5 01009      6303\n## 6 01011      1076\n# Finally, merge the numerator and denominator in order to calculate prevalence\nuninsured <- insure_denom %>%\n  left_join(insure_num, by = 'GEOID') %>%\n  mutate(uninsured = no_insure / TOTPOP) %>%\n  select(GEOID, uninsured)\n\n# Take a look at the resulting object\nhead(uninsured)## # A tibble: 6 × 2\n##   GEOID uninsured\n##   <chr>     <dbl>\n## 1 01001    0.0714\n## 2 01003    0.102 \n## 3 01005    0.112 \n## 4 01007    0.0791\n## 5 01009    0.110 \n## 6 01011    0.108"},{"path":"cartography-for-epidemiology-i.html","id":"obtaining-and-preparing-geographic-data","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.3 Obtaining and preparing geographic data","text":"final type data needed geographic geometry data. , source geometry data varies study specifics: may need polygons (e.g. political administrative boundaries), lines (e.g. transportation networks), points (e.g. hospitals, food stores, toxic waste sites, etc). hand may need data raster format, including weather air pollution surfaces. open-access versions many types geographic data online.choropleth mapping, area units including administrative political boundaries commonly used. U.S. context, Census geographies frequently used, including blocks, block groups, tracts, zip-code tabulation areas, counties, cities & places, metropolitan areas, tribal areas, states, regions. section provide brief introduction downloading census boundary files directly R.","code":""},{"path":"cartography-for-epidemiology-i.html","id":"obtain-geometry-data-from-tidycensus","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.3.1 Obtain geometry data from tidycensus","text":"first option minor modification code previous section acquiring census count data. get_acs() function argument geometry = FALSE default. However, change geometry = TRUE, automatically retrieve data sf object including geometry column!One argument get_acs() demonstrated shift_geo. FALSE default, set shift_geo = TRUE, return boundaries projected Albers Equal Area, states Hawaii Alaska artificially shifted fit thematic map U.S.","code":"\ninsure_tidy <- get_acs(geography = 'county',\n                     variables = myVars,\n                     year = 2018, \n                     geometry = TRUE,   # added geometry = T\n                     survey = 'acs5') "},{"path":"cartography-for-epidemiology-i.html","id":"obtain-geometry-data-from-tigris","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.3.2 Obtain geometry data from tigris","text":"tidycensus package actually requests geometry depending another package called tigris (Census geography files called TIGER files). obtaining attributes (e.g. population counts) geometries time, tidycensus package makes sense. However, sometimes need geometry, perhaps data come sources Census Bureau.want directly obtain areal boundary units, coastline data, road rail networks, voting districts, spatial data maintained Census Bureau, consider using tigris package. Try looking help documentation (e.g. ?tigris, click Index link bottom see options).demonstrate retrieving U.S. county boundaries:code :counties() function one dozens tigris downloading specific kinds boundary datacb = TRUE adjusts level detail resolution boundaries. default cb = FALSE returns detailed data, quite large. Setting cb = TRUE defaults generalized (1:500k scale) shape.resolution = '5m' specification want even generalized boundary file. 1:5 million scale coarse terms resolution curves county boundaries, also smaller file. must decide balance file size resolution specific need.year = 2018 specifies vintage boundary files. Tracts, counties, cities, etc change boundaries year year.class = 'sf' results object returned sf object, rather sp class data (default).can see summary data CRS/projection EPSG code 4269 (unprojected).boundary file look like?Census boundaries include information U.S. counties territories! Therefore map looks way Guam, American Samoa, Puerto Rico, well Hawaii Alaska included. interested mapping “lower 48” contiguous states, exclude . code , also transform project data Albers Equal Area using EPSG code","code":"\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\nus <- counties(cb = TRUE,\n                        resolution = '5m', \n                        year = 2018,\n                        class = 'sf')\nsummary(us)##    STATEFP            COUNTYFP           COUNTYNS           AFFGEOID        \n##  Length:3233        Length:3233        Length:3233        Length:3233       \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##     GEOID               NAME               LSAD               ALAND          \n##  Length:3233        Length:3233        Length:3233        Min.   :8.209e+04  \n##  Class :character   Class :character   Class :character   1st Qu.:1.079e+09  \n##  Mode  :character   Mode  :character   Mode  :character   Median :1.563e+09  \n##                                                           Mean   :2.833e+09  \n##                                                           3rd Qu.:2.367e+09  \n##                                                           Max.   :3.770e+11  \n##      AWATER                   geometry   \n##  Min.   :0.000e+00   MULTIPOLYGON :3233  \n##  1st Qu.:7.038e+06   epsg:4269    :   0  \n##  Median :1.950e+07   +proj=long...:   0  \n##  Mean   :2.161e+08                       \n##  3rd Qu.:6.159e+07                       \n##  Max.   :2.599e+10\nplot(st_geometry(us))\nus <- us %>%\n  filter(!(STATEFP %in% c('02', '15', '66', '60', '78', '72', '69'))) %>%\n  select(GEOID, STATEFP, COUNTYFP, NAME) %>%\n  st_transform(5070)\n\nplot(st_geometry(us))"},{"path":"cartography-for-epidemiology-i.html","id":"merging-attributes-and-geography","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.4 Merging Attributes and Geography","text":"final step data preparation bringing together attribute data geometry data, assuming already incorporated. Assuming attributes data.frame (perhaps tibble, tidyverse data table object), geometry sf object (also class data.frame), merge straightforward. needed merging joining data:Unique key ID variable attribute data matches ID geometry dataUnique key ID variable geometry data matches ID attribute dataMatching ID’s require variable name require variable type.merging several datasets, one sf object, put dataset first sequence, insure final object remains class sf. put sf first, may need re-define object sf end. See Appendix st_as_sf() detail.","code":"\nus2 <- us %>%\n  left_join(death, by = c('GEOID' = 'FIPS')) %>%\n  left_join(uninsured, by = 'GEOID')"},{"path":"cartography-for-epidemiology-i.html","id":"mapping-mortality-uninsured","chapter":"Week 2 Cartography for Epidemiology I","heading":"2.3.5 Mapping Mortality & Uninsured","text":"","code":"\nlibrary(tmap)\n\nt1 <- tm_shape(us2) + \n  tm_fill('crude',\n          style = 'quantile',\n          palette = 'BuPu',\n          title = 'Rate per 100,000 py') + \n  tm_borders(alpha = 0.2) +\ntm_credits('Source: CDC Wonder',\n           position = c('RIGHT', 'BOTTOM')) + \ntm_layout(main.title = 'All-cause mortality rate, 2016-2018',\n          bg.color = 'grey85')\n\nt2 <- tm_shape(us2) + \n  tm_fill('uninsured',\n          style = 'quantile',\n          palette = 'YlOrRd',\n          title = '% Uninsured',\n          legend.format = list(fun=function(x) paste0(formatC(x * 100, \n                                                              digits=1, \n                                                              format=\"f\"), \"%\"))) +\n  tm_borders(alpha = 0.2) +\ntm_credits('Source: American Community Survey',\n           position = c('RIGHT', 'BOTTOM')) + \ntm_layout(main.title = 'Uninsured Prevalence, 2014-2018',\n          bg.color = 'grey85')\n\n\ntmap_arrange(t1, t2, ncol = 1)"},{"path":"cartography-for-epidemiology-ii-spatial-ethics.html","id":"cartography-for-epidemiology-ii-spatial-ethics","chapter":"Week 3 Cartography for Epidemiology II: Spatial Ethics","heading":"Week 3 Cartography for Epidemiology II: Spatial Ethics","text":"","code":""},{"path":"cartography-for-epidemiology-ii-spatial-ethics.html","id":"learning-objectives-2","chapter":"Week 3 Cartography for Epidemiology II: Spatial Ethics","heading":"3.1 Learning objectives","text":"","code":""},{"path":"cartography-for-epidemiology-ii-spatial-ethics.html","id":"additional-resources-2","chapter":"Week 3 Cartography for Epidemiology II: Spatial Ethics","heading":"3.2 Additional resources","text":"Report confidentiality issues policies related geospatial data public health applicatins","code":""},{"path":"cartography-for-epidemiology-ii-spatial-ethics.html","id":"important-vocabulary-2","chapter":"Week 3 Cartography for Epidemiology II: Spatial Ethics","heading":"3.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 3","code":""},{"path":"cartography-for-epidemiology-ii-spatial-ethics.html","id":"spatial-thinking-in-epidemiology-w3","chapter":"Week 3 Cartography for Epidemiology II: Spatial Ethics","heading":"3.4 Spatial Thinking in Epidemiology, w3","text":"“Progress achieving health depends upon effectively collecting, integrating, \nutilizing medical, public health, socioeconomic, environmental, physical science data.”“Although new technological advances can empower individuals neighborhoods seeking\nresources better health care, also heightened concerns individual privacy\nconfidentiality.”– Confidentiality Issues Policies Related Utilization Dissemination Geospatial Data Public Health ApplicationsEthical concern justice, beneficence, respect persons ground guidelines practices responsible conduct public health research. work geospatial data concerns lessened instead often heightened, power locational information means discerning private information risk intended unintended breaches confidentiality even transmission stigma groups highlighting health status marginalized populations.","code":""},{"path":"cartography-for-epidemiology-ii-spatial-ethics.html","id":"risks-of-privacy-breaches-in-collection-of-geospatial-information","chapter":"Week 3 Cartography for Epidemiology II: Spatial Ethics","heading":"3.4.1 Risks of privacy breaches in collection of geospatial information","text":"Geographic identifiers scale state (e.g. county, city, census tract, address) considered Protected Health Information  HIPAA connected individual health information. Surveillance research activities routinely collect geospatial information contact notification purposes, reporting, although many consent forms explicitly explain intended purpose use geospatial information.individual expect protection privacy individual PHI date birth name, always explicit information address can uniquely identifiable linkable data. Privacy breached app-based geocodes captured without consent, geospatial information collected without express consent (e.g. research respondent asked report address someone social network without persons consent).respect personal autonomy dictates individuals permitted control private information, can also risks beyond breach privacy. instances, disclosed geospatial information result harms participant others. example collected address information inadvertently released someone seeking commit violence (e.g case intimate partner violence). Similarly, studies collecting geospatial information can () requested force law aid investigation prosecution suspected crimes. Thus collection geospatial information must well reasoned respect risk benefit participant, appropriate notification consenting process, protections place maintain confidentiality.","code":""},{"path":"cartography-for-epidemiology-ii-spatial-ethics.html","id":"risk-of-confidentiality-breaches-through-unintentional-de-identification","chapter":"Week 3 Cartography for Epidemiology II: Spatial Ethics","heading":"3.4.2 Risk of confidentiality breaches through unintentional de-identification","text":"private geospatial data collected, responsibility data owners (e.g. public health agencies, researchers) protect confidentiality disclosed private information. Confidentiality protection refers secure control confidential data well avoidance unintended re-identification data deemed ‘de-identified’ data linkages.Maintaining data security critical public health research surveillance activities, sometimes geospatial data ignored unique identifier. one instance submitted data request public health agency obtain surveillance data abortion incidence. data delivered Excel sheet individual identifiers name date birth removed, field address residence included. address incredibly powerful unique identifier, particularly combined fields including age sex.Geospatial data can stored separately research attributes, maintaining key linkage instances spatial data needed. needed, less risk accidental disclosure fields.Another risk drives many public health agencies restrictive guidelines around data suppression reporting, concern re-identification individuals aggregated data small cell size ability discern identity quasi-identifiers. example, age, race, ethnicity, health outcome quasi-identifiers instances cross-tabulation make individuals unique nearly .study 1990 decennial census, researchers found 87% U.S. population uniquely identified three variables: exact date birth, zip code, gender! due part combined granularity specificity two variables: date birth zip code. instances, reporting health events zip code level without respect age, perhaps age categorized coarse groups eliminate risk. take home message stratification data prefer better scientific understanding can quickly lead least sub-groups individually nearly individually identifiable.","code":""},{"path":"cartography-for-epidemiology-ii-spatial-ethics.html","id":"risk-of-stigmatization-of-place","chapter":"Week 3 Cartography for Epidemiology II: Spatial Ethics","heading":"3.4.3 Risk of stigmatization of place","text":"final ethical concern particularly relevant disease mapping activities concern unintentional harm persons populations stigmatization place. can happen map identifies locations marginalized populations spend time, serves either stigmatize group, stigmatize others unassociated group, sharing location. stigmatization can lead psychosocial harms, also can alter behavior institutional forces including social services, law enforcement, health services.Examples stigmatization place include identification venues men sex men seek partners, mapping concentrations commercial sex workers injection drug users. concern stigmatization place also raised point view social epidemiology, predominantly Black brown neighborhoods repeatedly characterized ‘unhealthy’. potential harm perpetrated maps arise (presumably well-intended) desire highlight unjust burdens, failure similarly highlight resilience face burdens.Relatedly, many spatial representations economic racial disparities fail name factors give rise inequities, including role socio-historical structural discrimination. failing name structural racism policies serve concentrate affluence separately concentrated poverty, maps contribute narrative communities way blame health outcomes.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"disease-mapping-i-aspatial-empirical-bayes","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","text":"","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"getting-ready-2","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.1 Getting ready","text":"","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"learning-objectives-3","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"additional-resources-3","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.1.2 Additional Resources","text":"Arianna Planey blog spatial thinking MAUPWaller L, Gotway C. Applied Spatial Statistics Public Health Data. Hoboken, NJ: John Wiley & Sons, Inc; 2004.Clayton D, Kaldor J. Empirical Bayes estimates age-standardized relative risks use disease mapping. Biometrics. 1987 Sep;43(3):671–81.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"important-vocabulary-3","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 4","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"spatial-thinking-in-epidemiology-2","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.2 Spatial Thinking in Epidemiology","text":"Disease mapping located intersection statistics, geography, epidemiology. Whereas ---box GIS approach making maps health statistics (e.g. ’ve referring epidemiologic cartography) takes raw data simply shows map, disease mapping typically implies interested going beyond just making pretty maps. Instead driven core epidemiologic questions concerned fundamental epidemiologic statistical issues.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"why-do-we-need-disease-mapping","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.2.1 Why do we need disease mapping?","text":"defining driver purpose epidemiology interest characterizing estimating distribution determinants health populations. Disease mapping primarily focused former (distribution health), providing novel insight geographic patterns health disease. latter (determinants health) can begin addressed Modules 3 4 course focusing Clustering Spatial Regression.spatially describe distribution disease, epidemiologists primarily interested two -arching question:intensity disease health spatially heterogeneous spatially homogeneous?spatial structure spatial correlation location disease intensity?Spatial heterogeneity means differences intensity disease sub-regions compared others. Another way saying , local parameter (e.g. rate, risk, prevalence, etc) least sub-regions study area different global parameter (e.g. overall rate, risk, prevalence, etc) entire study area.contrast, spatial homogeneity means know overall, global parameter, basically know every local parameter, plus minus random variation. Looking heterogeneity whole reason mapping! occurrence disease everywhere, map tell us much! previous weeks mapped disease, epidemiologic cartography efforts date fall short attend following three challenges:Parameter estimate instability due sparse data/rare events;Spurious heterogeneity arising ‘confounding’ nuisance covariates;Biased description spatial heterogeneity arising modifiable areal unit problem (MAUP), form ecologic fallacyThe following three sections provide additional detail challenges.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"the-problem-and-approach-to-data-sparsity","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.2.1.1 The problem and approach to data sparsity","text":"Ideally like maps visualize values target parameter (e.g. risk, rate, prevalence, SMR, etc) accurate, reliable, precise. words don’t want map shows differences simply result random error due small populations small number events.Therefore, reliable precise estimation epidemiologic parameters requires sufficient data (e.g. sample size region mapped) produce summary. either disease quite rare – resulting small numerator – population risk quite sparse – resulting small denominator – estimate disease burden inherently unstable imprecise. means adding just one two events persons risk notable impact estimate.example, imagine county 10 people risk death three consecutive years (e.g. die, others born move county). one year, perhaps none die, next year one dies, third year three die. mortality rate estimated 0%, 10% 30%. find implausible actual risk death 0% location; also exceptional actual mortality risk 10%. words estimates mathematically true given year, statistically unstable, frankly implausible indicator true underlying risk population. problem estimate mortality rate derived little data.practice, public health agencies often suppress data counts small, concern confidentiality, also resulting estimates unstable potentially misleading. already discussed two approaches address data sparsity resulting parameter instability imprecision:Aggregate counts disease events population risk multiple years time intervals increase opportunity events, extend amount person-timeAggregate counts disease events population risk geographic units pool together larger populations. example data mortality may sparse census tract level might stable pooling tracts respective county level.spend next several weeks exploring range methods together constitute third option: statistical smoothing stabilization. tools use amount information (function sample size) smooth extreme highs extreme lows effort recover plausible ‘true’ amount spatial heterogeneity. critical goal disease rate stabilization smooth necessary, true highs lows persist, spurious unstable values adjusted.week use aspatial global Empirical Bayes estimators first approach parameter stabilization. future weeks explore spatial Empirical Bayes, kernel density estimators, fully Bayesian estimators additional strategies producing maps highlights signal spatial heterogeneity net noise random error.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"the-problem-and-approach-to-confounding-or-nuisance-sources-of-heterogeneity","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.2.1.2 The problem and approach to confounding or nuisance sources of heterogeneity","text":"Confounding epidemiology refers specific causal structure, wherein association putative exposure target disease outcome spuriously associated backdoor path one confounders.disease mapping formal ‘exposure’ (e.g. directly measure aspect place, instead estimating aggregate outcome), place perhaps stand-unmeasured attributes vary space. Therefore probably call confounding strictest sense word.Instead can imagine covariates simply nuisance. means explain amount spatial heterogeneity, epidemiologist particularly interested role explanation. Instead wish know still spatial heterogeneity beyond covariates. example consider comparison mortality rates state:Using crude mortality rate, clear Florida mortality rate perhaps 30% higher Alaska, suggesting something really awful going Florida compared Alaska! However adjust standardize age, actually Alaska slightly higher (age-adjusted) mortality rate.numbers technically accurate, depending purpose either number useful. However, many purposes wish map mortality across states, might think differences age-structure (e.g. many retirees Florida Alaska) nuisance accomplishing goal; disease mapping age-adjusted estimate might useful.strategies spatial epidemiology addressing ‘confounding’ (e.g. removing effects nuisance variables) similar non-spatial epidemiology. Standardization, stratification, regression control conventional tools.disease mapping quite common use standardization tool balance condition one covariates, age. However methods including fully Bayesian models later spatial regression models, possible control multiple covariates.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"the-problem-and-approach-to-maup","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.2.1.3 The problem and approach to MAUP","text":"Another source bias error disease mapping thinking occur aggregate data individual’s health outcomes areal units like census tracts, zip codes, counties, school districts, etc. “aggregation bias” called modifiable areal unit problem MAUP, spatial version familiar ecologic fallacy.present, MAUP can produce misleading disease maps aggregate risk, prevalence, rate good reflection experience individuals within polygon.two ways MAUP can occur:Arbitrary zoning boundaries create aggregates. Often, polygons, shapes, boundaries use aggregate health data used convenient, rather meaningfully capture something social environmental spatial organization health events. result mis-alignment actually happening way count .Arbitrary scale level aggregation. occurs aggregate level scale different level scale population health generated. single ‘right’ scale. depends process interest. ‘correct’ scale understanding effect Medicaid expansion ACA (e.g. probably states good scale) likely different ‘correct’ best scale understanding role healthy commercial food retailers obesity (e.g. probably city neighborhood scale appropriate).One key take away discussion bias MAUP arises way carry analysis align way health outcomes generated produced. words, aggregation zoning similarly harmful (e.g. MAUP always problem)!work spatial epidemiologist consider aligned (mis-aligned) available aggregation respect hypothetical process generated spatial variation health. Sometimes possible explore sensitivity results choice scale zoning repeating analyses alternative boundaries scales.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"using-statistics-and-probability-models-to-improve-disease-mapping","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.2.2 Using statistics and probability models to improve disease mapping","text":"epidemiology, spend lot time trying disentangle ‘noise’ ‘signal’ collections data relationships. evident focus two broad buckets error: random error comes chance related sample size; systematic error comes structural bias (e.g. confounding, selection, misclassification/measurement error) driven sample size therefore fixed corrected increasing sample size).make inference (make meaning decisions) data take account random error adopt statistical probability models describe role chance alone generating values. instance many statistics operate assumptions related Gaussian normal distributions. also rely Poisson binomial distributions evaluate variation differences count binary data respectively.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"how-are-statistics-different-in-space","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.2.2.1 How are statistics different in space?","text":"Spatial statistics huge field, well broader cover week, entire semester. However worth introducing key ideas motivate statistics using.Health outcome events typically occur level individual, individuals can referenced respect location space. Consider, example study region represented blue square image . population distributed across region composed individuals occupying particular \\(x,y\\) location.population defined geographic bounds, may individuals experiencing health event. observe (subset) individuals corresponding point locations point time. observation represents specific realization spatial point process. words can imagine individual random chance experiencing event, set events indexed location one realization version random process evident space.\nFIGURE 2.1: Spatial point process\ndescribe quantify observed describe spatial disease intensity event spatially continuous surface. words every location, intensity amount disease per unit-area (e.g. cases per square kilometer). calculate single, global, measure spatial intensity figure divide events area:\\(\\frac{events}{Area}=\\frac{14}{4km^{2}}=\\frac{3.5}{km^{2}}\\)simplistic case assumed population risk evenly distributed across study region. realistically, can normalize events spatially-varying population risk (e.g. difference density people city, suburbs rural) quantify spatial intensity disease.\nFIGURE 2.2: Approximating intensity areal aggregates\noften exact \\(x,y\\) location every person risk every health event, observe full spatial point process thus estimate continuous spatial intensity surface. However, can approximate spatial intensity aggregating health events population summarizing ratio (e.g. risk, rate, prevalence) per areal unit. figure , rectangle contains \\(n=100\\) person-years risk, producing following disease rates estimating spatial intensity disease:data form (e.g. counts events counts population), can use one several parametric statistical probability distributions common epidemiology including Poisson, binomial, negative binomial.probability distributions useful?probability distribution mathematical expression might expect happen simply due random chance; said another way, probability distribution describes expectations null hypothesis. choose different probability distributions suit different types data continuous, binary, count.Relating count disease events probability distribution permits calculation standard errors confidence intervals expressing precision certainty estimate. Alternatively calculate p-value means test evidence consistency null hypothesis.brief summary probability distributions common disease mapping:formulas :\\(Y_i\\) count health events \\(i_{th}\\) areal unit\\(N_i\\) count population risk \\(i_{th}\\) areal unit\\(r_i\\) risk \\(i_{th}\\) areal unit (e.g. estimated \\(Y_i / N_i\\))\\(E_i\\) expected count, calculated \\(N_i\\times r\\), \\(r\\) overall reference level risk (note subscript \\(_i\\); means global risk rather risk local region \\(_i\\). expected simply means burden disease \\(i_{th}\\) areal unit experienced reference risk.\\(\\theta_i\\) relative risk \\(i_{th}\\) areal unit; essentially relative deviation region expected.Don’t panic looking formulas. take away points:Poisson distribution classic distribution use evaluating counts events possibly offsetting time--risk person-years. latter point “offset” use Poisson distribution model disease rates.\nPoisson assumes mean distribution variance distribution.\nPoisson distribution approximates disease intensity rate well rare disease processes. Therefore Poisson good choice outcome rare.\nPoisson assumes mean distribution variance distribution.Poisson distribution approximates disease intensity rate well rare disease processes. Therefore Poisson good choice outcome rare.Binomial distribution useful characterizing disease occurrence non-rare common disease processes.Poisson-gamma Mixture may foreign. However, may heard Negative Binomial distribution count data? Poisson-gamma mixture essentially negative binomial model. probability distribution like Poisson, except without expectation mean equal variance. words robust called -dispersion, variation count greater expected Poisson.\n-dispersion quite common spatial epidemiology often unobserved factors driving occurrence disease area, unobserved differences produce event intensity strictly Poisson nature. use Poisson-gamma reason.\n-dispersion quite common spatial epidemiology often unobserved factors driving occurrence disease area, unobserved differences produce event intensity strictly Poisson nature. use Poisson-gamma reason.want learn Poisson point processes probability distributions spatial epidemiology, highly recommend Lance Waller’s text, Applied Spatial Statistics Public Health Data (Waller & Gotway, 2004). available electronically via Woodruff Library.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"spatial-analysis-in-epidemiology-2","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3 Spatial Analysis in Epidemiology","text":"example dataset, next four weeks disease mapping aim estimate spatial heterogeneity county level occurrence low birthweight (VLBW; weight birth < 1500 grams) babies 2018-2019. data derived Georgia OASIS website.outcome public health importance high morbidity mortality associated born early small. However, overall rate VLBW 1.8%, rare outcome, likely sparse data many rural counties.maps , can visualize observed VLBW prevalence well prevalence restricted counties meeting NCHS suppression rule natality records (e.g. suppress cell sub-population reporting count < 10). map right 85 159 counties Georgia suppressed data. suggests , even know values (e.g. aren’t suppressed) thinking issues imprecision instability estimates (therefore map overall) many counties sparse data.four disease mapping objectives wish accomplish fully describe data:Test whether statistical evidence spatial heterogeneity versus homogeneityDescribe precision VLBW prevalence estimates countyAccount possibly spurious nuisance differences counties due confounding covariate maternal age structureProduce overall covariate-adjusted smoothed stabilized rate estimates using global Empirical Bayes.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"disease-mapping-is-there-spatial-heterogeneity","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.1 Disease mapping: Is there spatial heterogeneity?","text":"","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"calculating-expected-counts-and-the-smr","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.1.1 Calculating expected counts and the SMR","text":"now primarily represented disease burden using risks, rates, prevalence. However, introduce statistical estimation Poisson Poisson-gamma (negative binomial), often testing whether disease intensity (risk, rate, prevalence) given geographic area deviates expected value, expected value might considered average risk/rate/prevalence entire study region. natural way represent deviation using Standardized Morbidity Ratios (SMRs):\\(SMR_i=\\frac{Y_i}{E_i}\\)standardized morbidity ratio (also standardized mortality, incidence, prevalence depending counted) measure relative excess risk. quantifies deviation population parameter (case live birth prevalence low birthweight geographically-defined population) reference value (case VLBW risk whole state Georgia). SMR calculated Observed count events, \\(Y_i\\), divided Expected count, \\(E_i\\), events.Calculating expected counts VLBW events data straightforward: first calculate overall risk entire study region, \\(r\\), multiply population risk county, \\(N_i\\), get events expected homogeneity risk, \\(SMR=1.0\\) counties.can see maps , SMR represents underlying pattern, simply different scale, relative excess risk rather absolute risk.Spatial heterogeneityBoth maps illustrate (qualitatively least) spatial heterogeneity variation VLBW prevalence. absolute scale lets us see absolute burden varies one part state compared another. relative scale SMR specifically highlights counties one three types:Better average means lower risk average, statewide population. SMR < 1.0.Worse average means higher risk average, statewide population. SMR > 1.0.average meaning risk expected given statewide prevalence. SMR approximately equal 1.0.","code":"\n# the overall (statewide) ratio of events to population is the global risk\nrisk <- sum(vlbw$VLBW) / sum(vlbw$TOT) \n\n\n# Now add a variable to the dataset representing expected count and SMR\nvlbw <- vlbw %>%\n  mutate(expect = risk * TOT,  # calculate the EXPECTED count for each county\n         SMR = VLBW / expect)  # calculate the SMR as observed/expected"},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"testing-for-spatial-heterogeneity","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.1.2 Testing for spatial heterogeneity","text":"Perhaps fundamental purpose disease mapping describe represent magnitude patterns spatial heterogeneity variation health across sub-areas study region.isn’t real variation!? instance consider scenarios:numerical values disease intensity mapped practically regions, use arbitrarily narrow cutpoints make regions look different cartographically.appears large differences values sub-areas, counts sparse possible seeming difference simply due chanceFor reasons sensible start evaluating evidence versus heterogeneity. none, little reason proceed spatial analysis! Luckily standard statistical tests designed just purpose: evaluate whether count events significantly different across observations, accounting number trials persons risk.R package DCluster function chi-square test optimized needs aggregated data spatial epidemiology. test called achisq.test() can evaluate variation numerator denominator Poisson Negative Binomial (recall Poisson-gamma) distribution.sf data object containing VLBW information called vlbw; within sf object column named VLBW representing count babies born low birthweight county, another variable named TOT representing count live births; denominator population. language Poisson, \\(Y_i\\) count variable VLBW county, evaluate count offset log number births risk.Look help documentation function; specify statistical model Poisson. argument R=499 refers number random permutations use calculating empirical p-value.null hypothesis relative risk SMR equal one counties. words, null, significant difference risk counties. Based 499 simulated permutations null, observed data appear quite inconsistent null assumption, evidenced p.value = 0.002. words strictly Poisson probability model, appears significant spatial heterogeneity risk VLBW.R = 499 mean?conventional statistics often closed form formulas calculating standard errors, confidence intervals p-values. However, spatial statistics simple parametric assumptions always hold. One empirical alternative closed-form formula use random permutations data simulate random data null hypothesis.case achisq.test(), null hypothesis observed count equal expected count. Random permutations take random Poisson draw count county null. repeat hundreds times (specifically \\(499\\) times code ), distribution kind patterns might expect simply due random chance alone.compare actual observed values distribution truly random distribution looks like. observed values different set random values, might say evidence null. words, observed data unlikely occured due random chance alone.happen allowed distribution null Negative Binomial (e.g. Poisson-Gamma) rather strictly Poisson? specify re-calculate p-value testing evidence significant heterogeneity:assumption seems give us entirely different picture going ! always occur (e.g. many instances test heterogeneity either Poisson Negative Binomial result consistent determination statistical significance), also complete surprise. two points worth making comparison two results.First, understand possible might help visualize probability distributions fix minds ‘-dispersion’ ‘extra-Poisson variance’ mean. plot 10,000 draws two random distributions, Poisson Negative Binomial. , mean expectation null \\(10\\) events, indicated blue dotted line.left panel histogram many events occurred (assuming expected mean \\(n=10\\)) Negative Binomial, right panel shows distribution random draws Poisson.Negative Binomial distribution fatter, especially right-tail. means even null/expectation \\(n=10\\) true, expect wider range counts (including instances high counts) chance alone Negative Binomial compared Poisson.second point worth making early step – testing aspatial heterogeneity – just : first look. many reasons data behave variance excess Poisson expectation. -dispersion can arise important missing variables predict outcome event vary spatially. quite common.evidence spatial heterogeneity either distribution, might consider throwing towel now. However, given evidence unusual behavior Poisson expectation suggests exploration might worthwhile. However, clearly consider using Poisson-Gamma approach subsequent analyses including Empirical Bayesian smoothing, .","code":"\nDCluster::achisq.test(VLBW ~ offset(log(TOT)), \n                      data = vlbw, \n                      model = 'poisson',\n                      R = 499)## Chi-square test for overdispersion \n## \n##  Type of boots.: parametric \n##  Model used when sampling: Poisson \n##  Number of simulations: 499 \n##  Statistic:  416.6378 \n##  p-value :  0.002\nDCluster::achisq.test(VLBW~offset(log(TOT)), \n                      data = vlbw, \n                      model = 'negbin',\n                      R = 499)## Chi-square test for overdispersion \n## \n##  Type of boots.: parametric \n##  Model used when sampling: Negative Binomial \n##  Number of simulations: 499 \n##  Statistic:  416.6378 \n##  p-value :  0.794"},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"disease-mapping-how-precise-are-county-estimates","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.2 Disease mapping: How precise are county estimates?","text":"Following question whether global spatial heterogeneity (e.g. least counties \\(SMR\\neq 1.0\\)), natural follow question confident precise estimates , counties statistically significantly different null expectation?function estimate continuous p-value associated SMR probmap function package spdep. function calculates probabilities (Poisson probability model) observing event count extreme actually observed, given expected count (e.g. might expect every county overall risk). test one-tailed test, default alternative hypothesis observed less expected, SMR <1.0 (test extremes greater 1.0, set argument alternative = 'greater').frequentist statistics familiar focusing small p-values evidence reject null hypothesis. case continuous p-value returned probmap, can think probabilities either side spectrum.instance, default alternative = 'less', probability \\(p<0.05\\) indicate \\(SMR<1\\) statistically significant (\\(\\alpha=0.05\\)). contrast, \\(p>0.95\\) suggest unusual finding, null, \\(SMR>1\\). \\(p>0.95\\) alternative = 'less' therefore equivalence \\(p<0.05\\) alternative = 'greater' describing significance \\(SMR>1\\).probmap expects several arguments including vector count cases, vector population risk, optionally row.names vector help align observations. objective identify counties SMR excess expected (e.g. >>1), easier interpret change alternative hypothesis one-sided test alternative = 'greater'.function returns expected count VLBW births (yet another way get number!), well SMR (case named relRisk, somewhat oddly function multiplies SMR 100 numbers appear different!), Poisson probability observed count ‘extreme’ actually observed.can see, function calculates:Raw rate, simply \\(\\frac{Y_i}{N_i}\\)Expected count, simply \\(r\\times N_i\\), \\(r\\) overall expected rate based VLBW births Georgia datasetRelative risk, SMR ratio observed expected. Note function multiplies SMR 100. value 103 actually refers SMR 1.03p-value, probability SMR county significantly greater 1.0For mapping, grab SMR (e.g. relRisk divided 100 make conventional) p-value term, pmap, can easily add sf object:","code":"\nlibrary(spdep)\nx <- probmap(n = vlbw$VLBW, x = vlbw$TOT, \n              row.names = vlbw$GEOID,\n             alternative = 'greater')\nhead(x) # look at what is returned##              raw   expCount   relRisk       pmap\n## 13121 0.01885984 424.339856 103.69047 0.22974886\n## 13029 0.01214953  19.461794  66.79754 0.95041991\n## 13135 0.01791989 414.117867  98.52267 0.62470508\n## 13127 0.02416255  33.121426 132.84452 0.04028446\n## 13271 0.01204819   4.528959  66.24038 0.82965314\n## 13279 0.01364256  13.332238  75.00616 0.85502966\nvlbw$pmap <- x$pmap\n\nvlbw$SMR <- x$relRisk / 100"},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"mapping-the-p-value-for-the-smr","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.2.1 Mapping the p-value for the SMR","text":"produce p-value map depicting continuous probability observe SMR extreme observed (specifically case, greater observed), assuming null described expected count true, use probability retrieved previous code map, next map SMR :interesting, perhaps useful quantify probabilities familiar thresholds. example use output probmap() function calculate custom p-value categories.preceding code designed make new spatial object (e.g. create object pv original county object vlbw) whose purpose represent county boundaries shapes counties SMR statistically significantly greater 1. use new object additional layer map outline highlight statistically significant counties. notes code :REMEMBER: probmap carried 1-sided test, make align results confidence intervals identified counties extreme values either direction, implicitly two-sided, look counties p > 0.975. Thus mutated variable pmap.pv equal 1 county SMR>1 pmap value less 0.05. Otherwise pmap.pv equal zero.using group_by(pmap.pv) along summarise(), first separate counties significant (remember pmap.pv binary 1/0), merge dissolves adjacent counties category (e.g. significant significant). result spatial object general borders around sets significant counties rather around county separately.Finally, using filter(pmap.pv == 1) code removes counties significant. result object boundaries counties statistically significantly higher risk expected. can plot object map.","code":"\nsmr_map <- tm_shape(vlbw) +\n  tm_fill('SMR',\n          style = 'fixed', \n          palette = '-RdYlBu',\n          breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'VLBW in Georgia, 2018-2019',\n            inner.margins = c(0.02, 0.02,0.05,0.2))+\n  tm_shape(ga) +\n  tm_borders(lwd = 2, col = 'black')\n\nprob <- tm_shape(vlbw) + \n  tm_fill('pmap',\n          style = 'cont',\n          palette = 'PiYG',\n          n=7,\n          title = 'Prob SMR > 1\\nby chance alone') + \n  tm_borders() + \n  tm_layout(main.title = 'Probability Map',\n            inner.margins = c(0.02, 0.02,0.05,0.2))+\n  tm_shape(ga) +\n  tm_borders(lwd = 2, col = 'black')\n\ntmap_arrange(smr_map, prob)\npv <- vlbw %>%\n  mutate(pmap.pv = ifelse(SMR > 1 & pmap < 0.05, 1, 0)) %>%\n  group_by(pmap.pv) %>%\n  summarise() %>%\n  filter(pmap.pv == 1)\ntm_shape(vlbw) +\n  tm_fill('SMR',\n          style = 'fixed',\n          palette = '-RdYlBu',\n          breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'SMR of VLBW, GA 2018-2019',\n            inner.margins = c(0.1, 0.02,0.05,0.2)) +\n  # Add dark borders for significant\n  tm_shape(pv) +\n  tm_borders(lwd = 2, col = 'black') +\n  #tm_shape(ga) + \n  tm_borders(lwd = 1.5, col = 'black') +\n  tm_credits('Counties with higher than expected risk (p<0.05) highlighted with dark borders')+\n  tm_shape(ga) +\n  tm_borders(lwd = 1, col = 'black')"},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"disease-mapping-adjusting-for-covariates","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.3 Disease mapping: Adjusting for covariates","text":"SMR straightforward overall, marginal total, also possible calculate SMR adjusts covariate, maternal age, using indirect standardization. means apply reference rate within strata (e.g. age case) population--risk within county-age strata.may recall earlier classes (perhaps EPI 530) learned direct indirect age-standardization (familiar direct indirect standardization, helpful review old Epi Methods course notes refresher!).standardization may mentioned much since , tool adjust confounding, just might stratification \\(2\\times 2\\) tables, multivariable regression. way adjust individual-level covariates spatial analysis, common approach 1 perhaps 2 categorical covariates.Calculating expected count indirect standardization categorical variable (e.g. maternal age) requires data arranged row data within county representing count deaths age-strata. hand-calculate standardized expected counts, convenience function calculating expected counts using covariate strata may find easier.convenience function part SpatialEpi package. expected() function expects 3 arguments:vector count population risk, including row every covariate strata within every region;vector count number events cases (separately strata covariate region);number strata within region (e.g. many age covariate categories possible within county?)age-adjust vlbw data, need different object including counts county, age-category within county. Luckily can retrieve Georgia OASIS. object age structure vlbw except instead \\(159\\) rows \\(159\\) counties \\(1431\\) rows \\(159\\times 9\\) age categories. Said another way, data long.expected() function take covariate-stratified counts, calculate single expected count region (e.g. “many VLBW births expect maternal age structure mothers county \\(x\\) statewide maternal age structure?”). can used produce age-adjusted SMR’s. Notice output following function vlbw object, N=159 rows data, despite inputs (e.g. information right assignment arrow) age object, \\(1431\\) rows.might wonder whether age-adjustment impact. can see plot , showing unadjusted SMR versus age-adjusted, case indirect adjustment age created extreme outliers. may result stratifying already-sparse events even smaller cells, producing instability estimates. revisit global Empirical Bayes smoothing rate stabilization.","code":"##   GEOID    NAME  AGECAT VLBW TOT\n## 1 13001 Appling 10 - 14    0   1\n## 2 13001 Appling 15 - 17    2  12\n## 3 13001 Appling 18 - 19    0  30\n## 4 13001 Appling 20 - 24    4 149\n## 5 13001 Appling 25 - 29    4 141\n## 6 13001 Appling 30 - 34    0  72\nlibrary(SpatialEpi)\n# First, must insure that data are sorted by county and covariate category\nage <- age %>%\n  arrange(GEOID, AGECAT)\n\n# Calculate the age-adjusted expected counts\nvlbw$expected_indirect <- SpatialEpi::expected(population = age$TOT, \n                                          cases = age$VLBW,\n                                          n.strata = 9)\n# Remember, if you added 0.5 to observed above, do so here as well!\n\nvlbw$SMR_adj <- vlbw$VLBW / vlbw$expected_indirect"},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"disease-mapping-rate-stabilization-with-global-empirical-bayes","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.4 Disease mapping: Rate stabilization with global Empirical Bayes","text":"Everything covered focused representing precision/stability/certainty SMR observed data, possibly adjusted covariates. However, case VLBW (many small-area mapping projects), may want try extract signal underlying spatial trend data, net random noise induced small event counts widely varying population sizes. process sometimes referred small area estimation goes beyond just showing observed values, instead trying estimate underlying true trend.Empirical Bayes (EB) estimation one technique producing robust small area parameter estimates. EB estimation approach parameter shrinkage, wherein extreme estimates (e.g. SMR) judged reliable -reliable based variance, function number events. words county extreme SMR, small event count, SMR parameter less reliable. absence information, might guess extreme small event count try adjust, shrink, back range reasonable values.hand county relatively extreme SMR, many events, extreme value might deemed reliable. result, shrunk less. EB estimation just : uses overall average rate (SMR) global reference shrinks, adjusts, SMR towards global mean, inversely proportionate variance. ideal result true patterns persist, noise eliminated.DISCLAIMER: need understand Bayesian statistical theory work effectively EB estimators class. However provide superficial discussion happening want . less interested, focus code producing estimates credible intervals. really interested, likely superficial intro unsatisfying. can point resources desired!","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"a-bit-about-bayes","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.4.1 A bit about Bayes…","text":"may learned Bayes Theorem statistics, may gone much . Bayesian statistics take slightly different perspective analysis inference compared frequentist statistics underlying conventionally use.\nFIGURE 4.1: Bayes Theorem\nBayes theorem familiar likelihood component, essentially estimate observed data. likelihood piece inference based frequentists. Bayesians, theorem posits prior belief combined likelihood provides new updated posterior belief.fully Bayesian analysis, prior actually probability distribution , Empirical Bayes, prior derived observed data. Often prior expectation overall rate (either globally today, locally next week). Therefore, combine prior expectation observed data, can produce statement updated belief large small SMR . posterior typically just single number, fully distribution, can also say something precision certainty estimate area.","code":""},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"poisson-gamma-mixture-model","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.4.2 Poisson-Gamma mixture model","text":"Recall assumption Poisson distribution mean variance . uncommon real dataset roughly Poisson-distributed, perhaps processes (e.g. unmeasured predictors outcome) may extra-Poisson dispersion (e.g. mean >> variance).excess variation called -dispersion. problem leads biased statistical testing. may also learned alternative Poisson distribution Negative Binomial distribution, also works count data, extra dispersion parameter. However instead using Negative binomial directly, look Poisson-Gamma mixture model, achieves similar ends, natural fit Bayesian framework common many disease mapping applications.Poisson-Gamma mixture model pairing two parametric distributions better account squirrely data possible extra-Poisson variance. specifically gamma distribution serves prior Poisson mean parameter, \\(\\theta\\). words describes variable deviations Poisson mean can .package SpatialEpi, function called eBayes() estimates Empirical Bayes smoothed estimates disease burden (specifically relative excess risk SMR), based Poisson-Gamma mixture.First, let’s estimate EB-smoothed relative risk. function expects argument, Y, vector event counts, argument, E, expected count. Note also option include covariate matrix, wanted estimate covariate-adjusted EB-smoothed rates.Notice object global_eb1 returned function eBayes() actually list 5 elements. includes SMR (based observed data, smoothed!), well RR (mean estimate smoothed relative risk), RRmed (median estimate smoothed relative risk, case nearly identical mean). Notice also estimates \\(\\beta\\) (beta) \\(\\alpha\\) (alpha) parameters Gamma prior estimated data.can now add smoothed stabilized estimates dataset map raw unsmoothed SMR compared Empirical Bayes smoothed SMR…map symbolized using independent quantile categorization. result, notice two things map comparison :general patterns highs lows quite similar, although identicalThe cutpoints legend relatively different.Looking little closely differences illustrated plot can observe several things Empirical Bayes smoothing relation population size degree parameter shrinkage towards mean:counties largest sample size (larger dots plot) fall along diagonal observed smoothed rates similarConversely, counties likely ‘fanned ’ diagonal (indicating different value observed versus smoothed) smallest number events (e.g. small dots)Similarly bluer dots (least shrinkage) also larger less extreme valueThe redder dots (shrinkage) tended smaller.","code":"\nglobal_eb1 <- eBayes(vlbw$VLBW, vlbw$expect)\n# names(global_eb1)  # look at the object returned\n\nnames(global_eb1)## [1] \"RR\"    \"RRmed\" \"beta\"  \"alpha\" \"SMR\"\n# this adds the smoothed relative risk (same as SMR) to the vlbw dataset\nvlbw$ebSMR <- global_eb1$RR\nsmr_map <- tm_shape(vlbw) +\n  tm_fill('SMR',\n          style = 'quantile', palette = '-RdYlBu',\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'Raw SMR of VLBW',\n            inner.margins = c(0.02, 0.02, 0.1, 0.05),\n            legend.format = list(digits = 2))+\n  tm_shape(ga) +\n  tm_borders(lwd = 2, col = 'black')\n\neb_map <- tm_shape(vlbw) +\n  tm_fill('ebSMR',\n          style = 'quantile', \n          palette = '-RdYlBu',\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'EB smoothed SMR of VLBW',\n            inner.margins = c(0.02, 0.02, 0.1, 0.05),\n            legend.format = list(digits = 2))+\n  tm_shape(ga) +\n  tm_borders(lwd = 2, col = 'black')\n\ntmap_arrange(smr_map, eb_map)"},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"estimating-bayesian-exceedance-probabilities-from-poisson-gamma-eb-estimates","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.4.3 Estimating Bayesian exceedance probabilities from Poisson-Gamma EB estimates","text":"estimating parameters Gamma-prior Poisson parameter, \\(\\theta\\), can describe point estimates actually can describe entire posterior distribution estimated smooth rate. posterior distribution way saying Bayesian statistics just one answer, instead probabilistic range answers. Whereas frequentist statistics talk confidence intervals, Bayesian statistics roughly corresponding idea called credible interval, essentially specific thresholds posterior.interpretation credible intervals identical confidence intervals, close enough now. necessary disease mapping, might help illustration visualize posterior estimate two counties. One Dekalb county, large population, Baker county small population. can see, SMR (based observed data) quite different, mean posterior estimate EB-smoothed RR nearly identical. can also see precision certainty , much wider (greater) uncertainty Stewart County compared Dekalb county.describe likely unlikely EB-smoothed relative risk given county different null value 1, can use Bayesian exceedance probabilities. sound similar p-values mapped probmap() function, interpretation different Bayesian framework. Specifically, instead somewhat convoluted way interpret p-values (e.g. “probability observe counts extreme infinite repeated samples, assuming null true”), Bayesian exceedance probabilities straightforward. Specifically simply , “probability true parameter, \\(\\theta\\) greater 1.0, given prior observed data”.function called EBposththreshold() calculation, requires several arguments including conventional observed expected counts, also two parameters ,alpha beta estimated previous step. also need specify threshold beyond interested making inference. relative risk typically 1.0, wanted ask probability exceeding different value (e.g. “probability RR greater 2?”), option.necessary disease mapping, might interested different Bayesian frequentist approach . plot shows dataset.two things note plot comparing two estimates certainty precision:First, apparent inversely related. words frequentist p-value increases, predictive probability Bayesian model gets smaller. simply evaluating inverse parts question. frequentist p-value evaluating probability observing data extreme null true (e.g. small p-values lend support rejection null). contrast Bayesian exceedance probability reporting probability county RR greater 1.0. , higher probability consistent true extremes, rather spurious ones.Second, largely consistent, albeit identical one another. words track along diagonal suggesting county given p-value corresponding proportionate partner exceedance probability. differences reflect smoothing stabilization due EB methods.","code":"\nvlbw$eb2_prob <- EBpostthresh(Y = vlbw$VLBW, \n                              E = vlbw$expect, \n                              alpha = global_eb1$alpha, \n                              beta = global_eb1$beta, \n                              rrthresh = 1)"},{"path":"disease-mapping-i-aspatial-empirical-bayes.html","id":"mapping-poisson-gamma-eb-estimates-and-exceedance","chapter":"Week 4 Disease Mapping I: Aspatial Empirical Bayes","heading":"4.3.4.4 Mapping Poisson-Gamma EB estimates and exceedance","text":"Finally, map smoothed estimates indication high probability different Georgia average rate (e.g. probability exceeding SMR 1.0 95%).Comparing two maps see fewer significant counties using Empirical Bayes approach. surprising, consistent goal trying separate signal random noise. suggest least counties appearing significantly different global rate, fact plausibly outliers small amounts information stably precisely estimated.","code":"\n# Identify counties with p-value < 0.05\npv <- vlbw %>%\n  mutate(pmap.pv = ifelse(SMR > 1 & pmap < 0.05, 1, 0)) %>%\n  group_by(pmap.pv) %>%\n  summarise() %>%\n  filter(pmap.pv == 1)\n\n\nm3<- tm_shape(vlbw) +\n  tm_fill('SMR',\n          style = 'quantile',\n          palette = '-RdYlBu',\n          #breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'SMR of VLBW,\\nGA 2018-2019',\n            inner.margins = c(0.1, 0.02,0.05,0.2),\n            legend.format = list(digits = 2)) +\n  # Add dark borders for significant\n  tm_shape(pv) +\n  tm_borders(lwd = 2, col = 'black') +\n  #tm_shape(ga) + \n  tm_borders(lwd = 1.5, col = 'black') +\n  tm_credits('Counties with higher than expected risk (p<0.05) highlighted with dark borders')+\n  tm_shape(ga) +\n  tm_borders(lwd = 1.5, col = 'black')\n\n# Identify counties with EB exceedance probability > 0.95 (corresponds to p<0.05)\npv2 <- vlbw %>%\n  mutate(eb.pv = ifelse(ebSMR > 1 & eb2_prob > 0.95, 1, 0)) %>%\n  group_by(eb.pv) %>%\n  summarise() %>%\n  filter(eb.pv == 1)\n\nm4 <- tm_shape(vlbw) +\n  tm_fill('ebSMR',\n          style = 'quantile',\n          palette = '-RdYlBu',\n          #breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Std. Morbidity Ratio') + \n  tm_borders() +\n  tm_layout(main.title = 'Empirical Bayes smoothed\\nSMR of VLBW',\n            inner.margins = c(0.1, 0.02,0.05,0.2),\n            legend.format = list(digits = 2)) +\n  # Add dark borders for significant\n  tm_shape(pv2) +\n  tm_borders(lwd = 2, col = 'black') +\n  tm_credits('Counties with higher than expected risk (p<0.05) highlighted with dark borders')+\n  tm_shape(ga) +\n  tm_borders(lwd = 1.5, col = 'black')\n\ntmap_arrange(m3, m4)"},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"disease-mapping-ii-spatial-empirical-bayes","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"Week 5 Disease Mapping II: Spatial Empirical Bayes","text":"","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"getting-ready-3","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.1 Getting Ready","text":"","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"learning-objectives-4","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"additional-resources-4","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.1.2 Additional Resources","text":"Anselin, L. Spatial Regression Analysis R: workbook. 2007.GeoDa Center Resources: Section distance-based spatial weightsGeoDa Center Resources: Section contiguity-based spatial weights","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"important-vocabulary-4","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 5","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"spatial-thinking-in-epidemiology-3","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.2 Spatial Thinking in Epidemiology","text":"first time formally incorporate make explicit spatial means spatial analysis. Although work now represented map (thus spatially contextualized), formally incorporate spatial relationships aspect analysis. Specifically, last week calculated statistical tests heterogeneity, estimated precision statistical significance, produced Empirical Bayes smoothed (stabilized) estimates parameters interest.tasks treated spatial unit spatially geographically independent every spatial unit. assumption units geographically independent referred aspatial analysis.","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"an-argument-for-the-relevance-of-space","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.2.1 An argument for the relevance of space","text":"formally explicitly incorporate spatial relatedness need clear constitutes spatial relationships. two aspects considering spatial relatedness, apply two sides spatial thinking health. discussed recorded lecture, fundamental dimension spatial relations geography distance (relatedly idea proximity), whether euclidean (e.g. crow flies) distance, social distance, network distance.one hand, distance used metric defining aspect local population homogeneity distinct broader regional (e.g. study region-wide) heterogeneity. words, based Tobler’s First Law Geography, near things tend alike (e.g. correlated although necessarily causally linked) distant things (average), implying kind dependence correlation among local units might evident overall.concept – seems hold true many human non-human systems – means faced sparse data, concern uncertainty, can ‘borrow’ statistical information spatial neighbors supplement estimation local disease parameters. exactly spatial Empirical Bayes estimation, instead using overall (global) rate disease prior, use local rate neighbors surrounding entity kind custom, place-specific prior.deeper level, distance also important spatial thinking epidemiology. hypothesize – interested – whether entities geographically socially connected share health-relevant experiences. experiences exposures include microbial space (e.g. person--person transmission infectious agents), social norms (e.g. acceptability smoking body image perceptions), built environments (e.g. lead exposure municipal water systems, food environments), access health resources (e.g. health care, cancer screening), access opportunity structures (e.g. good schools, safe streets, employment opportunities).Distance Cartesian!examples emphasize role distance Cartesian (geographic) space. However, worth emphasizing complex versions distance proximity come play.example, air travel makes linear Cartesian distance two places less relevant economic social drivers flows people back forth comes infectious disease transmission Zika Ebola. still distance dimension, defined push pull human mobility migration.possible define spatial neighbors abstract (e.g. non-geographic) ways. example, political scientists created spatial weights matrices connect states geographic boundaries, similarly legislatures act policy decisions. way distance measure ideology rather geography, still meaning spatial analysis health. today focus specific example geographic space rather social, political, economic space.sum, notion explicitly spatial analysis way incorporate theoretical conceptual aspects humans relate one another environment understanding distribution determinants disease.Whether treat spatial dependence relatedness primarily statistical feature exploitation (e.g. spatial disease mapping Empirical Bayes), attribute local ecosystem disease generation, clear neighbors defined influential final numerical results inference take . definition spatial neighbors, corresponding symbolization relatedness creation spatial weights matrices fundamental bridge theory geography meaning spatial epidemiology.","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"on-making-meaning-from-neighbors","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.2.2 On making meaning from neighbors","text":"discussed , primary means spatial epidemiologists can make space explicit incorporating information places geographic units related one another; words distance (proximity, connectivity, contiguity, etc) single unit units begins put pieces puzzle together whole. often done creation numerical weight quantifies distance local unit neighbors (presume proximate, least degree).challenge spatial epidemiologist twofold:Conceptualizing spatial scale extent health-relevant process interest occursTranslating conceptual idea explicit definition neighbors, therefore spatial weightsFor areal analysis (e.g. spatial analysis polygons), two broad classes neighborhood definitions: contiguity-based definitions distance-based definitions. reality abstract expressions ‘distance’, differ ‘near’ ‘far’ operationalized.primarily focused analysis areal units course, possible create neighbor definitions among point-referenced data using tessalation process creation Thiessen polygons (e.g. see discussion Contiguity-based weights points).table explored explained detail online lectures, lab activity. key takeaway point can define units near units using different definitions, definitions slightly different assumptions results.brief summary several common neighbor definitions:choice neighbor definition use influenced several study-specific factors, can conflict others:Variation size areal units across study area. areal units small (e.g. counties Eastern U.S.) large (e.g. counties Western U.S.), geographic area defined adjacent counties quite different (e.g. think long takes drive across two counties like Dekalb Fulton Atlanta, versus long might take traverse two counties Nevada Utah). contrast, fixed-distance neighbors consistent among linear distance index units neighbors.Assumptions requirements statistical analysis interest. algorithms require/expect features neighbor symmetry spatial weights row standardization account unequal numbers neighbors.assumed meaning space analysis. possible , instance, meaning distance Western counties different travel basic services norm denser areas East.purpose audience map. important make analysis accessible interpretable target audience.Aspects geography including islands presence non-contiguous units (e.g. Hawaii, Alaska, Puerto Rico)","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"spatial-analysis-in-epidemiology-3","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3 Spatial Analysis in Epidemiology","text":"apply concepts specific spatial analysis, continue use Georgia low birthweight dataset used previous module eBook. reminder, county-level dataset \\(n=159\\) Georgia Counties containing count live births (denominator) well count VLBW births (weight birth < 1500 grams) babies 2018-2019. data derived Georgia OASIS website.section first introduce create examine several different spatial neighbor definitions. never create neighbors just sake. purpose creating spatial neighbors weights matrices always use definition spatial analysis. Later section see use spatial weights producing spatial Empirical Bayes estimates.","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"creating-contiguity-neighbor-objects","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.1 Creating contiguity neighbor objects","text":"R, spdep package series functions useful creating spatial weights matrices. general, process going spatial object (e.g. sf class data object) usable spatial weights matrix requires one step, steps vary depending eventual use.Since starting areal (polygon) data, starting point use utility function, poly2nb(), take polygon spatial object (class sf sp) determine specific polygon regions contiguous (touch, share boundaries ) regions. review help documentation, see function takes spatial sf object input, arguments specifying whether use Queen contiguity (default; Rook alternative). function returns something called neighbor list.summary() function objects class nb (neighbor object created spdep) provides useful high-level info, including presence regions zero links (neighbors – problem occur islands, example), distribution number links neighbors.might want look structure queen_nb object also, either using str(queen_nb), perhaps just viewing first elements list (e.g. nb objects class list R, use double-bracket indexing lists like queen_nb[[1]] view neighbors first region).neighbor object essentially list length equal number regions (\\(n=159\\) counties case). elements list correspond order input dataset, first list item first county current sort order. element list vector identifying counties neighbors .One important attribute spatial relationships whether symmetric . context spatial neighbors, spatial symmetry implies \\(region_i\\) neighbor \\(region_j\\), \\(region_j\\) also neighbor \\(region_i\\). Contiguity neighbors symmetric design: definition neighbor shared boundaries, true either partner relationship. see definitions spatial relationships neighbors result symmetric relationships. quick way check whether neighbor object symmetric code:better understand set spatial relationships, can useful visualize neighbor links connections choosing among neighbor definition, simply see relative density pattern connectivity. Note function plot.nb() nb object first argument, must also include matrix centroids second argument. reason nb object defines region connects , say space. centroids tell plot link line begins ends.Notice density neighbors generally lower coast state boundaries. systematic difference neighbors can produce patterns sometimes referred edge effects. edge effects source bias, counties interior state neighbors (thus ‘local information’ average) border counties. especially true absence neighbors artificial case counties bordering Alabama, Tennessee, North South Carolina, Florida. contrast, counties coast ‘real’ absence neighbors.","code":"\n# load the package spdep\nlibrary(spdep)\n\n# Create a queen contiguity neighbor list\nqueen_nb <- poly2nb(vlbw, queen = TRUE)\n\n# Examine the resulting object\nsummary(queen_nb)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 860 \n## Percentage nonzero weights: 3.401764 \n## Average number of links: 5.408805 \n## Link number distribution:\n## \n##  1  2  3  4  5  6  7  8  9 10 \n##  1  4 12 29 36 37 28  9  1  2 \n## 1 least connected region:\n## 64 with 1 link\n## 2 most connected regions:\n## 1 66 with 10 links\nis.symmetric.nb(queen_nb)## [1] TRUE\n# Create a matrix of the x,y coordinates for each county centroid\nga_cent <- st_centroid(st_geometry(vlbw))\n\n# Plot the outlines of counties with light grey boundaries\nplot(st_geometry(vlbw), border = 'grey')\n\n# Add the plot of the Queen contiguity connections\nplot.nb(queen_nb, ga_cent, points = F, add = T)"},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"creating-k-nearest-neighbors","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.2 Creating k-nearest neighbors","text":"K-nearest neighbors flexible approach assuring balanced number neighbors, can help size density spatial regions varies across study area. instance fixed-distance buffer (e.g. perhaps counties within 50 miles) might work identify relevant neighbors Eastern Midwestern U.S., West, county may 100-200 miles across, zero neighbors definition. K-nearest neighbors, smaller Eastern larger Western counties neighbors (albeit differing spatial scales).k-nearest neighbors depend either arbitrary fixed distance, contiguity, always produce neighbors even islands. example analyses U.S. states, Alaska Hawaii contiguous neighbors. However, k-nearest neighbor approach still assign nearest neighbor regardless far away. instance nearest neighbor Hawaii might California. question must ask whether meaningful say Hawaii California neighbors. interested food environment, seems implausible. However, great deal social, cultural, economic interaction Hawaii California; instances plausible meaningful connection.create k-nearest neighbor object, first must identify relative proximity candidate neighbors. define nearest , convention measure Euclidean distance centroids polygons (literally geometric center), assumption average location describe polygon. requires two steps.First, knearneigh() function takes centroids, calculates pair-wise distances, sorts closest furthest, selects \\(k\\) nearest (smallest distance) units. knn2nb() function takes information creates formal nb neighbor object.Notice summaries interesting force everyone number links! However checking symmetry, important concern rises:K-nearest neighbors almost always produce asymmetric neighbors. Thinking U.S. states perhaps easy way understand . Consider state Hawaii: nearest states probably California, Oregon, Washington. However inverse true. nearest 2 (3 4 5) states California contiguous ‘lower 48’ states; Hawaii certainly among nearest places California.asymmetry problem spatial analytic tasks including spatial Empirical Bayes smoothing week. However, cluster analysis analyses future weeks, neighbor symmetry assumed required. choose k-nearest neighbor definition, also require symmetric spatial relationships, can force symmetry least two ways.First, specify sym  = TRUE knn2nb() call . essentially breaks rigid k-nearest neighbors forces reciprocity ‘neighborliness’. second method appropriate already created asymmetric neighbors, wish retrospectively force symmetry: make.sym.nb(). simply takes asymmetric neighbor object adds links make relationships symmetric. Note, however, alters number links neighbors region: now others.Note now four counties 8 links, rather 5. means counties nearest least 3 others, even though 3 nearest .","code":"\n# First create two sets of neighbors: 2 nearest and 5 nearest\nknn2 <- knearneigh(ga_cent, k = 2)\nknn5 <- knearneigh(ga_cent, k = 5)\n\n# Now take those lists of neighbors and make an nb object\nknn2_nb <- knn2nb(knn2, row.names = vlbw$GEOID)\nknn5_nb <- knn2nb(knn5, row.names = vlbw$GEOID)\n\nsummary(knn5_nb)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 795 \n## Percentage nonzero weights: 3.144654 \n## Average number of links: 5 \n## Non-symmetric neighbours list\n## Link number distribution:\n## \n##   5 \n## 159 \n## 159 least connected regions:\n## 13121 13029 13135 13127 13271 13279 13301 13007 13143 13221 13137 13289 13105 13051 13073 13189 13103 13319 13209 13317 13241 13033 13261 13249 13309 13113 13123 13157 13215 13311 13265 13019 13291 13171 13263 13001 13303 13027 13305 13133 13251 13163 13195 13013 13153 13205 13025 13009 13021 13217 13213 13151 13185 13181 13313 13183 13031 13245 13141 13191 13049 13079 13283 13083 13139 13107 13179 13229 13075 13267 13039 13077 13219 13315 13285 13095 13115 13225 13045 13035 13161 13097 13071 13237 13081 13011 13109 13017 13255 13197 13003 13015 13275 13211 13235 13131 13065 13293 13287 13155 13227 13173 13223 13277 13145 13297 13129 13295 13055 13165 13243 13047 13233 13187 13117 13111 13063 13067 13207 13101 13167 13193 13239 13149 13069 13125 13085 13091 13201 13061 13321 13169 13281 13299 13037 13053 13307 13273 13159 13023 13093 13177 13257 13175 13269 13059 13089 13043 13147 13057 13231 13253 13087 13005 13119 13247 13099 13199 13259 with 5 links\n## 159 most connected regions:\n## 13121 13029 13135 13127 13271 13279 13301 13007 13143 13221 13137 13289 13105 13051 13073 13189 13103 13319 13209 13317 13241 13033 13261 13249 13309 13113 13123 13157 13215 13311 13265 13019 13291 13171 13263 13001 13303 13027 13305 13133 13251 13163 13195 13013 13153 13205 13025 13009 13021 13217 13213 13151 13185 13181 13313 13183 13031 13245 13141 13191 13049 13079 13283 13083 13139 13107 13179 13229 13075 13267 13039 13077 13219 13315 13285 13095 13115 13225 13045 13035 13161 13097 13071 13237 13081 13011 13109 13017 13255 13197 13003 13015 13275 13211 13235 13131 13065 13293 13287 13155 13227 13173 13223 13277 13145 13297 13129 13295 13055 13165 13243 13047 13233 13187 13117 13111 13063 13067 13207 13101 13167 13193 13239 13149 13069 13125 13085 13091 13201 13061 13321 13169 13281 13299 13037 13053 13307 13273 13159 13023 13093 13177 13257 13175 13269 13059 13089 13043 13147 13057 13231 13253 13087 13005 13119 13247 13099 13199 13259 with 5 links\nis.symmetric.nb(knn2_nb)## [1] FALSE\nis.symmetric.nb(knn5_nb)## [1] FALSE\nknn5_symmetric <- make.sym.nb(knn5_nb)\nsummary(knn5_symmetric)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 910 \n## Percentage nonzero weights: 3.599541 \n## Average number of links: 5.72327 \n## Link number distribution:\n## \n##  5  6  7  8 \n## 73 61 21  4 \n## 73 least connected regions:\n## 13121 13135 13127 13289 13105 13051 13103 13319 13317 13241 13033 13249 13113 13157 13215 13171 13027 13133 13251 13013 13009 13021 13181 13031 13245 13049 13079 13083 13107 13179 13267 13039 13219 13315 13285 13095 13225 13081 13017 13275 13235 13155 13145 13295 13055 13165 13243 13047 13233 13111 13207 13101 13167 13239 13149 13091 13201 13061 13281 13053 13159 13093 13177 13257 13175 13059 13147 13057 13231 13253 13087 13005 13099 with 5 links\n## 4 most connected regions:\n## 13007 13223 13129 13187 with 8 links\nis.symmetric.nb(knn5_symmetric)## [1] TRUE"},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"visualizing-differences-between-competing-neighbor-definitions","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.2.1 Visualizing differences between competing neighbor definitions","text":"spatial analyst, might interested choice neighbors affects results. better understand different one definition next can helpful visualize side--side. visualization likely interest broader audience. words probably publish map. Instead production helps , spatial epidemiologist, better understand options make informed decisions.Using just base-R plotting (create fancier maps desired ggplot tmap), can easily visualize county polygons, lines connecting centroids neighboring counties indication shared influence, contact, interaction.function plot.nb() requires spatial neighbor object (e.g. object class nb), matrix \\(x, y\\) locations polygon centroids, work drawing connecting lines.function code named diffnb() simply utility function compare two nb objects determine different. can plot different values red order quickly see differs one neighbor definition next.surprise lots red lines knn5 compared knn2. Every single county 3 additional neighbors former compared latter. However interesting see many initially asymmetric relationships added links order enforce symmetry (e.g. red lines right-hand plot, compared middle).","code":"\npar(mfrow = c(1, 3),        # set plotting space for 2 side-by-side plots\n    mar = c(.2,.2,1,.2))    # Set margins for plotting\n\n# Plot the knn = 2 neighbor connections\nplot(st_geometry(vlbw), border = 'grey', main = 'knn = 2')\nplot.nb(knn2_nb, ga_cent, point = F, add = T)\n\n# Plot the knn = 5 neighbor connections\nplot(st_geometry(vlbw), border = 'grey', main = 'knn = 5')\nplot.nb(knn5_nb, ga_cent, point = F, add = T, col = 'blue')\nplot.nb(diffnb(knn2_nb, knn5_nb), ga_cent, point = F, add = T, col = 'red')\n\n# Plot the knn = 5 AND the differences (in red) when knn = 5 is made symmetric\nplot(st_geometry(vlbw), border = 'grey', main = 'Symmetric Knn5')\nplot.nb(knn5_nb, ga_cent, point = F, add = T, col = 'blue')\nplot.nb(diffnb(knn5_nb, knn5_symmetric), ga_cent, point = F, add = T, col = 'red')\npar(mfrow = c(1,1))"},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"creating-graph-based-triangle-neighbor-objects","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.3 Creating Graph-based triangle neighbor objects","text":"contiguity framework takes reasonable approach local implies direct interaction indicated shared borders. However many instances, odd shape polygons means regions quite close one another share border. different approach – one two methods ’ll discuss called graph-based neighbors – defines local neighbors relative proximity using geometry approach.process subdivides space non-overlapping triangles, using centroids region vertices triangle. neighbor therefore region connected edge (link) two vertices (centroids). practically, results neighbor region near(ish) required touching-borders. Graph-based neighbor objects symmetric design.Look back summary queen_nb object created previously. graph-based neighbor definition results slightly connections every county compared Queen contiguity, also reduces variation number links. Queen counties many 10 links 1 link; contrast graph-based definition results counties ranging minimum 3 neighbors maximum 8.visualize triangularized neighbors can plot links, next Queen contiguity compare.Notice graph-based neighbors strange connections along Western border Georgia. Delauney triangle algorithm makes unexpected connections centroids along edges. thought unreasonable spatial relationships (think !), can prune using Sphere Influence graph restrict proximate relationships. instances, carrying pruning produce sphere influence graph neighbors sensible compared using product stage.code little intimidating looking: includes nested functions original triangle neighbor object fed soi.graph() function, fed graph2nb() function. basically looking ties connections defined triangularization algorithm also proximate.see Queen neighbors compares can plot :","code":"\ntri_nb <- tri2nb(ga_cent)\nsummary(tri_nb)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 918 \n## Percentage nonzero weights: 3.631185 \n## Average number of links: 5.773585 \n## Link number distribution:\n## \n##  3  4  5  6  7  8 \n##  1 18 40 63 31  6 \n## 1 least connected region:\n## 15 with 3 links\n## 6 most connected regions:\n## 58 65 82 107 118 120 with 8 links\nis.symmetric.nb(tri_nb)## [1] TRUE\nsoi_nb <- graph2nb(soi.graph(tri_nb, ga_cent))\n\nsummary(soi_nb)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 850 \n## Percentage nonzero weights: 3.362209 \n## Average number of links: 5.345912 \n## Link number distribution:\n## \n##  3  4  5  6  7  8 \n## 11 35 32 53 25  3 \n## 11 least connected regions:\n## 14 15 21 64 71 87 96 120 123 124 152 with 3 links\n## 3 most connected regions:\n## 65 82 107 with 8 links"},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"creating-fixed-distance-neighbors","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.4 Creating fixed-distance neighbors","text":"concept buffering around locations define exposure probably familiar. uncommon exposures access health services, healthy food stores, exposure toxin emitters quantified using fixed-distance buffers. fixed-distance neighbor definition therefore natural extension, believe definition local near can described () falls within given radius. way different previous approaches neither sharing borders, k-nearest neighbor required.required place (specifically centroid place), falls within designated distance. number units falling within given threshold range zero maximum number units study, certainly vary one location another (e.g. thinking distinctions Eastern Western counties U.S.).Determining appropriate distance can challenging unless clear theory evidence (e.g. distance required avoid exposure radiation fixed point source). Often analysts consider range distances understand whether pattern changes competing scenarios. approach used investigate spatial clusters disease.define fixed-distance neighbors, use function dnearneigh() must define minimum distance (probably always set zero), maximum distance defining buffer.Beware using distance measures unprojected data.Note distance parameters described scale coordinate measures spatial object. case, spatial object projected, units meters. Therefore distance 1000 1 kilometer. However, unprojected data units angular degrees, readily interpretable.calculate neighbors two distance buffers: counties within 25km within 50km centroid county neighbors; counties neighbors. Notice output instance dnearneigh() just distances , actually formal neighbor (nb) object.can compare linkages two distance bands one another:compare one previous definition (e.g. Queen contiguity).","code":"\ndist_25 <- dnearneigh(ga_cent, d1 = 0, d2 = 25000)\ndist_50 <- dnearneigh(ga_cent, d1 = 0, d2 = 50000)\nsummary(dist_50)## Neighbour list object:\n## Number of regions: 159 \n## Number of nonzero links: 1072 \n## Percentage nonzero weights: 4.240339 \n## Average number of links: 6.742138 \n## Link number distribution:\n## \n##  3  4  5  6  7  8  9 10 11 12 \n## 10 18 20 21 33 21 21 10  4  1 \n## 10 least connected regions:\n## 4 14 22 41 61 71 96 120 152 153 with 3 links\n## 1 most connected region:\n## 156 with 12 links"},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"from-spatial-neighbors-to-spatial-disease-mapping","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.5 From spatial neighbors to spatial Disease Mapping","text":"main reason struggling preceding ins outs spatial neighbors like define reasonable version local given spatial dataset, use definition advance spatial epidemiologic goals. discussed , primary goal production statistically stable rates, less bouncing around parameters simply due small denominators.state : statistics solve fundamental problems sparse data! However, statistical disease mapping methods, including Empirical Bayes fully Bayesian methods, can use available information recover important underlying geographic trends instances.","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"empirical-bayes-overview","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.5.1 Empirical Bayes Overview","text":"introduced last week, Bayesian thinking mathematical operationalization relatively intuitive process engage : often prior information prior beliefs effect size risk rate plausible, informed experiences literature evidence date. look result analysis (e.g. data likelihood), internally (often sub-consciously) combine pieces (prior data) develop new, updated belief, posterior belief.Bayesian process framework moving implicit cognitive process open, stating mathematically prior belief , therefore arrived new updated, posterior, belief.Empirical Bayes disease rate smoothing process take set regions, consider data, question, ‘truest underlying rate disease place?’ compare observed data prior belief expectation rate plausibly (specifically, approximately within range).get prior important potentially impactful; last week aspatial Empirical Bayes smoothing used overall average rate entire study region (e.g. state Georgia) prior. words, sum cases across spatial units (e.g. counties), population risk across units, calculate single reference rate, variance around expectation.reference rate (prior) combined observed data weighted fashion prior weighted higher small-population regions, data weighted higher large-population regions. result weighted calculation posterior smoothed estimate rate.Last week calculated aspatial Empirical Bayes estimate low birthweight. prior information estimation comes size county’s expected count. Specifically mean, \\(\\mu\\), variance, \\(\\sigma^2\\) estimated \\(n=159\\) counties expected count, single, global, overall prior used strategy used last week.Beware output given function providing! One possible place confusion output global aspatial Empirical Bayes estimate using eBayes() excess relative risk county compared global referent (e.g. statewide prevalence VLBW). However function use spatial Empirical Bayes () output rate rather relative excess risk.Luckily two closely related. RR eBayes() represents relative deviation county statewide average. know global average (e.g. calculated r), simply multiplying RR value individual county single global referent, r, gives us aspatial Empirical Bayes smoothed estimate rate county. therefore comparable estimates spatial Empirical Bayes estimators.","code":"\n# Calculate aspatial EB\nglobal_eb1 <- eBayes(vlbw$VLBW, vlbw$expected)\n\n# Add the crude/observed SMR to the data\nvlbw$eb_global <- global_eb1$RR\n\n# Convert the aspatial EB RR to a smoothed aspatial EB rate by multiplying by referent rate, r\nvlbw$EB_global <- r * vlbw$eb_global"},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"spatial-empirical-bayes","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.6 Spatial Empirical Bayes","text":"using newly-created definitions local neighbors among Georgia counties can extend Empirical Bayes approach changing source prior information.aspatial global EB, total rate Georgia prior reference rate. However another option providing statistical information locally-varying expected rates use average one’s neighbors prior. produces sort borrowing statistical information space, assumption local counties tell us specific place counties far away.Note expectation counties next one another risk rate, instead average local information informative non-local (global) prior information. said, statistical approaches disease mapping believe important spatial dissimilarities exist neighbors, searching boundaries areas high low rates. can implemented package CARBayes introduced upcoming (optional) section fully Bayesian disease mapping.spatial EB, thus follows process global aspatial EB, different prior. prior defined local neighbors, different choices neighbor object likely least influence resulting geographic smoothed patterns.function estimating spatial Empirical Bayes EBlocal() spdep package, requires count events count population risk county, also nb neighbor object. Although highlighted importance neighbor symmetry spatial analysis, symmetric neighbors required spatial Empirical Bayes estimation.NOTE:help documentation state EBlocal() requires “numeric vector populations risk…” represent denominator rate estimation. purposes lab, exactly want. note – last week – used indirect adjustment calculate age-adjusted expected count, also feed expected counts EBlocal() function.difference instead output function spatially smoothed rate risk prevalence (e.g. events divided population risk), output instead spatially smoothed excess relative risk (RR) SMR (e.g. ratio observed expected).NOTE: currently function R estimate spatial EB rates credible/confidence intervals p-values, Poisson-Gamma model aspatial. Fully Bayesian disease mapping (e.g. Disease Mapping IV) best approach spatial methods producing credible/confidence intervals desired.Now can create spatial EB estimate neighbor definitions order understand robust sensitive ultimate results choice neighbors.","code":"\n# Estimate spatial (local) EB under the Queen contiguity neighbor definition\neb_queen <- EBlocal(vlbw$VLBW, vlbw$TOT, nb = queen_nb)\n\n# The output fro EBlocal() is a 2 column data.frame. The second column is the EB estimate\nvlbw$EB_queen <- eb_queen[,2]\n# Use the sphere of influence-pruned Delauney triangle definition\neb_soi <- EBlocal(vlbw$VLBW, vlbw$TOT, nb = soi_nb)\nvlbw$EB_soi <- eb_soi[,2]\n\n# Use the k-nearest neighbors (k=5) definition\neb_knn5 <- EBlocal(vlbw$VLBW, vlbw$TOT, nb = knn5_nb)\nvlbw$EB_knn5 <- eb_knn5[,2]\n\n# Use the 50-km fixed distance neighbors\neb_dist50 <- EBlocal(vlbw$VLBW, vlbw$TOT, nb = dist_50)\nvlbw$EB_dist50 <- eb_dist50[,2]"},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"visualizing-alternate-smoothing-approaches","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.6.1 Visualizing alternate smoothing approaches","text":"code simple visual comparison raw/observed, aspatial EB, variety spatially-smoothed EB estimates. review maps, might ask following questions:EB smoothing versus raw/observed estimates differ?spatial EB estimates differ aspatial EB estimate?differences notice among various spatial EB estimates, distinguished unique definitions local?saw last week, differences observed (crude) rates aspatial EB. However can see even dramatic differences four spatial EB rates compared either observed aspatial. Among spatial EB estimates minor differences suggesting – among set neighbor definitions, outcome – relatively consistent patterns VLBW regardless choice neighbors (e.g. answer relatively robust neighbor definition).","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"final-thoughts-making-choices","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.7 Final thoughts: Making choices","text":"past two weeks quickly amassed large number analytic tools address one problem spatial epidemiology: reliably characterize spatial heterogeneity presence rate instability uncertainty due data sparsity. analytic strategies include two approaches Empirical Bayes smoothing, also myriad neighbor definitions choose spatial approach.Unfortunately simple rule follow choosing tool use, summary considerations. Ultimately make decisions context epidemiologic question, constraints data, audience end-user results. many things epidemiologic analysis, important role science also need experts can engage art analysis.","code":""},{"path":"disease-mapping-ii-spatial-empirical-bayes.html","id":"choosing-neighborhood-definitions","chapter":"Week 5 Disease Mapping II: Spatial Empirical Bayes","heading":"5.3.7.1 Choosing ‘neighborhood’ definitions","text":"can see, lot ways describing local, haven’t even talked inverse-distance weighting. go choosing one definition another? example intersection art science spatial epidemiology., might wonder one decides smooth smooth, smoothing, neighbor definition use? lot written final answers. evidence approximately 6 neighbors provides nice balance informative prior information local units versus biased estimates. beyond broad recommendation, three general approaches selecting neighbor definition:maximizing precision fit - method statistical nature implies best smoother fits data best. possible estimate mean-squared error (MSE) root mean squared error (RMSE) describe far, average, observed rate observed data, idea closest average distance best. discuss extensions idea model fit move fully Bayesian mapping. code estimate RMSE.theory, context, question - emphasis explaining neighbors approach. clearly important analyst bring clarity question hand, local context, decision sensibly local given disease health outcome. instances, one method clearly stands others. However, uncommon moderate support (theoretically) multiple. can see mapped methods , case minor difference definitions data.empirically estimating weights - covering approach . However, briefly, idea use evidence spatial auto correlation inform spatially important one unit another.bottom line recommendation think spatially consider data, question, goals hand. purposes disease mapping, error reduction precision driving goals, comparing RMSE across competing options make sense. However purposes, statistical fit might equate unbiased estimation target parameters (true non-spatial analysis!).one simple approach calculating root mean squared error (RMSE). Recall RMSE square root average squared difference observed value model-predicted (case EB-smoothed) value. first bit code defining simple function calculate RMSE. define function 2 arguments: eb Empirical Bayes estimate \\(region_i\\), o observed rate \\(region_i\\).based , fits best? case, RMSE quite similar, slight advantage given spatial Queen contiguity definition. Although strong evidence preferring one strategy another based statistical fit alone. Therefore factors (including purpose map, audience, theory place underlying analysis) important making decisions.","code":"\nRMSE <- function(eb, o){\n  sqrt(mean((eb - o)^2))\n}\n\nRMSE(vlbw$EB_global, vlbw$rate)## [1] 0.006809047\nRMSE(vlbw$EB_queen , vlbw$rate)## [1] 0.006404844\nRMSE(vlbw$EB_soi , vlbw$rate)## [1] 0.006471757\nRMSE(vlbw$EB_knn5 , vlbw$rate)## [1] 0.006529195\nRMSE(vlbw$EB_dist50 , vlbw$rate)## [1] 0.006433534"},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"disease-mapping-iii-introduction-to-fully-bayesian-mapping","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","text":"","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"getting-ready-4","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.1 Getting Ready","text":"","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"learning-objectives-5","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.1.1 Learning objectives","text":"","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"additional-resources-5","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.1.2 Additional Resources","text":"Waller LA, Carlin BP. Disease mapping. Chapman Hall/CRC handbooks Mod Stat methods. 2010;2010(1979):217–43. (posted Canvas)CARBayes package vignetteCARBayesST spatio-temporal vignette","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"important-vocabulary-5","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 6","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"spatial-thinking-in-epidemiology-4","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.2 Spatial Thinking in Epidemiology","text":"","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"what-is-bayesian-inference","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.2.1 What is Bayesian Inference?","text":"Disease Mapping & II introduced global (aspatial) local (spatial) Empirical Bayes estimation. modules, introduced Bayes Theorem, high-level idea importance prior, likelihood, posterior Bayesian inference.However (intentionally) skirted much detail sections. section go little bit deeper; clear lot know learn Bayesian inference presented . hopefully summary helps motivate use fully Bayesian analysis spatial epidemiology.","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"frequentist-versus-bayesian-inference","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.2.1.1 Frequentist versus Bayesian Inference","text":"Frequentist statistics inference probably learned ‘statistics’ now. words, typical Bayesian inference taught depth, even , many statistics courses. interesting history current dominance frequentist inference much personalities, egos, power utility. ’s another day.core idea frequentist inference centers mental model premised comparing data observed abstract thought experiment expected infinite repetitions. strategy developed agricultural trials survey sampling; words settings meaningful think either repetitively resampling finite subset large population, repetitively conducting experiment order conceive parameter might expected vary simply due random error.Bayesian inference refers alternate philosophical statistical approach analysis inference observed data. Instead assuming frequency often something happen (e.g. abstract empirical thought experiment), Bayesian inference combines mental models.Bayesian’s articulate statement plausible distribution parameter given past experience knowledge (e.g. prior), combine directly data actually suggest. result combination updated statement distribution parameter (e.g. posterior).common critique Bayesian inference priors introduce subjective information compared objective assumptions frequentist inference. Instead, Bayesian priors simply explicit transparent assumptions made; contrast unrealistic unstated assumptions required frequentist inference.\nFIGURE 2.1: Frequentist vs. Bayesian Inference\ncartoon truth universe (e.g. sun exploded: true false) measured neurino detector. measurement almost always reports truth ’s measurement, rolls double-six dice, lies . measure occurs, dice rolled answer “sun exploded”.frequentist statistician left, finds probability telling lie small, given answer “sun exploded”, sun must exploded (e.g. null rejected). Bayesian statistician right skeptical.exaggerated example role prior belief. strictly frequentist interpretation, matters probability observed data hypothetical range possibilities stated null.Obviously silly cartoon. interesting discussion joke might also based incorrect interpretation frequentist statistics link.One nuance brushed aside simplified cartoon frequentist terms, joint hypothesis. Instead implied single hypothesis, “probability sun exploded?”, illustrated dual hypothesis, “probability sun exploded neurino detector told lie?”. clear way make logical flaw thinking illustrated fit well frequentist well Bayesian framework.","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"bayesian-inference-in-spatial-epidemiology","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.2.2 Bayesian inference in spatial epidemiology","text":"Bayesian statistics widely incorporated statistical methods across disciplines, Disease Mapping perhaps ubiquitous use Bayesian inference epidemiology. Aside appealing flexibility Bayesian inference generally, two specific reasons Bayesian inference makes sense disease mapping:Borrowing statistical strength spatial (even spatio-temporal) neighbors, efficient way improve reliability precision small area disease rate estimates. already seen spatial Empirical Bayes estimation, different way Kernel Density Estimation. Leveraging notion near things tend alike far thing, incorporation spatial neighbors source prior information can reduce variance estimates, smooth shrink implausible extreme values.Modeling spatial auto correlation explicitly important statistics conventionally rely assumptions independence among observations. Therefore, disease rates two adjacent counties correlated shared environment, demographic structure, interaction, dependence can result biased parameter estimates. Empirical Bayes smoothing explicitly address , fully Bayesian models spatial priors can explicitly model auto correlation, thus allowing estimation likelihood assumptions conditional independence.Bayesian statistics inherently complex frequentists statistics. However, likely substantially opportunity assimilate ideas frequentist thinking, thus Bayesian statistics may feel quite foreign. two concepts incorporate disease mapping Bayesian framework: hierarchical models (conditional autoregressive (CAR)) prior.","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"bayesian-hierarchical-models","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.2.2.1 Bayesian hierarchical models","text":"Bayesian models hierarchical sense conceive parameters discrete point estimates, instead range plausible values described probability distribution function (PDF). Thus target distribution parameters might lower-level hierarchy, parameters given PDF (e.g. mean variance), assumed arise random probability distribution, representing another hierarchical level.Thus, describe probability random variables lowest level (e.g. perhaps excess relative risk disease \\(region_i\\) compared expected), need specify “second level”, possibly third level hierarchical fashion.Take example interest disease mapping characterizing spatial heterogeneity, specifically value region-specific excess relative risk indicator deviation expectation.previous settings notated likelihood excess relative risk \\(\\theta_i\\); examine \\(\\theta_i\\) log scale, defining related parameter \\(\\psi\\) (spelled psi pronounced like sigh): \\(\\psi_i = log(\\theta_i)\\). Therefore following two statements observed data, \\(Y_i\\) probability model say thing:\\[Y_i | \\beta, \\psi_i \\sim Poisson(E_i exp(x_i \\beta + \\psi_i))\\]\n\\[Y_i| \\theta_i \\sim Poisson(E_i \\theta_i)\\]\ndifference two likelihood statements :) first log scale whereas second ;b) first explicitly incorporates possible covariates resulting parameters, \\(\\beta\\).Importantly, time use one Greek letters, saying represents random variable. Therefore, Bayesian framework, must specify distribution parameters random variable. distributional specifications prior!differ prior Empirical Bayes specify full probability distributions, rather discrete values calculated observed (empirical) data. model becomes hierarchical. first equation two random variables: \\(\\beta_i\\) \\(\\psi_i\\). requires specification prior.\\[\\beta \\sim N(0,100000)\\]\nparticular example, specified prior \\(\\beta\\) (e.g. prior range plausible coefficients possible included covariates model, rural/urban population density) relatively uninformative.words, saying \\(\\beta\\) arises normal distribution mean zero (e.g. average expect association) variance 100,000, saying little specific prior information; therefore data (e.g. likelihood) win cases. common practice fixed effects (e.g. global summary stationary parameters assumed vary space), quite similar frequentist assumption anything possible.However, strategy prior \\(\\psi_i\\) little different. Recall, discussing global local (aspatial spatial) Empirical Bayes, chief distinction specification prior: global/aspatial single prior expectation entire study region, whereas local/spatial EB, unique prior region, defined part values neighboring regions.similar approach taken fully Bayesian disease modeling. One approach define single prior applies region, irrespective spatial relatedness one another. often called spatially unstructured random effect. words random variable defined spatial connectivity, instead arises non-spatial phenomenon. Instead uninformative, incorporate prior knowledge form information plausible range values across study region.\\[\\psi_i \\sim N(0,\\sigma^2)\\]says range possible values \\(\\psi\\) arise normal distribution, centered zero (e.g. expectation region exactly expected), variance \\(\\sigma^2\\).mentioned , Empirical Bayes, value variance term (e.g. specification different think regions can plausibly one another) specified using empirical observed data. However, hierarchical Bayesian setting, can go yet another level say even \\(\\sigma^2\\) random variable prior . example common prior variance term (prior prior called hyperprior!) :\\[\\sigma^2 \\sim inverse-gamma(1, 0.01)\\]\ninverse gamma distribution specified two parameters, \\(\\alpha\\) \\(\\beta\\). Theoretically specify yet another hyper prior two parameters, example –following convention–specify numeric values 1 0.01. example prior distribution looks like (recall \\(\\sigma^2\\) describes variance log scale):summary hierarchical Bayesian models explicitly incorporated spatial relatededness neighbors. next section introduces spatial prior.","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"conditional-auto-regressive-priors","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.2.2.2 Conditional auto-regressive priors","text":"mentioned previous section, just global local priors Empirical Bayes, fully Bayesian disease mapping. global prior \\(\\psi_i\\) described arising common shared normal distribution regions.helps address concern instability estimates due sparse data single region, shrinking smoothing local regions towards global distribution. However, strategy address ubiquitous presence spatial-dependence. words global strategy neither addresses violation assumption independence among regions (e.g. often actually quite dependent auto-correlated!), take advantage fact provide stronger priors.Conditional Auto Regressive (CAR) model commonly used frequentist Bayesian spatial statistics. particular, informs estimation local log-relative risk parameter, \\(\\psi_i\\) conditioning information neighbors. CAR model setup data assumed distributed normally (e.g. Gaussian), mean variance defined conditional neighbors. fully Bayesian framework, can use CAR conditioning parameterize (e.g. prior ) values local \\(\\psi_i\\).CAR model incorporated multiple different types Bayesian priors disease mapping. basic setup CAR model described :\\[\\psi_i|\\psi_{j \\neq } \\sim N\\left(\\frac{\\sum_{j \\neq } w_{ij}\\psi_i}{\\sum_{j \\neq }w_{ij}}, \\frac{1}{\\tau_{CAR}\\sum_{j \\neq }w_{ij}}\\right)\\]says values \\(\\psi_i\\) (e.g. local log-relative excess risk) normally distributed, conditional values \\(\\psi_i\\) neighbors \\(\\). mean region-specific normal distribution weighted average values \\(\\psi_i\\) neighbors, variance distribution informed \\(\\tau_{CAR}\\), hyperprior denoting conditional variance among neighbors. term \\(w_{ij}\\) binary spatial weights matrix created much spatial Empirical Bayes identifying neighboring adjacent units \\(\\) \\(j\\) \\(w_{ij}=1\\) non-adjacent non-neighbor pairs \\(\\) \\(j\\) \\(w_{ij}=0\\)commonly used spatial prior Bayesian disease mapping called Besag-Yorke-Mollie BYM prior. combines spatially-explicit CAR prior characterize parts spatial heterogeneity spatially structured (e.g. related spatial dependence data), along global spatially unstructured Gaussian prior described previous section.idea sources variation regions fact spatially dependent (e.g. diffusion, selection similar populations, common exposure, etc), whereas sources difference spatially dependent (e.g. abrupt changes rural-suburban-urban, might region-specific exposures shared neighbors). combination types prior sometimes called convolution priors combine two separate random effects together.can describe fully hierarchical Bayesian BYM model like :\\[Y_i|\\beta, \\psi_i \\sim Poisson(E_i, exp(\\beta, \\psi_i))\\]\nnow specifying local values \\(\\psi_i\\) contributed two distinct random components (e.g. one spatially structured one unstructured), can define \\(psi_i\\) sum two parts: \\(\\psi_i = u_i + v_i\\), \\(u_i\\) spatially structured random variable \\(v_i\\) unstructured random variable. therefore needs prior:\\[u_i|u_{j \\neq } \\sim N\\left(\\frac{\\sum_{j \\neq } w_{ij}u_i}{\\sum_{j \\neq }w_{ij}}, \\frac{1}{\\tau_{CAR}\\sum_{j \\neq }w_{ij}}\\right)\\]\n\\[v_i \\sim N(0,\\sigma^2)\\]Finally, need specify hyperpriors two variance terms, \\(\\tau_{CAR}\\), \\(\\sigma^2\\); can defined relatively non-informative manner using Gamma Inverse Gamma distributions allow wide range possibilities.specification Bayesian disease mapping priors using CAR model can seen additional resources cited beginning section.Somewhat confusingly, CARBayes package described uses slightly different Greek letter nomenclature. Specifically, package authors use \\(\\psi\\) indicate spatially-correlated structure random effects, describes set random effects (e.g. convolution model spatially structured unstructured random effects)–\\(\\psi\\) one component–Greek letter \\(\\phi\\) (spelled phi pronounced like figh).models CARBayes package, spatially-correlated spatially unstructured random effects, Leroux, \\(\\psi_i\\) = \\(\\phi_i\\). point model output matrix named phi, might seem confusing calling thing psi (\\(\\psi\\)).","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"making-inference-from-bayesian-models","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.2.2.3 Making inference from Bayesian Models","text":"basic logic Bayesian inference relatively straightforward, can see Bayesian hierarchical framework looks complex! simple settings possible calculate full posterior distribution (e.g. combination likelihood prior via Bayes Theorem) using closed form strategies.However common closed-form solution exists complex, hierarchical conditional models. Therefore, currently two analytic strategies used make inference simple formula doesn’t work.Markov Chain Monte Carlo (MCMC) simulation used decades Bayesian analysis. ‘brute force’ method takes advantage two statistical tools make inference shape posterior distribution even complex Bayesian models.Markov Chain algorithm drawing (possibly highly dimensional) parameter space. uses stochasticity (randomness) ‘check’ different possible parts parameter space, using comparison well current location fits compared previous. result algorithm can ‘learn’ without getting stuck one location. goal eventually (random sampling plus learning) settle likely answer parameter value.Monte Carlo simulations repetitive sampling drawing posterior. describe exactly shape posterior distribution, take large number samples distribution (e.g. Markov Chain) can create summary shape posterior. instance, mean median large number samples becomes parameter point estimate, \\(2.5^{th}\\) \\(97.5^{th}\\) percentiles samples become Bayesian Credible Intervals.Integrated Nested Laplace Approximation (INLA). much newer strategy aims provide efficient way approximate shape scale posterior. INLA works R especially well suited complex hierarchical, spatial, spatio-temporal models areal point data.","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"spatial-analysis-in-epidemiology-4","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3 Spatial Analysis in Epidemiology","text":"Bayesian analysis requires bit care caution part analyst. strongly recommend proceeding project great caution (ideally expert consultant!). However, tools made Bayesian modeling accessible, part pre-programming ‘sensible’ defaults functions.module, discuss MCMC methods implemented single package, CARBayes, package represents reasonable point--entry interested starting Bayesian path. However excellent tutorial resources learning INLA, many Bayesian tools well.CARBayes package functions fitting wide range spatial disease mapping models including:Besag-York-Mollie (BYM) described , spatial heterogeneity modeled sum two random processes: spatially structured process spatial CAR prior; spatially independent unstructured processLeroux CAR model extension CAR single random effect (e.g. two BYM), variable degree spatial autocorrelation parameterized random hyperprior, \\(\\rho\\) describes places might versus less spatial dependence.Dissimilarity model uses CAR prior identify boundaries risk rate abruptly changes. model therefore highlights distinct differences amongst neighbors opposed encouraging similarity; can useful identifying spatial clustering.Localised CAR model another extension similar dissimilarity model aims identify abrupt changes surface highlight clusters.Multilevel CAR models nice alternative access individual level outcomes nested within areal (ecologic) units, opposed relying counts aggregated units.Multivariate Leroux model distinct preceding models univariate, meaning single ‘outcome’ unit observation. multivariate analysis (distinct common multivariable analysis multiple regression), multiple correlated outcomes unit analysis. One example modeling incidence three kinds cancer simultaneously.models can fit Poisson, Binomial, Normal (Gaussian), Multinomial distributed data.CARBayes package vignette provides additional detail specification different models, examples fitting using built-functions. addition, sister package, CARBayesST extensions spatio-temporal data, regions observed multiple times. information package available CARBayesST vignetteThe example uses commonly implemented Besag-York-Mollie (BYM) model.","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"packages-data","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3.1 Packages & Data","text":"addition now-familiar packages, also need load CARBayes package.example continue use low birthweight data used previous parts eBook. following code reads sf calculates raw rate VLBW use subsequent comparisons.","code":"\nlibrary(sf)        # sf provides st_read for importing\nlibrary(spdep)     # spdep has functions for creating spatial neighbor objects\nlibrary(tmap)      # tmap for plotting results\nlibrary(dplyr)     # dplyr for pipe processing of data\nlibrary(CARBayes)  # CARBayes has functions for fitting a range of CAR models\nvlbw <- st_read('ga-vlbw.gpkg') %>%\n  mutate(rate = VLBW / TOT )\n\nr <- sum(vlbw$VLBW) / sum(vlbw$TOT)\nvlbw$expected <- r*vlbw$TOT"},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"preparing-for-carbayes","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3.2 Preparing for CARBayes()","text":"addition usual preparation analytic data set, primary concern fitting Bayesian CAR model creation weights matrix, W, serves identify set neighbors county serve inputs describing shape prior probability distribution. can use tools previous weeks creating range neighbor objects, following caveats:Neighbors (weights) must symmetric, means \\(region_i\\) neighbor \\(region_j\\), \\(region_j\\) also neighbor \\(region_i\\). Contiguity graph-based neighbor objects symmetric design, k-nearest neighbors often asymmetric. Thus, created k-nearest neighbors object may need force symmetry using function make.sym.nb().regions must least one neighbor. formally, sum rows weights matrix must least 1. neighbor approach results unlinked regions (areas zero neighbors, case islands), need excluded, alternate adapted weights matrix created.object use CARBayes function must literal weights matrix (e.g. \\(159 \\times 159\\)) just nb object.create simple Queen contiguity neighbor object, convert object weights matrix. use style = 'B' creation weights matrix says values resulting matrix binary (0 1). default (style = 'W') results row-standardized weights, useful analytic tasks, necessary CAR models, CAR prior inherently adjusts number neighbors.Make sure weights match final data!always important spatial neighbors weights objects made final dataset! changes (additions deletion rows, also re-sorting rearranging) result misalignment spatial weights matrix data.","code":"\nqnb <- poly2nb(vlbw)\nqnb_mat <- nb2mat(qnb, style = 'B')\n\ndim(qnb_mat)  # confirming the dimensions of the matrix## [1] 159 159\nclass(qnb_mat) # confirming its class## [1] \"matrix\" \"array\""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"how-many-monte-carlo-samples-are-needed","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3.2.1 How many Monte Carlo Samples are needed?","text":"depends complex model , strong signal data . general concepts worth mentioning.First, tendency (randomly selected) starting location Markov Chain influence early samples. reason common plan discard portion samples burn-period. essentially means hope dependence initial conditions removing first \\(n\\) samples. burn-can quite large, e.g. tens thousands samples!goal model convergence achieved, meaning Markov Chain ‘learned’ enough settle relatively consistent area parameter space. can take many thousands samples, thus convergence diagnostics often used guide decisions many samples required.end day, need \\(n=1000\\) reliable high quality samples posterior accurately describe . may take \\(10,000\\), \\(50,000\\) even \\(100,000\\) samples achieve preceding goals burn-convergence. One option just keep last \\(1,000\\) samples. preferable option use thinning sample every \\(10^{th}\\) every \\(100^{th}\\) sample, burn-period. achives two goals: requires less memory store samples, also reduces residual auto-correlation among sequential samples.","code":""},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"fitting-a-besag-york-mollie-bym-bayesian-model","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3.3 Fitting a Besag-York-Mollie (BYM) Bayesian model","text":"fitting model, convenient (required) specify fixed-effect component model. specify outcome predictors. CAR Bayesian models can incorporate covariates categorical, continuous, even non-linear (e.g. spline polynomials) likelihood. two reasons might choose include covariates:Covariates strongly predictive outcome improve prediction local fitted rates. One interpretation random effects (e.g. \\(\\psi_i\\)), represent unmeasured causes correlates outcome. Addition relevant covariates explain previously-unmeasured factors.second reason may interest describing geographic trends disease conditional covariate. example used previously local age structure, although covariates might nuisance interpreting geographic patterns disease.now, covariates, thing fixed-effect portion model specification outcome variable (count deaths) offset variable (log denominator risk death), necessary Poisson model counts regions different populations risk. Note creating formula object. anything naming formula.Now three main ingredients:Data, form vlbw objectThe spatial weights matrix, W, represents spatial relationships (qnb_mat)fixed effects portion model.call BYM model specify:Poisson Poisson-Gamma?Notice family = 'poisson', even though previous work observed may extra-Poisson variation data. led us prefer Poisson-Gamma probability model (negative binomial); choose ?part fully Bayesian CAR model actually allowing extra-Poisson variation conditional prior, Gamma prior variance.formula, family, data, W discussed. next three arguments require explanation.burnin = 30000 specifies many MCMC samples discarded. general, discarding large number beginning via burnin argument recommended reduce sensitivity initial conditions ignore time spent early Markov Chain process.n.sample = 60000 specifies total number samples draw. Obviously number must larger burnin else nothing left look ! case take 60,000 samples, discarding first 30,000, leaving 30,000 examination.thin = 30 says keep every 30th sample drawn. reasons thinning reduce auto correlation among consecutive values, save memory, keeping useful number samples describe posterior. keeping 1000 samples, adequate summarizing parameters interest.actually many options analyst can choose specifying BYM model. instance, mentioned appealing feature CARBayes package built-number sensible defaults models, analyst doesn’t make many decisions.However, defaults can changed. example, inverse gamma prior variance, \\(\\tau^2\\), default settings, can modified additional arguments. might able digest now, might useful look briefly help documentation S.CARbym.","code":"\nmod <- VLBW ~ offset(log(TOT))\nbym <- S.CARbym(formula = mod, \n                        family = 'poisson', \n                        data = vlbw, \n                        W = qnb_mat,\n                        burnin = 30000, \n                        n.sample = 60000, \n                        thin = 30,\n                        verbose = FALSE)"},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"summarizing-carbayes-model-output","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3.4 Summarizing CARBayes model output","text":"summary() function returns list objects returned, nothing useful. print() function provides subset model summary statistics.total parameters estimated summarized . Specifically print() function display fixed-effects (global intercept included specify covariates), well hyper priors, \\(\\tau^2\\) \\(\\sigma^2\\).parameter \\(\\tau^2\\) characterizes conditional variance spatially structured random effects (e.g. \\(u_i\\) BYM convolution prior).parameter \\(\\sigma^2\\) characterizes variance unstructured global random effects.clear variability \\(\\tau^2\\) \\(\\sigma^2\\) suggesting total variability county-specific prevalence VLBW, seems attributable spatially-structured processes compared unstructured processes.note cautionAlthough appealing interpret two variance components BYM model just (e.g. describing proportionate contribution total variation), also known BYM models particular, two clearly identified independent one another. words together can describe variation, safe make inference either separate .Instead can confident sum two reliable prior.Also evident results print(bym) results hyper parameters, fixed-effects, summarized median value posterior samples (e.g. n=1,000 retained posterior samples case), well \\(2.5^{th}\\) \\(97.5^{th}\\) percentiles posterior (e.g. Bayesian Credible Intervals).Deviation Information Criteria (DIC) Bayesian model fit statistic, like fit statistics, smaller better.Finally, Geweke Diagnostic test designed characterize well Markov Chain converged stationary location parameter space. assumption Markov Chain, steps chain move towards optimum (best fitting) values, remain area. Thus Geweke compares mean value sampled posterior end samples, earlier point.model converged, two means similar. resulting test statistic standard Z-score. Values -1.96 1.96 suggestive good convergence, whereas values greater 1.96, less -1.96, may converged.model 60,000 total samples (30,000 burn-), Geweke diagnostic suggests good convergence intercept perhaps poor convergence two variance hyperpriors. model re-fit iterations see improves convergence. approaches include better model-specification (e.g. perhaps important variables missing, making model non-identifiable).","code":"\nsummary(bym)\nprint(bym)## \n## #################\n## #### Model fitted\n## #################\n## Likelihood model - Poisson (log link function) \n## Random effects model - BYM CAR\n## Regression equation - VLBW ~ offset(log(TOT))\n## \n## \n## #################\n## #### MCMC details\n## #################\n## Total number of post burnin and thinned MCMC samples generated - \n## Number of MCMC chains used - \n## Length of the burnin period used for each chain - \n## Amount of thinning used - \n## \n## ############\n## #### Results\n## ############\n## Posterior quantities and DIC\n## \n##                Mean    2.5%   97.5% n.effective Geweke.diag\n## (Intercept) -3.9946 -4.0465 -3.9434      1000.0        -0.8\n## tau2         0.1156  0.0482  0.2135       278.9        -0.8\n## sigma2       0.0093  0.0017  0.0275        77.9         0.2\n## \n## DIC =  889.1872       p.d =  54.33981       LMPL =  -452.83"},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"making-inference-from-bayesian-posteriors","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3.5 Making inference from Bayesian posteriors","text":"output S.CARbym() model function complex. use summary() names() output object, see several components. one called samples contains posterior draws MCMC process. Within object several data matrices, containing posterior samples different parameters. can understand little bit looking shape dimension dim().One matrices inside bym object named fitted.values. interest , instead characterizing heterogeneity \\(\\psi_i\\) (e.g. log-relative risk), wish map model-fitted rates.Fitted values scale observed data. means case Poisson model, fitted values counts VLBW predicted model. see , counts divided known denominator (total birth count) produce model-predicted risk prevalence.However, want know posterior specific random variables, \\(\\beta\\) look bym$samples$beta; want know random effects \\(\\psi_i\\), look bym$samples$psi.One thing might like examine Markov Chain trace plot understand sampled parameter space sequential steps. useful indicator convergence (e.g. trace plot settled common range, likely converged, whereas wanders place, ).Another visualization can useful see shape sampled posterior probability distribution. package coda designed specifically working MCMC samples Bayesian models kinds, functions creating plots. two functions visualizing posterior estimates global intercept, \\(\\beta\\).y-axis sampled values posterior distribution \\(\\beta\\), x-axis 1,000 samples retained (e.g. 60,000 draws - 30,000 burnin, thinning). Notice traceplot() shows Markov Chain moving around test different values. lot variation, bulk samples centered relatively narrow range, \\(-4.02\\) \\(-3.96\\), suggesting good convergence.Also notice chain ‘leaps’ forays away central area parameter space values. another feature Markov Chain: randomly evaluate parts parameter space see might fit better current. fact plot always returns quickly place suggests rejection alternate values.densplot(), can see shape sampled posterior, indicative probability density \\(\\beta\\). instance, clear probability mass median value around -4.0, probability mass lower, higher; words variation certainty true posterior spatial auto correlation value.Recall \\(\\beta\\) represents average log-risk VLBW Georgia. make numbers interpretable, exponentiate get \\(e^{-4.0} = 0.018\\). ‘average’ risk VLBW therefore 1.8%, counties vary around (e.g. ) value.preceding illustrations examine data plot specific parameters extended well beyond \\(\\beta\\) intercept alone! number parameters evaluated, remember \\(n=159\\) different values \\(\\psi_i\\).","code":"\nnames(bym)          # what is inside the model output object?##  [1] \"summary.results\"     \"samples\"             \"fitted.values\"      \n##  [4] \"residuals\"           \"modelfit\"            \"accept\"             \n##  [7] \"localised.structure\" \"formula\"             \"model\"              \n## [10] \"X\"\nnames(bym$samples)  # what is inside the 'samples' sub-object?## [1] \"beta\"   \"psi\"    \"tau2\"   \"sigma2\" \"fitted\" \"Y\"\ndim(bym$samples$beta) # 1000 draws for 1 beta fixed effect (the intercept)## [1] 1000    1\ndim(bym$samples$psi)  # 1000 draws for the psi = ui + vi for each of 159 counties## [1] 1000  159\ncoda::traceplot(bym$samples$beta)\ncoda::densplot(bym$samples$beta)"},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"extracting-summaries-for-mapping-or-analysis","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3.6 Extracting summaries for mapping or analysis","text":"Finally, want extract summaries data purposes analyzing visualizing. presence 1000 samples posterior every single parameter, makes working data cumbersome. Luckily extractor functions can help.fitted values separate matrix within samples, contain model-predicted value \\(\\hat{Y_i}\\) county. can useful calculating model-smoothed rate risk.random effects, \\(\\psi_i\\), interpreted log-relative risk county. words quantify degree county departs overall average (specifically global intercept, case captured beta matrix within bym$samples head(bym$samples$beta)).might wish summarize posterior county’s log-relative risk taking median samples \\(\\psi_i\\) (summarise posterior modeled parameter, matter!).First, worth noting data class objects containing bym$samples$psi:mcmc data class defined coda package, toolbox functions specific manipulating summarizing MCMC data objects. bym$samples$psi object basically Monte Carlo Markov Chain posterior summarization single parameter model. loaded package coda, able use summary() function object class mcmc success:","code":"\ny_fitted <- fitted(bym)\nvlbw$rate_bym <- y_fitted / vlbw$TOT\nclass(bym$samples$psi)## [1] \"mcmc\"\n# Load coda package\nlibrary(coda)\n\n# Extract the median (50th percentile) and 95% credible interval of posterior of psi\npsi <- summary(bym$samples$psi, quantiles = c(0.5, 0.025, 0.975))$quantiles\n\n# Exponentiate the estimates to put them on RR scale\nvlbw$RR_bym <- exp(psi[,1]) # RR\nvlbw$RR_lci <- exp(psi[,2]) # Lower 95% credible interval\nvlbw$RR_uci <- exp(psi[,3]) # Upper 95% credible interval"},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"plot-raw-versus-smoothed","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3.7 Plot raw versus smoothed","text":"might interested see different Bayesian values raw observed values. can use base-R plot .Just saw Empirical Bayes, Bayesian smoothed rates smoothed towards mean compared raw values.","code":"\nplot(vlbw$rate, vlbw$rate_bym)"},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"mapping-rates","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3.8 Mapping rates","text":"added modeled parameters vlbw spatial object, ready map., map appears similar spatial Empirical Bayes procedure Disease Mapping II. makes sense Bayesian use definition spatial neighbors. value-added fully Bayesian modeling compared spatial Empirical Bayes smoothing include:CAR-prior Bayesian smoothing borrows strength neighbors (like spatial EB), also models local spatial auto correlation.Bayesian modeling readily accommodates covariates regression, unlike Empirical Bayes procedure last week.Sampling Bayesian posterior permits inference including use 95% credible intervals exceedance probabilities possible spatial EB.","code":"\ntm_shape(vlbw) + \n  tm_fill(c('rate', 'rate_bym'),\n          style = 'quantile',\n          palette = 'BuPu',\n          title = c('Observed rate', 'CAR smoothed')) +\n  tm_borders() + \n  tm_layout(legend.position = c('RIGHT', 'TOP'))"},{"path":"disease-mapping-iii-introduction-to-fully-bayesian-mapping.html","id":"mapping-exceedance-probability","chapter":"Week 6 Disease Mapping III: Introduction to Fully Bayesian mapping","heading":"6.3.9 Mapping exceedance probability","text":"mentioned many times challenging can visualize parameter estimate, also measure precision, variance, reliability. nature Bayesian inference lends well characterizing probability posterior consistent threshold. can directly interpret posterior samples probability distribution, need look many MCMC samples exceeded given threshold order make inference confident estimate.example random effect parameters, \\(\\psi_i\\) represent deviation \\(region_i\\) overall mean rate value estimated global intercept. words random effects centered 0 (county \\(\\psi_i\\) = 0 county rate = intercept value).exponentiated, say relative risk centered null value 1. mapped relative risk (like SMR), ’re interested counties different 1, expected value state.calculate probability county greater (less ) 0 log scale (1 RR scale), simply look proportion samples posterior exceed value. proportion exceedence probability. might summarize counties high probability greater threshold (e.g. 0).Similarly, can look counties low probability exceeding zero. means samples zero. suggestive sub-zero deviation. way exceedance probability quantifies exceedingly high exceedingly low counties.calculate exceedence probabilities, use function summary(), time focus specific threshold interest. looking random effects, \\(\\psi_i\\), know meaningfully different value one equal zero (threshold different different parameters, e.g. \\(\\sigma^2\\)).can use quantiles returned summary() code . selected \\(2.5^{th}\\) \\(97.5^{th}\\) percentile posterior. compared actual value percentiles null divide estimates “exceed null 95% probability” .new variable vlbw_95 three-level indicator reflecting whether counties significantly lower risk average (e.g. upper credible interval smaller null 1), significantly higher risk average (e.g. lower credible interval larger null 1), whether posterior estimate county consistent state average (e.g. decent probability estimate \\(psi_i\\) actually different state expectation).worth noting Bayesian’s typically talk significance way frequentist’s. inherently estimating posterior distributions, rather testing discrete null hypotheses. However, ease, used word significance describe posteriors 95% credible interval excluding zero value.several ways incorporate new information map, two simple versions. first, layer three shapes top one another; first county values, second borders counties lower average, third borders counties higher average.alternative approach using tm_symbols() plot colored symbols high low counties.","code":"\n# Create a 1, 0 variable in vlbw indicating P<0.025 or P>0.975\nvlbw$rr_95 <- ifelse(vlbw$RR_lci > 1, 'High', \n                     ifelse(vlbw$RR_uci < 1, 'Low', NA))\ntm_shape(vlbw) +\n  tm_fill('RR_bym',\n          style = 'fixed',\n          palette = 'PRGn',\n          breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Relative Risk') +\n  tm_borders() + \ntm_shape(subset(vlbw, rr_95 == 'Low')) + \n  tm_borders(col = 'purple', lwd = 2) +\ntm_shape(subset(vlbw, rr_95 == 'High')) + \n  tm_borders(col = 'green', lwd = 2) +\n  tm_add_legend(type = 'line', \n                labels = c('Low risk county', 'High risk county'), \n                col = c('purple','green')) +\n  tm_layout(legend.position = c('RIGHT','TOP'))\ntm_shape(vlbw) +\n  tm_fill('RR_bym',\n          style = 'fixed',\n          palette = 'PRGn',\n          breaks = c(0.13, 0.67, 0.9, 1.1, 1.4, 2.3),\n          title = 'Relative Risk') +\n  tm_borders() + \ntm_shape(vlbw) + \n  tm_symbols(shape = 'rr_95',\n             col = 'rr_95',\n             palette = 'Dark2',\n             size = .5,\n             shapeNA = NA,\n             showNA = FALSE,\n             legend.shape.show = FALSE,\n            title.col = 'Significance') +\n  tm_layout(legend.outside = TRUE)"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"disease-mapping-iv-kernel-density-estimation","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"Week 7 Disease Mapping IV: Kernel Density Estimation","text":"","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"getting-ready-5","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.1 Getting Ready","text":"","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"learning-objectives-6","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"additional-resources-6","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.1.2 Additional Resources","text":"Adrian Baddeley tutorial analysis spatial point processesHazelton. Kernel Smoothing Methods. Chapter 10, Handbook Spatial Epidemiology. Posted Canvas","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"important-vocabulary-6","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 7","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"spatial-thinking-in-epidemiology-5","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.2 Spatial Thinking in Epidemiology","text":"","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"revisiting-spatial-point-processes","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.2.1 Revisiting spatial point processes","text":"People exist places, uniformly randomly distributed. live cities; fewer live rural area. conditional people actually live work play, null expectation treat occurrence health events (e.g. disease, death, behaviors) random variables, leverage tools statistics characterize occurrence expect versus something unusual.spatial analysis, treat health events random events among individuals located space. Thus, conditional people , might assume (, null hypothesis) occurrence events generated according assumed probability distribution. utility Poisson Point Process becomes apparent see divide region small sub-regions count number events within , assuming count follows Poisson distribution.\nFIGURE 2.1: Poisson point process\nfigure, quantify spatial intensity events calculating \\(\\lambda = \\frac{n}{area}\\). Thus, statistical analysis date premised idea spatial location points can interpreted lens Poisson probability distribution.calculate spatial intensity continuously, without constraint specific parametric distribution, without using possibly arbitrary boundaries zoning schemes areal geographic units census tracts, zip codes, counties?Spatial point process analysis focuses characterizing patterns derived directly location points , without arbitrary aggregation. study point process analysis broad, focus one particularly flexible strategy week: kernel density estimation (KDE). Kernel density estimation several features making useful spatial epidemiology including:non-parametric, meaning rely specific probability distribution (e.g. Poisson, negative binomial)provides alternative means characterizing local neighbors. strategy KDE estimators similar inverse distance weightingIt can used primary tool, can intermediate step creating spatial weights, see geographically weighted regressionWhile really designed analyzing points, can use areal/polygon data well.","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"what-is-a-kernel-density-estimator","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.2.2 What is a kernel density estimator?","text":"kernel function (e.g. mathematically described ‘shape’) iteratively centered point data. example, Gaussian kernel means Gaussian bell-shaped curve centered point; width curve defined parameter, \\(h\\), stands bandwidth.\nFIGURE 2.2: Kernel density estimator\nestimate spatial intensity points, \\(\\hat{\\lambda}\\), can sum area kernels estimate overall kernel density location. kernel density estimate essentially reports spatially continuous summary local intensity events.result can summarize study region spatially-referenced point data using spatially continuous intensity surface. analyst decides smooth bumpy surface increasing decreasing value bandwidth parameter, \\(h\\). see , decision bandwidth made subjectively (e.g. produce visually appealing surface), minimizing error cross-validation.\nFIGURE 2.3: Kernel density smoothing\n","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"spatial-heterogeneity-versus-spatial-non-stationarity","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.2.3 Spatial heterogeneity versus Spatial non-stationarity","text":"now familiar concept spatial heterogeneity phenomenon local estimates intensity, risk, rate different least locations global estimates across entire region. local rates statistically similar global rate say process spatially homogeneous, likely much interest spatial cartographic perspective.vocabulary Disease Mapping 1, first introduced terms ‘stationarity’ versus ‘non-stationarity’ referred closely related idea. stationary process one estimate statistic (e.g. intensity, density, risk, rate, also correlation regression coefficient) dependent location within region. words, estimate statistic invariant choice local area within global study region. contrast, spatial non-stationarity present estimate statistic dependent location measures made.Distinguishing heterogeneity non-stationarityThese two concepts clearly similar. One way unpack subtle differences think two kinds spatial heterogeneity:Areal units (e.g. counties) different values parameter interest (e.g. disease rate), high-rate counties low-rate counties randomly distributed. words conclude heterogeneity even randomly mixed exact location counties.Area units different values, values tend clustered space high-rate counties near one another, low-rate counties near one another. Thus value given county dependent spatial location.discussion spatial heterogeneity spatial non-stationarity relevant discussion spatially varying statistics second part Spatial Analysis module week, especially relevant begin discuss spatial clustering spatial regression.","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"limitations-in-kernel-density-estimation","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.2.4 Limitations in kernel density estimation","text":"several features real data can limit accurate estimation true underlying spatial intensity surface. :Study region edge effects: almost situation, data available given dataset represents subset universe points events interest. can carry KDE available data, boundaries data collected (e.g. within given state boundary), intensity points near boundaries may mis-estimated due missing data. Several statistical smoothers options incorporate adjustments edge effects.Determining bandwidth: choice kernel bandwidth perhaps influential decision driving final results appear. occasionally may theoretical grounds priori specification bandwidth, often decision one subjective analyst choice (typically ideal) statistical optimization. sections discussion fixed versus adaptive bandwidths, well algorithms selecting values least upper lower bounds.","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"uses-for-kernel-density-estimation-kde-in-spatial-epidemiology","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.2.5 Uses for kernel density estimation (KDE) in spatial epidemiology","text":"kernel density estimation method learn semester working point data, clear one major application. However, generally, KDE broad applications. examples might think using kernel density estimates:producing spatially continuous (typically raster) surfaces representing disease risk. application consistent disease mapping, purpose describe spatial heterogeneity disease intensity risk.summarizing point-referenced resources (e.g. retail food outlets; health care clinics; toxic emitters; etc) exposure surface. strategy alternative calculating custom distances event point every resource exposure. Instead, kernel density surface summarizes average exposure resources given point space.smoothing summarizing data measured areal polygon unit. KDE optimized point data, possible extend smoothing data (exposure, covariate, health outcome) measured reported ecologic areal unit.Building extension KDE polygons, can summarize social economic exposure surfaces. useful way extend socio-economic-cultural measures might available census geography represent contained within specific boundaries, explicitly spatially situated. See tutorial eBook appendix.Distinguishing point intensity risk intensityThe material emphasizes interest intensity points per unit area. point represent epidemiologically? Usually represents location observation, automatically clear whether observation event interest (e.g. illness, death, etc), whether person risk (e.g. sampled observed participant may may experience event).epidemiologic purposes care ratio events population risk. can translate idea kernel density surfaces estimating two kernel density estimates: one intensity events (e.g. deaths) per unit area, second intensity persons risk death per unit area. can take ratio two kernel density estimates produce risk surface.illustrated following sections.next section, introduce two different uses kernel density estimation:First intensity estimation spatial point processes consistent description .Second, illustrate use kernel density estimators create weights geographically-weighted summary statistics, including spatially varying mean risk rate. strategy can applied either points polygons.","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"guide-to-the-rest-of-this-section","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.2.6 Guide to the rest of this section…","text":"lot content contained module. worth highlighting broad distinctions help navigate.first section focuses tools using sparr spatstat packages create kernel density estimates point data. includes:Creating ppp objects planar point processesCreating owin objects define study regionsDiscussion several different strategies selecting kernel bandwidth dictates smoothingCreation kernel density surfaces single point processesCreation kernel density relative risk surfaces contrasting ratio numerator denominator intensityVisualizing output several ways.second section introduces seemingly quite distinct strategy incorporating kernel density estimators spatial epidemiology. introduces tools calculating geographically weighted summary statistics characterize spatial heterogeneity. tools use kernel density estimators geographically weight observations, can applied points polygons.NOTE:modules semester, Spatial Analysis section eBook uses examples introduce analytic approaches, -class Lab uses different data practice.‘newness’ kernel density estimators, large volume information, following material eBook closely aligned practice Lab.recommend read content lab, work lab , refer back eBook questions.","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"spatial-analysis-in-epidemiology-kernel-estimation-of-point-processes","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3 Spatial Analysis in Epidemiology: Kernel estimation of point processes","text":"section three specific objectives:Introduce new spatial data class R, ppp, necessary executing kernel estimation functionsIntroduce kernel density estimation spatial point processes, including selection fixed bandwidths, use adaptive bandwidthsIntroduce spatial relative risk surfaces, including estimation tolerance contours","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"preparing-packages-and-data","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.1 Preparing packages and data","text":"several new packages required work:data used example, lab, concerns exact \\(x,y\\) residential location births Dekalb Fulton county, including indication infants subsequently died within first year life.NOTE: data simulated based approximate patterns. representation actual birth event data.spatial point location births infant deaths two separate files. addition polygon file providing outline Dekalb Fulton counties provided describe study window.","code":"\npacman::p_load(tidyverse,  # for general data management\n               sf,         # for handling sf data class\n               tmap,       # for mapping\n               spatstat,   # A package with tools that underly the sparr package\n               sparr,      # A package for estimating spatial intensity and relative risk\n               raster)     # The outputs of these KDE functions will be raster. This package gives us tools for working with rasters\n# This is points for births in Dekalb/Fulton county\nb_point <- st_read('birth_points.gpkg')\n\n# This is points for deaths in Dekalb/Fulton county\nd_point <- st_read('death_points.gpkg') \n\n# This is an outline of Dekalb/Fulton county to be used as a study 'window'\ncounty <- st_read('DekalbFultonWindow.gpkg') "},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"introducing-a-new-spatial-data-class-ppp","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.2 Introducing a new spatial data class: ppp","text":"Much statistical methods spatial point process actually developed ecology, methods merging spatial analysis spatial epidemiology fields recent years. One consequence history, early developers methods R defined spatial data classes, case class called ppp point pattern data two-dimensional plane.create ppp data object need, minimum, two things:matrix \\(x,y\\) coordinates event pointsA definition spatial window study region.window necessary nearly data set sub-sample universe possible points, analysis point processes requires appreciation bounds sampling observation.define window formally R object class owin, can rectangular bounding box (e.g. outline available data), customized polygon. use outline Dekalb & Fulton counties customized spatial window observation births infant deaths.can see summary plot owin object looks like.","code":"\ncounty_owin <- as.owin(county)\nsummary(county_owin)## Window: polygonal boundary\n## single connected closed polygon with 698 vertices\n## enclosing rectangle: [1026366.3, 1098719.2] x [1220671, 1302019.3] units\n##                      (72350 x 81350 units)\n## Window area = 2086320000 square units\n## Fraction of frame area: 0.354\nplot(county_owin)"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"creating-the-ppp-objects","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.3 Creating the ppp objects","text":"Now use function ppp() create objects class ppp spatial point files, b_point (representing locations births, denominator infant mortality), d_point (representing locations deaths, numerator infant mortality). requires definition study window defined object name county_owin class owin., study window delineates versus study area demarcates edges study region. can look help documentation function ppp() see arguments. Note function requires specification \\(x,y\\) locations two separate vectors, extract coordinate values sf object using st_coordinates().two ppp objects?Recall estimation spatial point process agnostic whether point (person) event (e.g. death) (e.g. live birth lived past 1 year). represent ratio deaths live births (e.g. infant mortality rate), need estimate two kernel density surfaces, take ratio.might expect, built-methods (spatstat package) summarize plot ppp objects.summary includes information overall spatial intensity (e.g. events per unit area), well number points, observational window. plot d_ppp look just like plot d_point contain information. note, repeat code birth events, b_point, plot less readable 94,000 births compared 705 deaths!","code":"\n# Create the birth ppp object\nb_ppp <- ppp(x = st_coordinates(b_point)[, 1], \n             y = st_coordinates(b_point)[, 2],\n             window = county_owin)\n\n# Create the death ppp object\nd_ppp <- ppp(x = st_coordinates(d_point)[, 1], \n             y = st_coordinates(d_point)[, 2],\n             window = county_owin)\nsummary(d_ppp)## Planar point pattern:  701 points\n## Average intensity 3.359978e-07 points per square unit\n## \n## Coordinates are given to 2 decimal places\n## i.e. rounded to the nearest multiple of 0.01 units\n## \n## Window: polygonal boundary\n## single connected closed polygon with 698 vertices\n## enclosing rectangle: [1026366.3, 1098719.2] x [1220671, 1302019.3] units\n##                      (72350 x 81350 units)\n## Window area = 2086320000 square units\n## Fraction of frame area: 0.354\nplot(d_ppp)"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"bandwidth-selection","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.4 Bandwidth selection","text":"discussed , lecture, kernel density estimation requires analyst specify kernel function (e.g. Gaussian kernel, quartic bi-weight kernel), kernel bandwidth.two, bandwidth substantially impactful results choice kernel function.reminder, bandwidth (sometimes indicated variable \\(h\\)) describes width radius kernel function, result dictates smooth resulting intensity surface . small bandwidth produce bumpier rougher surface, whereas larger bandwidth result smoothing.Bandwidth measures proximity closenessConceptually might notice similarity bandwidth spatial analysis definition spatial neighbors . case kind ‘local’ inclusion data analysis, within broader global study region.Bandwidth defines sets points considered close .two general kinds bandwidth settings:Fixed bandwidths: single value \\(h\\) designates width kernel (thus resulting smoothness estimated intensity surface) entire study region. Fixed bandwidths commonly used, sensible study region relatively homogeneous population risk. However choosing single value can challenging practice density points varies substantially across study region, case study region includes range urban rural.Adaptive bandwidths: name implies, approach changes adapts size kernel density bandwidth according density points (data) differing sub-areas overall study region. result relatively smoothing (larger bandwidth) areas sparse point data, relatively less smoothing (smaller bandwidth) areas point density.","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"fixed-bandwidth-methods","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.4.1 Fixed bandwidth methods","text":"prefer fixed bandwidth, first challenge choosing . One option selecting fixed bandwidth incorporate theory prior knowledge process interest. instance, trying understand whether prevalence diabetes related local food environment urban area, might want bandwidth helps illuminate differences diabetes intensity scale consistent food environment. instance bandwidth 1-mile might reasonable urban areas one 50-miles, latter likely smooth away local variation interest.However, uncommon theory prior knowledge insufficient make clear choice, data sparsity mandates alternate approach driven concern stable estimates.package sparr several functions designed use primarily statistical optimization estimating ‘optimum’ bandwidth. introduce two commonly used statistical bandwidth selection optimizers:Cross-validation: approach divides data subsets, using one subset choose bandwidth, comparing performance subsets. goal find value works ‘best’ (e.g. optimize statistical parameter across multiple iterations). approach computationally intensive large datasets. discussed note , cross-validation can result small bandwidth estimation.-smoothing: alternate approach aims identify maximum amount smoothing necessary minimizing statistical error. definition maximum value rather ideal optimal value, can useful setting bounds.sparr package provides three cross-validation approaches estimation:LCSV.density (least squares cross validated);LIK.density (likelihood cross-validated);SLIK.adapt (described experimental likelihood cross-validation adaptive).‘optimizing’ different thing. LSCV.density minimizes unbiased estimate mean integrated squared error (MISE) whereas LIK.density maximizes cross-validated leave-one-average log-likelihood density estimate.look help documentation see (near bottom) prominent warning message. reports “CV bandwidth selection notoriously unstable practice tendency produced rather small bandwidths…”","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"cross-validation-with-lik.density","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.4.2 Cross-validation with LIK.density()","text":"LIK.density() uses likelihood estimation cross-validation optimal bandwidth. death dataset, d_ppp, runs just seconds. However took > 5 minutes produce value much larger births dataset, b_ppp. code lets try see produces:examine object returned (h_LIK_d), ’ll see just single number. value \\(h\\), optimized bandwidth. , words, radius 2-dimensional kernel density function units data, meters case (e.g. original data Albers Equal Area projected). means optimum kernel radius just 1.5 kilometers.","code":"\nh_LIK_d <- LIK.density(d_ppp)## Searching for optimal h in [27.463186212964, 12058.8149770359]...Done.\nprint(h_LIK_d)## [1] 1613.661"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"oversmoothing-algorithm-with-function-os","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.4.3 Oversmoothing algorithm with function OS()","text":"approach much less computationally intense, thus feasible spatial point processes (e.g. numerator deaths, denominator births). see , can use value returned OS() pilot value adaptive bandwidth estimation. words provides kind reference starting point adaptation process.Note birth data smaller optimal bandwidth (h_os_b) points. points means information available granular smoothing, whereas relatively sparse death data larger smoothing bandwidth (h_os_d).","code":"\nh_os_d <- OS(d_ppp)\nh_os_b <- OS(b_ppp)\n\nprint(h_os_d)## [1] 4257.798\nprint(h_os_b)## [1] 1897.371"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"selecting-a-common-bandwidth-for-both-numerator-and-denominator","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.4.4 Selecting a common bandwidth for both numerator and denominator","text":"One challenge bandwidth selection typically two related spatial point processes (e.g. numerator, death events; denominator, birth events). Therefore don’t want single KDE, instead need consider numerator representing spatial intensity deaths, denominator representing spatial intensity live births risk. raises question whether common bandwidth , whether optimized separately.may minor differences absolute intensity different bandwidths single point process, taking ratio two intensity surfaces can exaggerate small differences quite large. functions estimating single, joint, optimum bandwidth. function LSCV.risk() just LIK.density() , two spatial point processes. code example , like previous example, cross-validation approach birth data set takes excessive amount time (least exercise).","code":"\n#h_LSCV_risk <- LSCV.risk(d_ppp, b_ppp)"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"adaptive-bandwidth-methods","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.4.5 Adaptive bandwidth methods","text":"Adaptive methods specified time kernel density estimation. bandwidth constant, instead adaptive, usually still need specify pilot bandwidth, reference point adaptive modification occurs. mentioned , -smoothing approach OS() can used pilot value.","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"estimating-kernel-density-surfaces","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.5 Estimating Kernel Density surfaces","text":"now turn actual estimation kernel density approximations underlying spatial intensity disease. approach lab first illustrate estimate separate densities point process (e.g. deaths births), demonstrate two strategies creating spatial relative risk surfaces, generally target output spatial epidemiologists.note, discussion demonstrate use fixed adaptive bandwidths. general adaptive bandwidths may practical approach absence theoretical empirical preference otherwise. However instances fixed bandwidths (either theoretically informed, derived CV -smoothing algorithms) desired, thus seeing action useful.","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"bivariate.density-for-kde-of-single-point-process","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.5.1 bivariate.density() for KDE of single point process","text":"actually several R packages accomplish kernel density estimation, one particularly useful spatial epidemiology (kernel density estimator must 2-dimensional frequently take ratio two densities) sparr package, stands Spatial Spatiotemporal Relative Risk.sparr function bivariate.density() flexible useful tool carrying KDE either fixed adaptive bandwidths. many arguments bivariate.density() (see help documentation), several worth specifically highlighting.Intensity versus DensityUp now used words intensity density synonymous point process parameter, exactly accurate.Intensity average number points per unit area. density proportionate intensity, scaled values study region sum 1. words density surface proper probability density function (PDF).map two look identical except scale legend.","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"fixed-bandwidth-kde-with-bivariate.density","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.5.2 Fixed bandwidth KDE with bivariate.density()","text":"First, let’s try basic version uses -smooth estimate point process.can explore objects produced function call. instance, list objects, named sub-elements:also plotting functionality built sparr package allows us quickly visualize resulting density plot.","code":"\ndeath_kde <- bivariate.density(pp = d_ppp, h0 = h_os_d, edge = 'diggle')\nbirth_kde <- bivariate.density(pp = b_ppp, h0 = h_os_b, edge = 'diggle')\nsummary(birth_kde)## Bivariate Kernel Density/Intensity Estimate\n## \n## Bandwidth\n##   Fixed smoothing with h0 = 1897.371 units (to 4 d.p.)\n## \n## No. of observations\n##   94373 \n## \n## Spatial bound\n##   Type: polygonal\n##   2D enclosure: [1026366, 1098719] x [1220671, 1302019]\n## \n## Evaluation\n##   128 x 128 rectangular grid\n##   5808 grid cells out of 16384 fall inside study region\n##   Density/intensity range [6.313906e-14, 1.585466e-09]\nnames(birth_kde)## [1] \"z\"         \"h0\"        \"hp\"        \"h\"         \"him\"       \"q\"        \n## [7] \"gamma\"     \"geometric\" \"pp\"\npar(mfrow = c(1, 2)) # Plots in 1 row with 2 columns\nplot(birth_kde, main = 'Birth density')\nplot(death_kde, main = 'Death density')\npar(mfrow = c(1,1))  # Reset plotting space to be 1 row, 1 column"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"adaptive-bandwidth-kde-with-bivariate.density","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.5.3 Adaptive bandwidth KDE with bivariate.density()","text":"discussed , alternative single fixed bandwidth, implementation algorithm changes (adapts) bandwidth across study region response density sparseness data. approach still requires specification global bandwidth, adaptation multiplier making global smaller larger needed.code use argument h0 = xxx specify pilot bandwidth. adaptive bandwidth KDE requires adjustment across study region, notice functions take longer fixed bandwidth , especially large birth point process.adaptive maps similar fixed bandwidth?plots intensity adaptive bandwidth point processes look different *fixed bandwidth processes. reason units measurement (intensity) small differences readily apparent.However differences become apparent take ratio two surfaces, .","code":"\ndeath_kde_adapt <- bivariate.density(d_ppp, \n                                     h0 = h_os_d, \n                                     edge = 'diggle', \n                                     adapt = TRUE,\n                                     verbose = FALSE)\nbirth_kde_adapt <- bivariate.density(b_ppp, \n                                     h0 = h_os_b, \n                                     edge = 'diggle',\n                                     adapt = TRUE,\n                                     verbose = FALSE)\npar(mfrow = c(1, 2))\nplot(birth_kde_adapt, main = 'Birth density\\n(adaptive h)')\nplot(death_kde_adapt, main = 'Death density\\n(adaptive h)')\npar(mfrow = c(1,1))"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"plotting-kde-estimates-with-tmap","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.5.4 Plotting KDE estimates with tmap","text":"handy sparr package built-plotting functionality quickly visualize results. However may want control plotting, instance tmap even ggplot2.recall named elements list object returned bivariate.density(), first called z, density surface .class object im image. However almost spatial plotting operation outside sparr spatstat, want data raster class rather im format (fundamentally data raster model, data structure R quite data structure class raster). can convert im raster class like :Fixing missing CRS projectionNotice code specification crs(x) <- \"EPSG:5070\".im object lost information original coordinate reference system (CRS) projection. However, need raster object CRS information plot properly tmap. know original point data projected Albers Equal Area, specifically EPSG code 5070. re-define creating rasters .Now can plot tmap:maps look pixely?two reasons contributing .First, code specified colors quantiles order get range despite possibly skewed values. try re-plotting plots setting style = 'cont' continuous color palette, comment n=9. see produces much smoother looking plot. difference plots style = 'cont' style = 'quantile' gradation color intermediate levels intensity.First, code specified colors quantiles order get range despite possibly skewed values. try re-plotting plots setting style = 'cont' continuous color palette, comment n=9. see produces much smoother looking plot. difference plots style = 'cont' style = 'quantile' gradation color intermediate levels intensity.Another reason pixelation original call bivariate.density() used default output resolution 128 x 128 grid cells. done computational efficiency. However, note want higher-resolution surface (e.g. publication, presentation), can increase specifying resolution = creation KDE surface original call bivariate.density().Another reason pixelation original call bivariate.density() used default output resolution 128 x 128 grid cells. done computational efficiency. However, note want higher-resolution surface (e.g. publication, presentation), can increase specifying resolution = creation KDE surface original call bivariate.density().","code":"\nclass(birth_kde$z)## [1] \"im\"\ndeath_kde_raster <- raster(death_kde_adapt$z)\ncrs(death_kde_raster) <- \"EPSG:5070\"\nbirth_kde_raster <- raster(birth_kde_adapt$z)\ncrs(birth_kde_raster) <- \"EPSG:5070\"\n# Create map of death surface\nm1 <- tm_shape(death_kde_raster) + \n  tm_raster(palette = 'BuPu',\n            style = 'quantile',\n            n = 9,\n            title = 'Death density') +\n  tm_shape(county) +\n    tm_borders() + \n  tm_layout(legend.format = list(scientific = T))\n\n# Create map of birth surface\nm2 <- tm_shape(birth_kde_raster) +\n  tm_raster(palette = 'BuPu',\n            style = 'quantile',\n            n = 9,\n            title = 'Birth density') +\n  tm_shape(county) +\n    tm_borders() + \n  tm_layout(legend.format = list(scientific = T))\n\n# plot 2-panel arrangment\ntmap_arrange(m1, m2)"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"creating-relative-risk-surface-manually","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.6 Creating relative risk surface manually","text":"now, created KDE surface death birth points separately. epidemiology, rarely care numerator denominator separately! put two together informative disease map?Raster algebra term arithmetic algebraic manipulation raster grids. Recall raster data set simply array numbers. numbered value grid-point represents mean density intensity points per unit-area, mapped color make plot.simply matrix numbers, can take two rasters resolution study area add, subtract, multiply, log-transform, otherwise operate arithmetically.instance manually create spatial relative risk surface simply take ratio two KDE density surfaces. result relative measure akin SMR: quantifies relative deviation area overall average value. values 1 areas lower average risk, meaning intensity deaths less intensity live births, values 1 higher average risk (intensity deaths greater intensity live births).Intensity vs Density matters taking ratioFor interpreting ratio two kernel surfaces take care distinguish spatial intensity (number point events per unit area; value integrates sums total number points across study region) versus spatial density (probability point event occurring location, conditional total number points; integrates 1 across study region). distinction important.default output bivariate.density() function spatial density surface. ratio two density (probability) surfaces take value 1.0 probability death location proportionate probability birth location.contrast ratio two intensity surfaces interpreted absolute measure (e.g. risk, rate, prevalence) ranging zero 1. choose intensity = TRUE specifying bivariate.density() function get intensity rather (default) density surface.tmap call flipped color ramp using negative sign front name ramp. also specified continuous style rather discrete (e.g. quantile), specified legend breaks.Now can clearly see regions higher risk lower risk infant mortality!","code":"\n# Create risk surface as ratio of death density to birth density\nrisk <- death_kde_raster / birth_kde_raster\n\n# Map it...\ntm_shape(risk) + \n  tm_raster(palette = '-RdYlGn',\n            style = 'cont',\n            breaks = c(0.1, 0.6, 0.9,   1.1, 2, 4.9),\n            title = 'IMR SMR')  +\ntm_shape(county) +\n  tm_borders()"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"creating-relative-risk-surface-with-risk-function","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.7 Creating relative risk surface with risk() function","text":"preceding manual approach created two separate kernel density surfaces, manually relied raster algebra create spatial relative risk surface. useful know may use KDE setting work single spatial point process (e.g. imagine instead estimating disease intensity, wish estimate exposure density surface).However, sparr package provides shortcut estimation spatial relative risk surfaces function risk(). takes numerator denominator ppp object arguments calculates spatial relative risk surface automatically.demonstration comparing four pre-defined (possibly theoretically informed, possibly arbitrary), fixed bandwidths purposes data exploration, estimate four distinct spatial relative risk surfaces, well one adaptive KDE. case notice first two arguments numerator denominator ppp object. next argument pre-specified fixed bandwidth (e.g. 1000, 2000, 4000, 8000 meters).function also illustrates another feature allows us quantify statistical precision creating tolerance contours. Tolerance contours simply lines encircle regions statistically significant beyond given threshold. argument tolerate = T tells function estimate asymptotic p-values testing null hypothesis local relative risk death equal across study region.default, function estimates log relative risk, helpful reminder relative risk asymmetric. However, understand ratio measures, careful plot results appropriately. reason, set log = FALSE, although obviously omit keep everything log scale.NOTE: fixed bandwidth risk() functions run quickly, , adaptive bandwidth computationally intensive, take longer.Examine contents one objects. summary show us range estimated risk, resolution evaluation grid, number points evaluated.NOTICEYou can see range estimated risk \\([-8.216873e-14, 11.05946]\\). lower bound practical terms zero (e.g. , small), counter-intuitively also negative! estimated negative risk? answer seems related Diggle edge correction. example substitute edge = 'uniform' anomaly goes away. likely edge correction (big picture valuable strategy) reweights regions may result specific location estimates become negative., can use built-plotting functionality sparr produce maps spatial relative risk surface tolerance contours. (NOTE: default legend works best log relative risk, doesn’t behave well relative risk treats distance 0 1 distance 1 2, 4 5).code plots side--side, might find easier plot one time zoom closer.Things notice plots:Notice contour lines \\(p<0.05\\).Notice risk surface becomes smoother fixed bandwidth transitions \\(1km\\) \\(8km\\).Finally, notice adaptive bandwidth consistent maps, seems balance detail 2000 meter 4000 meter definitions.","code":"\nimr1000 <- risk(d_ppp, b_ppp, h0 = 1000, \n            tolerate = T,\n            verbose = F, \n            log = F,\n            edge = 'diggle')\n\nimr2000 <- risk(d_ppp, b_ppp, h0 = 2000, \n            tolerate = T,\n            log = F,\n            edge = 'diggle',\n            verbose = F)\n\nimr4000 <- risk(d_ppp, b_ppp, h0 = 4000, \n            tolerate = T,\n            log = F,\n            edge = 'diggle',\n            verbose = F)\n\nimr8000 <- risk(d_ppp, b_ppp, h0 = 8000, \n            tolerate = T,\n            log = F,\n            edge = 'diggle',\n            verbose = F)\n\nimradapt <- risk(d_ppp, b_ppp, \n                 h0 = h_os_d, \n                 adapt = T, \n                 tolerate = T,\n                 log = F,\n                 edge = 'diggle',\n                 verbose = F)\nsummary(imr1000)## Log-Relative Risk Function.\n## \n## Estimated risk range [-8.216873e-14, 11.05946]\n## \n## --Numerator (case) density--\n## Bivariate Kernel Density/Intensity Estimate\n## \n## Bandwidth\n##   Fixed smoothing with h0 = 1000 units (to 4 d.p.)\n## \n## No. of observations\n##   701 \n## \n## Spatial bound\n##   Type: polygonal\n##   2D enclosure: [1026366, 1098719] x [1220671, 1302019]\n## \n## Evaluation\n##   128 x 128 rectangular grid\n##   5808 grid cells out of 16384 fall inside study region\n##   Density/intensity range [-1.82875e-25, 2.78467e-09]\n## \n## --Denominator (control) density--\n## Bivariate Kernel Density/Intensity Estimate\n## \n## Bandwidth\n##   Fixed smoothing with h0 = 1000 units (to 4 d.p.)\n## \n## No. of observations\n##   94373 \n## \n## Spatial bound\n##   Type: polygonal\n##   2D enclosure: [1026366, 1098719] x [1220671, 1302019]\n## \n## Evaluation\n##   128 x 128 rectangular grid\n##   5808 grid cells out of 16384 fall inside study region\n##   Density/intensity range [2.143127e-16, 2.383589e-09]\nnames(imr1000)## [1] \"rr\" \"f\"  \"g\"  \"P\"\npar(mar = c(1,1,1,1)) # adjust the plotting margins\npar(mfrow = c(3,2))   # Plot 3 rows and 2 columns\nplot(imr1000)\nplot(imr2000)\nplot(imr4000)\nplot(imr8000)\nplot(imradapt)\npar(mfrow = c(1,1))  # reset the plot space to 1 row, 1 col"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"using-functions-to-map-rr-with-tmap","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.3.8 Using functions to map RR with tmap","text":"begin plotting several maps, conversion im raster code producing map panels can feel cumbersome. following section required, demonstration can write simple custom functions R speed repetitive tasks.function R like macro SAS; simply set instructions accepts arguments (inputs), carries action inputs, returns output. Thus, helps automate cumbersome tasks can repeatedly efficiently.function accepts single argument, labeled simply x . expectation x output one preceding calls risk() function.Notice function first extracts spatial relative risk surface (e.g. x$rr), assigns appropriate projection (got stripped along way).function extracts probability map set pixel-specific p-values. rasterToContour() function takes raster creates contour lines specified levels corresponding 95% tolerance contour. Finally, use return() tells returned function called.write function, need loaded given session; afterwords can call using prepRaster(x = my_risk_object).’re roll, also write function producing tmap panel:result work can easily (compactly), map four fixed-bandwidth spatial relative risk surfaces.","code":"\n### --- prepRaster() function --- ###\nprepRaster <- function(x){\n  rr <- raster(x$rr)\n  crs(rr) <- \"EPSG:5070\"\n\n  p_raster <- raster(x$P)\n    crs(p_raster) <- \"EPSG:5070\"\n  plines <- rasterToContour(p_raster, levels = c(0.025,  0.975))\n  \n  return(list(rr=rr,plines=plines))\n} ## END prepRaster() ##\n### --- make_map() function to create panel maps --- ###\nmake_map <- function(x, bw){\n  mtitle <- paste('IMR - ', bw, ' smooth', sep = '')\n  tm_shape(x$rr) +\n  tm_raster(palette = '-RdYlGn',\n            style = 'cont',\n            breaks = c(0.1, 0.6, 0.9,   1.1, 2, 4.9),\n            midpoint = NA,\n            title = 'IMR SMR') +\n  tm_shape(x$plines) +\n  tm_lines(col = 'level',\n           legend.col.show = F) + \n  tm_layout(main.title = mtitle,\n            legend.format = list(digits = 1)) \n} ## END make_map() function ##\n# First convert to raster and extract p-value contours\nrr_1000 <- prepRaster(imr1000)\nrr_2000 <- prepRaster(imr2000)\nrr_4000 <- prepRaster(imr4000)\nrr_8000 <- prepRaster(imr8000)\nrr_adapt <- prepRaster(imradapt)\n\n# Then produce map panels\nm1000 <- make_map(rr_1000, '1 km')\nm2000 <- make_map(rr_2000, '2 km')\nm4000 <- make_map(rr_4000, '4 km')\nm8000 <- make_map(rr_8000, '8 km')\nmapadapt <- make_map(rr_adapt, 'adaptive')\n\ntmap_arrange(m1000, m2000, m4000, m8000,mapadapt, ncol = 2)"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"gwss","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.4 Spatial Analysis in Epidemiology: Kernel estimation of areal data","text":"preceding section saw use KDE estimating smooth spatial intensity surface spatial point process. section introduce geographically-weighted statistics extendable areal units points. three key features use KDE preceding section expand :KDE points - whole notion kernel density estimation process indeed connected \\(x,y\\) point location, mean take advantage non-parametric smoothing kinds data, polygons. Typically centroid (geometric center) polygon used stand-point KDE done polygons.KDE binary values - spatial point process definition description location discrete points representing discrete state. instance section , visualized spatial intensity surface infant deaths, separately surface live births. want measure continuous value rather discrete, binary state spatial locations? mechanics KDE can still helpful!KDE kind spatial weighting procedure - true Part well…spatial intensity essentially spatially-weighted number points surrounding index location divided area kernel function. lab primary use kernel function produce weights calculating weighted-statistics including mean, median, etc quantity measured.accomplish extensions use functions package GWmodel (e.g. geographically weighted models) useful exploratory spatial data analysis. main function used , gwss(), stands geographically weighted summary statistics, uses non-parametric spatial weighting kernel density function compute locally varying descriptive statistics mean, median, standard deviation, correlation, . certainly works data represented \\(x,y\\) points, can also work polygon data.means gwss() function can useful exploring spatial heterogeneity form local spatial non-stationarity. Recall spatial stationarity notion statistical parameter global constant across space?previously encountered stationarity opposite spatial heterogeneity. context referring risk prevalence health states. statistic can stationary (constant) non-stationary (spatially varying).objective section extend understanding utility kernel density functions beyond simply computing intensity density surfaces seeing tool creating spatially local weights statistical function. use study region (Fulton Dekalb counties), now looking several socio-contextual covariates derived American Community Survey considered along infant mortality rate produced .section focuses primarily gwss() function accomplish following tasks:Estimate statistically-optimal fixed bandwidth explore adaptive bandwidths use gwss() functionCalculate local, spatially-weighted mean, median, SD, IQR four census-tract level continuous measures using kernel density functionsUsing Monte Carlo simulation produces significance contours estimates local, spatially-weighted summary statisticsCalculate local, spatially-weighted bivariate statistics summarizing correlations (Pearson Spearman) pairs variables varies space","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"packages-and-data","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.4.1 Packages and data","text":"new package introduced GWmodel, several familiar packages also useful:sp Data classIn addition new packages, also need sp package. Recall worked sf class data now, although learned sp older format spatial data R. packages functions (including GWmodel) incorporated compatibility sf data, need convert particular objects sf sp moving forward.methods introduced section work either spatial points spatial polygons. However example demonstrate use specifically spatial polygons, specifically polygons representing census tracts Fulton Dekalb counties. dataset 345 census tract polygons (4 tracts deleted due missing values), summarizes five summary measures tract:First, read gpkg data sf object, convert sp use GWmodel. sparr package, GWmodel yet fully sf compliant forced use sp data classes. likely change point future.","code":"\nlibrary(tidyverse) # For data piping and manipulationlibrary(GWmodel)   # Has the function gwss() \nlibrary(sf)        # For import of sf data\nlibrary(sp)        # For conversion to sp, required for GWmodel\nlibrary(tmap)      # For mapping\nlibrary(GWmodel)   # Geographically weighted statistics\n# This is Dekalb/Fulton census tracts\natl <- st_read('Fulton-Dekalb-covariates.gpkg') %>%\n  as('Spatial') # convert to sp class (required for GWmodel)"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"mapping-the-observed-values","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.4.1.1 Mapping the observed values","text":"understand new data, consider using summary(), exploratory functions, well producing simple maps see spatial distribution variables. Recall tmap package works sf sp data! means can map object ‘usual manner’ even though converted sp.Index Concentration Extremes (ICE) ranges -1 +1. value \\(-1\\) occurs everyone tract poor; value \\(+1\\) occurs tracts everyone affluent; value \\(0\\) suggests either balance affluence poverty, alternatively everyone ‘middle income’. Therefore makes sense map separately inevitably need divergent color ramp.","code":"\n# First map the 4 variables that are %\ntm_shape(atl) + \n  tm_fill(c('pctNOHS', 'pctPOV', 'pctMOVE', 'pctOWNER_OCC'),\n          style = 'quantile') +\n  tm_borders()\ntm_shape(atl) +\n  tm_fill('ICE_INCOME_all',\n          style = 'quantile',\n          palette = 'div') +\n  tm_borders()"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"why-are-we-using-kde-on-these-data","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.4.1.2 Why are we using KDE on these data?","text":"least two general reasons might think use spatial smoothing approach KDE continuous data :believe estimates spatial unit (polygons case, points) statistically unstable, believe averaging neighbors produce reliable estimate parameter interest; orYou interested identifying spatial patterns trends possibly larger scale data units . instance might looking regions city apparent clustering poverty, home ownership, residential instability. words suspect people exposed values within boundaries tracts, also nearby environments.","code":""},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"what-bandwidth-for-kernel-density-estimates","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.4.2 What bandwidth for kernel density estimates?","text":"Recall decision bandwidth kernel density function one influential using KDE spatial epidemiology. reason bandwidth defines smoothness bumpiness statistical estimation, different choices can produce dramatically different results., might theoretically important criteria selecting bandwidth, use statistical optimization approach. GWmodel function bw.gwss.average() used estimating ‘optimal’ bandwidth estimating spatially varying mean median using cross-validation. specific function statistics (e.g. SD, IQR, correlation coefficients). spatial structure might different variable, can evaluate variables:numbers units map, meters. suggests need pretty large fixed bandwidth (58 70 km!), least minimize error determined cross-validation approach.Adaptive bandwidth GWmodelAdaptive bandwidth GWmodel works little differently sparr package. choose adaptive = TRUE returned distance units map (e.g. meters) instead number nearest neighbors defines large small kernel function adapts.important conceptually adaptation means: GWmodel unit analysis census tract, individual person. census tract 10 people census tract 10,000 assumed amount information., see story CV approach suggests large bandwidth. 345 areal units dataset, suggests nearly included kernel. produce little spatial variation. idea statistical optimization approach appealing, discussed , CV methods known imperfect.now use common-sense approach. seem might balance local information spatial variation including 10% data single kernel density estimation location. choose use \\(n=35\\) neighbors definition adaptive bandwidth. Note alter number see results vary.","code":"\n# Fixed bandwidth selection\nbw.gwss.average(atl, vars = c('pctPOV', 'pctPOV', 'pctMOVE', \n                              'pctOWNER_OCC', 'ICE_INCOME_all'))##                   pctPOV   pctPOV  pctMOVE pctOWNER_OCC ICE_INCOME_all\n## Local Mean bw   58801.33 58801.33 58801.33     65740.07       65740.07\n## Local Median bw 65740.07 65740.07 58801.33     70028.45       65353.39\n# Adapative bandwidth selection\nbw.gwss.average(atl, vars = c('pctPOV', 'pctNOHS', 'pctMOVE', \n                              'pctOWNER_OCC', 'ICE_INCOME_all'),\n                adaptive = T)##                 pctPOV pctNOHS pctMOVE pctOWNER_OCC ICE_INCOME_all\n## Local Mean bw      315     268     297          337            333\n## Local Median bw    297     297     315          326            315"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"geographically-weighted-summary-statistics-gwss","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.4.3 Geographically weighted summary statistics: gwss()","text":"fully describe explore ‘raw’ data, want summarize smoothing extremes, looking broad spatial trends values. Finding local average value can done using either mean median quantify central tendency. Obviously, distribution data within local regions relatively normally distributed (least symmetric), mean median similar. data quite skewed, might prefer median summary measures.Similarly, knowing whether (much) local measures alike different informative. choose statistic works well describe variation normally-distributed (symmetric) data (e.g. standard deviation), one performs well non-normal skewed data (e.g. inter-quartile range). Finding large values either SD IQR suggest substantial local heterogeneity difference target measure, whereas small values suggest local areas relatively similar.gwss() function actually estimates just listed , now focus measures.calculate geographically-weighted summary statistics using gwss() need provide dataset, single variable (vector multiple variables), decision using fixed adaptive bandwidth, finally specification bw bandwidth . , value enter bw depends whether select adaptive = T .fixed bandwidth, value enter number units map (e.g. meters case). requesting adaptive bandwidth, value bw meters, actually number count many nearest neighbors minimally included kernel density estimation location. discussed , use \\(n=35\\) adaptive definition neighbors. result summary estimation including approximately 10% total data. like called robust statistics (e.g. median IQR robust skewed non-normal data), also must specify argument quantile = T.Perhaps unintuitively, way get summary result usual summary(), instead type print(atl.ss). see LOT results. focus moment just results top (local mean; local SD) bottom (local median; local IQR) output.summary gives information range smoothed values statistic, variable. Notice, example names variables. estimates geographically weighted mean end _LM stands local mean. Similarly estimates geographically weighted standard deviation end _LSD local SD.","code":"\natl.ss <- gwss(atl, vars = c('pctPOV', 'pctNOHS', 'pctMOVE', 'pctOWNER_OCC',\n                             'ICE_INCOME_all'),\n               adaptive = T,\n               bw = 35,\n               quantile = T)\nprint(atl.ss)##    ***********************************************************************\n##    *                       Package   GWmodel                             *\n##    ***********************************************************************\n## \n##    ***********************Calibration information*************************\n## \n##    Local summary statistics calculated for variables:\n##     pctPOV pctNOHS pctMOVE pctOWNER_OCC ICE_INCOME_all\n##    Number of summary points: 345\n##    Kernel function: bisquare \n##    Summary points: the same locations as observations are used.\n##    Adaptive bandwidth: 35 (number of nearest neighbours)\n##    Distance metric: Euclidean distance metric is used.\n## \n##    ************************Local Summary Statistics:**********************\n##    Summary information for Local means:\n##                           Min.   1st Qu.    Median   3rd Qu.   Max.\n##    pctPOV_LM          0.050645  0.119765  0.180836  0.270868 0.3947\n##    pctNOHS_LM         0.011338  0.031796  0.051985  0.079042 0.1369\n##    pctMOVE_LM         0.129222  0.166627  0.185688  0.215939 0.3356\n##    pctOWNER_OCC_LM    0.214225  0.421030  0.486894  0.568339 0.7545\n##    ICE_INCOME_all_LM -0.363659 -0.161687 -0.010174  0.180475 0.3921\n##    Summary information for local standard deviation :\n##                           Min.  1st Qu.   Median  3rd Qu.   Max.\n##    pctPOV_LSD         0.030466 0.080964 0.095033 0.122989 0.2000\n##    pctNOHS_LSD        0.013018 0.027740 0.038529 0.055891 0.1512\n##    pctMOVE_LSD        0.043404 0.062143 0.070973 0.085519 0.1594\n##    pctOWNER_OCC_LSD   0.128460 0.171147 0.193042 0.236043 0.3127\n##    ICE_INCOME_all_LSD 0.098726 0.153791 0.186239 0.214184 0.2949\n##    Summary information for local variance :\n##                              Min.    1st Qu.     Median    3rd Qu.   Max.\n##    pctPOV_LVar         0.00092817 0.00655511 0.00903123 0.01512619 0.0400\n##    pctNOHS_LVar        0.00016947 0.00076952 0.00148449 0.00312385 0.0229\n##    pctMOVE_LVar        0.00188388 0.00386175 0.00503717 0.00731354 0.0254\n##    pctOWNER_OCC_LVar   0.01650192 0.02929137 0.03726506 0.05571613 0.0978\n##    ICE_INCOME_all_LVar 0.00974689 0.02365154 0.03468479 0.04587486 0.0870\n##    Summary information for Local skewness:\n##                            Min.  1st Qu.   Median  3rd Qu.   Max.\n##    pctPOV_LSKe         -0.60296  0.40145  0.88760  1.50706 4.5718\n##    pctNOHS_LSKe        -0.26353  0.80588  1.38779  2.41449 7.7612\n##    pctMOVE_LSKe        -0.43164  0.27893  0.58307  0.97433 2.6409\n##    pctOWNER_OCC_LSKe   -1.21089 -0.41080 -0.10531  0.17408 1.1014\n##    ICE_INCOME_all_LSKe -1.03050 -0.18046  0.17660  0.55386 1.8477\n##    Summary information for localized coefficient of variation:\n##                             Min.    1st Qu.     Median    3rd Qu.    Max.\n##    pctPOV_LCV            0.21281    0.41550    0.57740    0.73493  1.1784\n##    pctNOHS_LCV           0.31989    0.49239    0.85060    1.30476  2.2432\n##    pctMOVE_LCV           0.23460    0.35345    0.38736    0.42998  0.6638\n##    pctOWNER_OCC_LCV      0.20111    0.35046    0.42753    0.51556  0.7493\n##    ICE_INCOME_all_LCV -127.78594   -1.02921   -0.35211    1.03900 94.7817\n##    Summary information for localized Covariance and Correlation between these variables:\n##                                                     Min.      1st Qu.\n##    Cov_pctPOV.pctNOHS                       -0.002293092  0.000901241\n##    Cov_pctPOV.pctMOVE                       -0.002989390  0.000907888\n##    Cov_pctPOV.pctOWNER_OCC                  -0.039119689 -0.020228990\n##    Cov_pctPOV.ICE_INCOME_all                -0.054587425 -0.021777955\n##    Cov_pctNOHS.pctMOVE                      -0.006067192  0.000036009\n##    Cov_pctNOHS.pctOWNER_OCC                 -0.028576009 -0.005198261\n##    Cov_pctNOHS.ICE_INCOME_all               -0.027722207 -0.005410855\n##    Cov_pctMOVE.pctOWNER_OCC                 -0.026758619 -0.012236511\n##    Cov_pctMOVE.ICE_INCOME_all               -0.017867269 -0.008467689\n##    Cov_pctOWNER_OCC.ICE_INCOME_all           0.007196400  0.020528232\n##    Corr_pctPOV.pctNOHS                      -0.317000144  0.330201284\n##    Corr_pctPOV.pctMOVE                      -0.331272313  0.157747220\n##    Corr_pctPOV.pctOWNER_OCC                 -0.926032270 -0.768233160\n##    Corr_pctPOV.ICE_INCOME_all               -0.921214422 -0.855191644\n##    Corr_pctNOHS.pctMOVE                     -0.555196633  0.009497939\n##    Corr_pctNOHS.pctOWNER_OCC                -0.742254592 -0.495928778\n##    Corr_pctNOHS.ICE_INCOME_all              -0.793477505 -0.545592647\n##    Corr_pctMOVE.pctOWNER_OCC                -0.891153190 -0.746883164\n##    Corr_pctMOVE.ICE_INCOME_all              -0.817237195 -0.594811585\n##    Corr_pctOWNER_OCC.ICE_INCOME_all          0.476114554  0.736449688\n##    Spearman_rho_pctPOV.pctNOHS              -0.313709999  0.399932352\n##    Spearman_rho_pctPOV.pctMOVE              -0.338963728  0.215510831\n##    Spearman_rho_pctPOV.pctOWNER_OCC         -0.932742660 -0.772363010\n##    Spearman_rho_pctPOV.ICE_INCOME_all       -0.945687048 -0.876406885\n##    Spearman_rho_pctNOHS.pctMOVE             -0.407210551  0.057271829\n##    Spearman_rho_pctNOHS.pctOWNER_OCC        -0.828797842 -0.551819264\n##    Spearman_rho_pctNOHS.ICE_INCOME_all      -0.840537241 -0.647642983\n##    Spearman_rho_pctMOVE.pctOWNER_OCC        -0.885112112 -0.731419517\n##    Spearman_rho_pctMOVE.ICE_INCOME_all      -0.832992937 -0.560002101\n##    Spearman_rho_pctOWNER_OCC.ICE_INCOME_all  0.454870395  0.712341847\n##                                                   Median      3rd Qu.    Max.\n##    Cov_pctPOV.pctNOHS                        0.001807506  0.003304389  0.0180\n##    Cov_pctPOV.pctMOVE                        0.002229805  0.004467243  0.0099\n##    Cov_pctPOV.pctOWNER_OCC                  -0.014516859 -0.009563665 -0.0016\n##    Cov_pctPOV.ICE_INCOME_all                -0.014606879 -0.009577934 -0.0023\n##    Cov_pctNOHS.pctMOVE                       0.000331801  0.000748402  0.0038\n##    Cov_pctNOHS.pctOWNER_OCC                 -0.002496901 -0.001218701  0.0065\n##    Cov_pctNOHS.ICE_INCOME_all               -0.002789993 -0.001573582  0.0039\n##    Cov_pctMOVE.pctOWNER_OCC                 -0.008392739 -0.005565503  0.0056\n##    Cov_pctMOVE.ICE_INCOME_all               -0.005403826 -0.002219461  0.0149\n##    Cov_pctOWNER_OCC.ICE_INCOME_all           0.031021243  0.044464726  0.0689\n##    Corr_pctPOV.pctNOHS                       0.528380318  0.709155971  0.9313\n##    Corr_pctPOV.pctMOVE                       0.336717286  0.519424037  0.7694\n##    Corr_pctPOV.pctOWNER_OCC                 -0.701980159 -0.604820576 -0.3121\n##    Corr_pctPOV.ICE_INCOME_all               -0.805848933 -0.738051250 -0.2913\n##    Corr_pctNOHS.pctMOVE                      0.134766298  0.300808384  0.6833\n##    Corr_pctNOHS.pctOWNER_OCC                -0.380222665 -0.228294821  0.4213\n##    Corr_pctNOHS.ICE_INCOME_all              -0.446961901 -0.323081384  0.3913\n##    Corr_pctMOVE.pctOWNER_OCC                -0.613431373 -0.433099806  0.2776\n##    Corr_pctMOVE.ICE_INCOME_all              -0.458003819 -0.189021343  0.5593\n##    Corr_pctOWNER_OCC.ICE_INCOME_all          0.813864246  0.868133644  0.9618\n##    Spearman_rho_pctPOV.pctNOHS               0.556988603  0.704535569  0.9311\n##    Spearman_rho_pctPOV.pctMOVE               0.374243796  0.529475103  0.7608\n##    Spearman_rho_pctPOV.pctOWNER_OCC         -0.699827624 -0.621225648 -0.3108\n##    Spearman_rho_pctPOV.ICE_INCOME_all       -0.839312766 -0.774693178 -0.3117\n##    Spearman_rho_pctNOHS.pctMOVE              0.174120074  0.303105444  0.5853\n##    Spearman_rho_pctNOHS.pctOWNER_OCC        -0.428396144 -0.245467087  0.2446\n##    Spearman_rho_pctNOHS.ICE_INCOME_all      -0.524973642 -0.381356579  0.1699\n##    Spearman_rho_pctMOVE.pctOWNER_OCC        -0.619497465 -0.450139874  0.3082\n##    Spearman_rho_pctMOVE.ICE_INCOME_all      -0.428406930 -0.173601402  0.4005\n##    Spearman_rho_pctOWNER_OCC.ICE_INCOME_all  0.797280146  0.852406288  0.9477\n##    Summary information for Local median:\n##                               Min.   1st Qu.    Median   3rd Qu.   Max.\n##    pctPOV_Median          0.042468  0.090310  0.161004  0.251916 0.3919\n##    pctNOHS_Median         0.003663  0.014632  0.042506  0.068828 0.1053\n##    pctMOVE_Median         0.095830  0.158633  0.186185  0.201781 0.3108\n##    pctOWNER_OCC_Median    0.191905  0.391960  0.490220  0.602281 0.8389\n##    ICE_INCOME_all_Median -0.384111 -0.195011 -0.023930  0.170068 0.3830\n##    Summary information for Interquartile range:\n##                            Min.   1st Qu.    Median   3rd Qu.   Max.\n##    pctPOV_IQR         0.0272517 0.0846864 0.1150697 0.1709975 0.3370\n##    pctNOHS_IQR        0.0071992 0.0233457 0.0362078 0.0567798 0.1961\n##    pctMOVE_IQR        0.0492332 0.0777156 0.0943680 0.1143242 0.2195\n##    pctOWNER_OCC_IQR   0.0695205 0.2406716 0.2930468 0.3634722 0.6663\n##    ICE_INCOME_all_IQR 0.0741381 0.1853240 0.2307883 0.2932845 0.5597\n##    Summary information for Quantile imbalance:\n##                            Min.    1st Qu.     Median    3rd Qu.   Max.\n##    pctPOV_QI         -0.8423900 -0.3575769 -0.1613870  0.0836692 0.7498\n##    pctNOHS_QI        -0.9376945 -0.5163889 -0.2601448  0.0084147 0.4966\n##    pctMOVE_QI        -0.7152669 -0.2848576 -0.0713806  0.1454639 0.7001\n##    pctOWNER_OCC_QI   -0.7647186 -0.1424103  0.0614975  0.2285407 0.7302\n##    ICE_INCOME_all_QI -0.8670434 -0.2368184 -0.0310687  0.1636269 0.8803\n## \n##    ************************************************************************"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"mapping-gwss-results","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.4.3.1 Mapping gwss results","text":"see results? Try looking result:many sub-objects within main result object. first one, always called SDF class SpatialPolygonsDataFrame. basically sp version polygon spatial file. examine closely (e.g. try summary(atl.ss$SDF)) see happens) see information need make maps (e.g. spatial object attribute data).First, let’s map geographically-weighted median value statistics:can also examine local variation diversity values mapping geographically-weighted IQRRemember, places higher IQR larger local differences values. places high variability similar , different , places high median values?can now repeat code ICE_INCOME_all variable, also repeat variables looking mean SD rather median IQR. evidence local mean local median different?","code":"\nnames(atl.ss)##  [1] \"SDF\"      \"vars\"     \"kernel\"   \"adaptive\" \"bw\"       \"p\"       \n##  [7] \"theta\"    \"longlat\"  \"DM.given\" \"sp.given\" \"quantile\"\nsummary(atl.ss)##          Length Class                    Mode     \n## SDF      345    SpatialPolygonsDataFrame S4       \n## vars       5    -none-                   character\n## kernel     1    -none-                   character\n## adaptive   1    -none-                   logical  \n## bw         1    -none-                   numeric  \n## p          1    -none-                   numeric  \n## theta      1    -none-                   numeric  \n## longlat    1    -none-                   logical  \n## DM.given   1    -none-                   logical  \n## sp.given   1    -none-                   logical  \n## quantile   1    -none-                   logical\n# Map geographically-weighted Median\ntm_shape(atl.ss$SDF) + \n  tm_fill(c('pctPOV_Median', 'pctNOHS_Median', 'pctMOVE_Median', 'pctOWNER_OCC_Median'),\n          style = 'quantile') +\n  tm_borders()\n# Map geographically-weighted IQR\ntm_shape(atl.ss$SDF) + \n  tm_fill(c('pctPOV_IQR', 'pctNOHS_IQR', 'pctMOVE_IQR', 'pctOWNER_OCC_IQR'),\n          style = 'quantile') +\n  tm_borders()"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"calculating-pseudo-p-values-for-smoothed-estimates","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.4.4 Calculating pseudo p-values for smoothed estimates","text":"motivation much disease mapping detection spatial heterogeneity spatial dependence epidemiologic data. Spatial heterogeneity statistical parameter means values truly different locations compared others. Spatial dependence random variable means values one location tend correlated values nearby locations values distant locations.related idea spatial stationarity implies value summary data (e.g. spatially-local mean) location independent. words divided study region 10 equal-sized sub-regions, mean value approximately . contrast, spatial non-stationarity means local summaries location-dependent. example estimate spatially local mean different one sub-region compared another.Note spatial non-stationarity implies heterogeneity parameter interest (values everywhere) spatial dependence observations (near values correlated distant values). heterogeneity, dependence expect, average, local summaries statistics still stationary.can restate definitions form testable hypotheses evaluate disease mapping analysis. First note example multiple candidate random variables (e.g. pctPOV, pctMOVE, ICE_INCOME_all, …), well multiple candidate statistical parameters (e.g. mean, median, SD, IQR, …). hone questions hand, let us assume interested describing mean value random variable pctPOV. use kernel density functions applied spatial data particularly well suited testing spatial stationarity versus spatial non-stationarity statistic parameters.noted , question spatial stationarity hinges largely presence spatial dependence versus spatial independence observed values. Therefore, null hypothesis, \\(H_0:\\), might posit observed values pctPOV independent one another, therefore spatially local mean estimate pctPOV location independent (e.g. summary one location , average, summary another location). alternative hypothesis, \\(H_A:\\), values pctPOV spatially dependent therefore spatially local mean estimate pctPOV location dependent (e.g. equal single global value, every location-specif value).can test hypothesis? seen previously, hypothesis testing spatial data made challenging complex structure data, difficulty making plausible assumptions conventional statistical rules.One effective empirical solution complexity carry Monte Carlo permutation testing null hypothesis. idea permutation testing, can empirically simulate believe data look like null distribution. can compare observed data simulated null distribution describe unusual observations , given expected due chance alone.Permutation testing particularly well-suited questions spatial independence versus spatial dependence, hard conceive means data values independent one another. example, imagine random variable, pctPOV measured 345 units, . null hypothesis spatial independence, geographic location measures irrelevant. example took exact vector \\(n=345\\) values pctPOV randomly changed geographic location, matter assume, null, geographic location irrelevant. randomly reassign vector \\(n=345\\) values pctPOV different locations many, many times begin see distribution arrangements null hypothesis, e.g. spatial independence.permutation testing ?First, measure interest case geographically-weighted average variable, say pctPOV. region spatially weighted average calculated weighted average ’s neighbors (defined kernel).null, assume value individual region’s pctPOV independent value value neighbors. Therefore, permutations empirical way approximate null assumption randomly re-assigning known values different geographic locations.time randomly reassign set locations, repeat process creating geographically weighted average variable, e.g. pctPOV. lot times, distribution geographically weighted pctPOV individual region look like null (spatial independence) true.can compare single observed realization geographically-weighted pctPov region long list hypothetical values (null) see typical unusual observed data . Essentially pseudo p-value just rank-ordered percentile observed data relation range values null.number random permutations guides precision eventual pseudo-p-value. p-value theoretically smaller null permutations. instance compare \\(n=1\\) observed realization data \\(n=99\\) null permutations extreme statement make data extreme \\(p = \\frac{1}{100} = 0.01\\) level. contrast \\(n=1\\) observed realizations \\(n=999\\) random permutations null extreme data \\(p = \\frac{1}{1000} = 0.001\\).function gwss.montecarlo() . specific context geographically weighted summary statistics, function follows 3-step process:First randomly reassign location variables interest \\(n\\) times (\\(n\\) specified user, typically reasonably large)Second, random permutation random variable (e.g. pctPOV), summary statistic (e.g. spatially-weighted local mean pctPOV) calculated.Finally, observed results (e.g. spatially-weighted mean pctPOV calculated using original gwss()) compared null distribution. calculated \\(n=999\\) random permutations, \\(n=1000\\) versions summarized statistic, including observed. pseudo p-value calculated number times spatial unit observed value extreme expected null. example defined extreme something happens fewer 5% time random chance alone, might classify observed value extreme (thus significant) observed value either less lower 2.5% null values, greater upper 97.5% null values.might sound like lot things happening. Mechanically relatively straightforward procedure, can time consuming, particularly lot permutations null. carry Monte Carlo permutation test geographically-weighted statistics single variable, pctPOV. arguments familiar, now must specify many permutations simulations wish using arguments nsim =. computer, took less 1 minute run \\(n=499\\) permutations. Note request \\(n=499\\) simulations, combined observed data, \\(n=500\\) total values spatially-weighted mean value pctPOV compare.can now examine result. First might find returned class matrix. notice columns 5 primary summary statistics estimated gwss_LM local mean_LSD local standard deviation_LVar local variance_LSKe local skewness_LCV local coefficient variationThe numbers column values range 0 1. numbers percentiles reflecting rank location single spatially weighted local mean pctPOV observed data compared \\(n=499\\) versions spatial location randomly assigned.calculate 2-side pseudo p-value conventional 0.05 threshold, interested census tracts observed data either lowest 2.5% highest 2.5% null distribution. words ask census tracts observed spatially-weighted local mean value extreme compared happen chance alone.NOTE:percentile values come specific set randomly distributed simulations. Repeating procedure produce slightly different values printed simply due random variation. based Central Limit Theorem, believe number simulations grows larger, consistent results .However result returned object p.val easy use just . convert something map? , can test census tracts extreme definition, make new spatial object includes significant tracts.code use sp function aggregate() similar way might use dplyr function group_by() sf class data: merge dissolve adjacent polygons share value. case merging census tracts p-values certain threshold order create bold outline map.aggregate() function requires action function (FUN) applied groups tracts merged together. care output function, just need put something . Somewhat arbitrarily choose function length works well numeric character data. However, pay attention value returned; instead care shape resulting polygon.Now can use results create map summarizing evidence relation null hypothesis geographically-weighted mean value pctPOV stationary, alternative hypothesis least locations significantly extreme local values expected null.see visual inspection geographically-weighted mean pctPOV suggests great deal spatial heterogeneity apparent spatial non-stationarity. However, permutation test suggests regions far North Fulton West Atlanta values extreme might expect assumption spatial independence.NOTE:important remember hypothesis testing! test whether poverty rate zero, test whether poverty rate different specific census tracts compared others.Instead, specifically test whether spatial dependence data give rise unexpectedly extreme local measures assumptions KDE specified neighbors.","code":"\np.val <- gwss.montecarlo(atl, vars = 'pctPOV', \n                         adaptive = T,\n                         bw = 35,\n                         nsim = 499)\n\nsummary(p.val)##    pctPOV_LM        pctPOV_LSD      pctPOV_LVar      pctPOV_LSKe    \n##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0020  \n##  1st Qu.:0.2480   1st Qu.:0.2480   1st Qu.:0.2480   1st Qu.:0.2460  \n##  Median :0.5040   Median :0.5040   Median :0.5040   Median :0.4920  \n##  Mean   :0.5001   Mean   :0.5002   Mean   :0.5002   Mean   :0.4992  \n##  3rd Qu.:0.7600   3rd Qu.:0.7480   3rd Qu.:0.7480   3rd Qu.:0.7580  \n##  Max.   :0.9980   Max.   :0.9980   Max.   :0.9980   Max.   :0.9980  \n##    pctPOV_LCV    \n##  Min.   :0.0020  \n##  1st Qu.:0.2460  \n##  Median :0.5180  \n##  Mean   :0.5013  \n##  3rd Qu.:0.7540  \n##  Max.   :0.9980\n# First, create TRUE/FALSE vectors testing whether column 1 (pctPOV_LM) is extreme\n# I am using 2 significance levels: 90% and 95%\nsig95 <- p.val[, 1] < 0.025 | p.val[, 1] > 0.975\nsig90 <- p.val[, 1] < 0.05 | p.val[, 1] > 0.95\n\n# Second create a spatial object that ONLY contains significant tracts\natl.sig95 <- atl[sig95, ] %>% \n  aggregate(dissolve = T, FUN = length) # this is sp code to merge adjacent tracts\n\natl.sig90 <- atl[sig90, ] %>%\n  aggregate(dissolve = T, FUN = length)\ntm_shape(atl.ss$SDF) + \n  tm_fill('pctPOV_LM', \n          style = 'quantile',\n          title = 'Local Average % Poverty') + \n  tm_borders() + \ntm_shape(atl.sig90) +\n  tm_borders(lwd = 2, col = 'blue') + \ntm_shape(atl.sig95) + \n  tm_borders(lwd = 2, col ='red') +\ntm_add_legend(type = 'line',\n              labels = c('p < 0.05', 'p < 0.10'),\n              col = c('red', 'blue'))"},{"path":"disease-mapping-iv-kernel-density-estimation.html","id":"estimating-geographically-weighted-bivariate-statistics","chapter":"Week 7 Disease Mapping IV: Kernel Density Estimation","heading":"7.4.5 Estimating geographically-weighted bivariate statistics","text":"final bits information examine geographically-weighted summary statistics function gwss() bivariate correlations covariances. time two variables supplied gwss() function, automatically calculate correlation coefficients (Pearson Spearman), well measures covariance, every pair variables.now, seen KDE function can produce smoothed estimate local mean, median, SD, etc. can also show whether correlation pairs variables spatially stationary (everywhere), spatially non-stationary (varies location).Two things illustrated maps.First, appears magnitude correlation among pairs variables larger areas smaller others.second, spatial patterns correlation pctPOV two variables distinct. words areas correlation relatively stronger weaker .might ask whether differences extreme might expect null hypothesis spatial independence spatial stationarity.Previously conducted Monte Carlo permutation test single variable, pctPOV. provide two variables gwss.montecarlo() function, get pseudo p-values univariate bivariate statistics. NOTE takes time work computer . code took little 1-minute computer.can use summary() dimnames() figure column want. want get permutation p-value Spearman_rho_pctPOV.pctMOVE, 13th column matrix.Now can map correlation pctPOV pctMOVE along significance test.","code":"\ntm_shape(atl.ss$SDF) +\n  tm_fill(c('Spearman_rho_pctPOV.pctNOHS',\n            'Spearman_rho_pctPOV.pctMOVE',\n            'Spearman_rho_pctPOV.ICE_INCOME_all'),\n          style = 'quantile') +\n  tm_borders()\np.val <- gwss.montecarlo(atl, vars = c('pctPOV', 'pctMOVE'), \n                         adaptive = T,\n                         bw = 35,\n                         nsim = 499)\n# First, create TRUE/FALSE vectors testing whether column 1 (pctPOV_LM) is extreme\n# I am using 2 significance levels: 90% and 95%\nsig95 <- p.val[, 13] < 0.025 | p.val[, 13] > 0.975\nsig90 <- p.val[, 13] < 0.05 | p.val[, 13] > 0.95\n\n# Second create a spatial object that ONLY contains significant tracts\natl.sig95 <- atl[sig95, ] %>% \n  aggregate(dissolve = T, FUN = mean) \n\natl.sig90 <- atl[sig90, ] %>%\n  aggregate(disolve = T, FUN = mean)\ntm_shape(atl.ss$SDF) + \n  tm_fill('Spearman_rho_pctPOV.pctMOVE', \n          style = 'quantile',\n          title = 'Local correlation Poverty\\n& Residential instability') + \n  tm_borders() + \ntm_shape(atl.sig90) +\n  tm_borders(lwd = 2, col = 'blue') + \ntm_shape(atl.sig95) + \n  tm_borders(lwd = 2, col ='red') +\ntm_add_legend(type = 'line',\n              labels = c('p < 0.05', 'p < 0.10'),\n              col = c('red', 'blue'))"},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"spatial-structure-and-clustering-i-morans-i-and-lisa","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","text":"","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"getting-ready-6","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.1 Getting Ready","text":"","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"learning-objectives-7","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"additional-resources-7","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.1.2 Additional Resources","text":"Waller L, Gotway C. Applied Spatial Statistics Public Health Data. Hoboken, NJ: John Wiley & Sons, Inc; 2004. (Available electronically via library)","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"important-vocabulary-7","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 8","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"spatial-thinking-in-epidemiology-6","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.2 Spatial Thinking in Epidemiology","text":"notion clusters disease fundamental clue epidemiologists harkens back John Snow’s Cholera map, appearance excess cases near Broad Street pump pointed effective intervention, even absence etiologic knowledge causation. words, appeal cluster analysis can identify patterns potentially inform action.course ‘clustering’ grouping health status one group compared another basis associational etiologic epidemiology. interested whether disease occurs less often one group versus another (e.g. exposed versus unexposed). say lung cancer clusters among smokers, especially compared non-smokers.natural extend idea explicitly spatial frame, since often find populations geographically-referenced groups share spread exposures resources affect health populations.","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"two-big-questions","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.2.1 Two big questions…","text":"spatial epidemiology questions boil two fundamental questions related broad hypotheses:health population (measured risk, rate, prevalence, etc) spatially homogenous constant (\\(H_o\\) hypothesis), health spatially heterogeneous variable (\\(H_a\\) hypothesis)?occurrences health events, value health parameters spatially independent (\\(H_o\\) hypothesis) events values spatially dependent clustered space, given variation population risk (\\(H_a\\) hypothesis)?first question driving question disease mapping, fact complete disease homogeneity constant risk, spatial epidemiology fruitful endeavor. saw past several weeks, question homogeneity versus heterogeneity simple. instance several concerns drove many statistical methods learned date:True heterogeneity versus spurious heterogeneity arising statistical imprecision instability due small numbers (e.g. either rare outcomes producing small numerators, small populations producing small denominators).\nStatistical disease mapping strategies including Empirical Bayes, Full Bayes modeling, kernel density estimation\nStatistical disease mapping strategies including Empirical Bayes, Full Bayes modeling, kernel density estimationMeaningful heterogeneity versus heterogeneity driven spurious nuisance factors including confounding factors\nStandardizing expected counts rates distribution covariate (e.g. age) one strategy addressing spurious nuisance patterning\nStandardizing expected counts rates distribution covariate (e.g. age) one strategy addressing spurious nuisance patterningBiased estimates spatial risk patterns derived spatial scale (e.g. size aggregating units) spatial zoning (e.g. particular arbitrary boundaries used), described modifiable areal unit problem (MAUP).\nbest strategy avoid MAUP related bias rely meaningful units analysis. absence clearly meaningful aggregation approaches, comparing sensitivity results alternate scale zoning useful\nbest strategy avoid MAUP related bias rely meaningful units analysis. absence clearly meaningful aggregation approaches, comparing sensitivity results alternate scale zoning usefulThe second question seemingly similarly straightforward, upon investigation shares many caveats concerns seen . surface numerous statistical tests designed detect clusters events, presence spatial autocorrelation continuous value. However reliable valid detection epidemiologically meaningful spatial clusters threatened concerns (e.g. instability due sparse data; spurious patterns due nuisance factors), well several specific spatial dependence testing including:Defining meaningful alternative hypothesis, including specification spatial weights neighbors appropriate process handMultiple comparisons implicit local test dependence autocorrelationConceptualizing underlying process function spatial scale (e.g. ‘cluster’ due diffusion spread one region another, measured regions share exposure?)","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"why-does-structure-or-clustering-matter","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.2.2 Why does structure or clustering matter?","text":"spectrum epidemiologic investigation spans purely descriptive activities one end purely etiologic causal . middle great deal room hypothesis generating, predictive analyses, applied cluster analysis part disease response. Disease mapping can serve descriptive hypothesis generating purposes. Formal spatial cluster analysis employed following reasons:Exploratory cluster detection descriptive analysis disease risk/rate variation already established. can crude tool generating hypotheses risk factors health.Public health response outbreak cluster concern raised citizen complaints, surveillance. includes efforts definitively identify clusters disease cancer, birth defects, geographically-limited infectious disease outbreaks.Advancing etiologic population health knowledge testing excess geographic variation beyond explained known risk factors. can suggest novel spatially-patterned risk factors, unmeasured confounding, spatial selection processes, complex spatially-referenced interactions.health outcomes vary time space, possible clustering health events space, specific time periods, possibly dimensions simultaneously.Therefore, spatial cluster analysis one tool broader epidemiologic process understanding biological, behavioral, social production population health, extension acting promote health prevent disease. bottom line epidemiologic (population health) perspective, wide variety sophisticated statistical tools can, appropriately used, advance insight knowledge substitute critical thinking triangulation necessary robust impactful epidemiologic understanding.","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"making-meaning-of-hypotheses-in-spatial-structure-testing","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.2.3 Making meaning of hypotheses in spatial structure testing","text":"make meaningful inference spatial structure presence, absence, location, magnitude spatial clusters health parameters, critical clarity question asked, implied null alternative hypotheses.starting point, three broad categories testing presence spatial clustering dependence:Global dependence refers single test statistic summarizing pattern evident across entire dataset. , global Moran’s statistic one example approach. tests especially useful comparing ‘models’ aim explain spatial patterns; changes strength global autocorrelation across competing approaches can provide clues drivers generators dependence dataLocal dependence search existence sub-regions study area clustered auto correlated expected chance alone. global test returns single value test statistic, local tests clustering return separate test statistic unit analysis. strategy can useful identify unusual patterns high low rates group together; extension searches local dependence generate hypotheses explanations disease variation populationsFocal clustering targeted search excess risk disease pre-defined region. Typically focal test defined relation putative hazard, toxic emitter.effectively conduct meaningful cluster analysis, Waller & Jacquez (1995) suggest framework thinking components statistical testing clustering spatial epidemiology.Statistically can define null hypotheses describe expectations spatial patterns points aggregated risks/rates assuming either spatially constant risk (spatial homogeneity intensity) spatial independence (spatially random distribution values across units). test data evidence observed data departs expectations null.Statistically can define null hypotheses describe expectations spatial patterns points aggregated risks/rates assuming either spatially constant risk (spatial homogeneity intensity) spatial independence (spatially random distribution values across units). test data evidence observed data departs expectations null.Epidemiologically, questions best-suited spatial cluster analysis concern presence , ultimately reasons , epidemiologically unusual spatial patterns disease occurrence. , makes pattern epidemiologically unusual interesting? words might statistical test significant helpful, least point view epidemiologic investigation?Epidemiologically, questions best-suited spatial cluster analysis concern presence , ultimately reasons , epidemiologically unusual spatial patterns disease occurrence. , makes pattern epidemiologically unusual interesting? words might statistical test significant helpful, least point view epidemiologic investigation?One challenge cluster testing strategies tend calibrated identify particular cluster morphology, clusters can occur variety shapes, sizes, intensities. example detect boundaries (e.g. zones rapid change; boundary overlap exposure outcome), detect outliers, use neighbors, circles ellipses define index sub-regions evaluation. two cluster statistics introduce week rely ‘template’ clustering defined choice unit analysis (e.g. specific scale zoning data aggregated) definition neighbors.move forward, additional concepts worth defining fully:Distribution cluster statistic null spatial model Just needed null spatial model theoretical computational, also need characterize distribution cluster statistic null order compare observed data","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"spatial-analysis-in-epidemiology-5","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3 Spatial Analysis in Epidemiology","text":"choice cluster statistic depends greatly nature spatial data parameters interest. example different strategies used data point referenced versus aggregated areal units, differences parameter statistic interest naturally continuous value might conceived normally distributed versus numerator denominator pairing representing Poisson rate.section introduce Global Local Moran’s statistics clustering low birthweight Georgia counties. Moran’s typically makes sense areal data. originally developed normally distributed data, extensions Poisson distributed data available.","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"packages-data-1","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3.1 Packages & Data","text":"addition familiar packages, use packages specific cluster statistics including DCluster spdep functions relevant spatial cluster analysis.terms data, continue use Georgia low birth weight (VLBW) data, counts aggregated scale county.","code":"\npacman::p_load(tidyverse, # Provides data manipulation and piping functionality\n               sf,        # Read/write spatial data\n               spdep,     # Functions for creating spatial weight, spatial analysis\n               DCluster,  # Package with functions for spatial cluster analysis\n               tmap)      # Mapping functionality\nvlbw <- st_read('ga-vlbw.gpkg') %>%\n  mutate(rate = VLBW / TOT )\n\nr <- sum(vlbw$VLBW) / sum(vlbw$TOT)\nvlbw$expected <- r*vlbw$TOT"},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"global-morans-i","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3.2 Global Moran’s I","text":"numerous tests global spatial auto-correlation including Geary’s \\(C\\) test, Getis-Ord \\(G^*\\) statistic others. focus commonly used, Moran’s statistic. discussed lectures, Moran’s statistic similar Pearson’s correlation coefficient, takes product differences two measures respective mean quantify dependence similarity. case Pearson’s correlation coefficient, two measures (e.g. \\(x,y\\)) two constructs (e.g. one variable, \\(x\\) second variable, \\(y\\)). However case Moran’s , two measures construct contrast index county (e.g. \\(x_i\\)), compared average value county’s neighbors (e.g. \\(w_{ij}x_j\\) \\(w_{ij}=1\\) regions neighbors, \\(w_{ij}=0\\) otherwise. Thus, spatial weights matrix \\(w_{ij}\\) encodes spatial relationships places.Moran’s statistic:\\[=\\frac{\\sum\\limits_{=1}^{n} \\sum\\limits_{j=1}^{n} w_{ij} (Y_i-\\bar Y)(Y_j-\\bar Y)}{ \\sum\\limits_{=1}^{n} \\sum\\limits_{j=1}^{n} w_{ij}} \\left( \\frac{1}{\\frac{1}{n} \\sum\\limits_{=1}^{n} (Y_i-\\bar Y)^2} \\right)\\]formula \\(\\) references count regions \\(1:n\\), weights matrix, \\(w_{ij}\\) represents relatedness pairs regions. example, regions neighbors, weight \\(1\\), weight \\(0\\). \\(\\bar Y\\) mean value parameter.Global Moran’s considers values observed pairs. adjacent pairs simultaneously (simultaneously ) average, contributes positive value test statistic. one region average , contributes negative value (e.g. opposites inverse correlation).null hypothesis Moran’s measures spatially independent, setting test statistic zero. However positive spatial autocorrelation, test statistic positive (generally greater 1), suggestive neighboring counties similar distant counties. unusual circumstances, Moran’s test statistic can negative (e.g. index county opposite neighbors).REMEMBER:null assumption spatial independence imply spatial homogeneity constant risk. can different rates counties still spatial independence. Rejecting null spatial independence suggests high low rate regions particular spatial pattern structure.First set spatial weights matrix represent starting alternative hypothesis defined local spatial relationships. Queen contiguity relationship specified , although definitions represent additional alternative hypotheses assumption spatial independence. Notice function, need neighbor object (class nb) converted another format called neighborhood list spatial weights (e.g. class listw).","code":"\nqnb <- poly2nb(vlbw)\nq_listw <- nb2listw(qnb, style = 'W') # row-standardized weights"},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"global-morans-i-for-normally-distributed-variable","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3.2.1 Global Moran’s I for normally distributed variable","text":"many kinds measures variables might choose carry spatial auto-correlation analysis. example, measures epidemiologic exposures (access care, environmental toxicants, built environment) might represented continuous, normally distributed values. spatial regression module discuss assessment spatial autocorrelation multivariable regression model residuals another use (e.g. diagnose residual spatially-structured errors); also expected normally distributed.following function, moran.test(), package spdep, takes single continuous value. case parameter interest risk, first version Moran’s statistic treats observed risk value equal, without considering differences sample size among counties.function allows consideration either theoretical null spatial model (e.g. assumes variance \\(\\) statistic consistent asymptotic normal distribution specified options randomization = FALSE), null spatial model computational (e.g. assume variance \\(\\) statistic normal, empirically approximated random permutations). specify computational null indicating randomisation = TRUE. can read arguments information returned function looking document help.Moran’s statistic \\(0.21\\), small p-value suggests evidence moderate spatial autocorrelation, result highly statistically significant.visualize mortality rate county compares county’s neighbors can use moran.plot().plot demonstrates correlation relationship county’s VLBW risk (x-axis) average risk county’s spatial neighbors (y-axis; assuming Queen contiguity). dotted lines divide plot quadrants representing mean value rate, spatially-lagged rates neighbors. solid line slope correlation (corresponds 0.21).Thinking quadrant turn can help make sense plot, interpretation spatial autocorrelation statistic. example, look Randolph county (labeled upper-right quadrant). county characterized higher average VLBW risk state (e.g. right side average x-axis), also average VLBW risk among Randolph county’s Queen contiguity neighbors higher average (e.g. average y-axis).none labeled, also counties left lower quadrant. points represent individual counties lower average risk VLBW, surrounded neighboring counties also lower (average) risk.Finally points ‘-diagonal’ quadrants: upper-left counties risk lower average (e.g. small values x-axis), surrounded counties (average) higher average risk (e.g. large values y-axis).clues clustering occurs, remember global Moran’s provides us one statistical test value entire state Georgia. question asked : “spatial autocorrelation VLBW risk Georgia” answer question “likely, yes”. However test explicitly quantify likelihood specific location clusters might .","code":"\nmoran.test(vlbw$rate, \n                 listw = q_listw, \n                 randomisation = T)## \n##  Moran I test under randomisation\n## \n## data:  vlbw$rate  \n## weights: q_listw    \n## \n## Moran I statistic standard deviate = 4.5281, p-value = 2.975e-06\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##       0.214197175      -0.006329114       0.002371817\nmoran.plot(x = vlbw$rate, \n           listw = q_listw, \n           labels = vlbw$NAME, \n           xlab = 'VLBW Risk', \n           ylab = 'Spatially-lagged mean risk')"},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"global-morans-i-for-poisson-distributed-rate-data","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3.2.2 Global Moran’s I for Poisson-distributed rate data","text":"many measures may appropriately assessed normality assumptions previous Global Moran’s , general disease rates best assessed way. rates may normally distributed, also variance rate likely differs different size population risk. example previous test assumed level certainty rate county, fact counties sparse data (high variance) others adequate data (relatively lower variance).short, one reason use Gaussian assumptions analyzing health data, instead using binomial, Poisson, distributions model disease occurrence. Luckily, extensions global Moran’s treat measures Poisson (negative binomial) rather normal.package DCluster several wrapper functions work moran.test() function spdep accommodate alterations. usual, wise use ?moranI.test() look help documentation detail.Note, addition arguments n=159 S0 = Szero(q_listw) now required function. n= argument specifies number regions, whereas S0 = argument calculates global sum weights. specifically helper function Szero() calculates several constants needed various autocorrelation statistics calculations.Also note instead using calculated rate, specify county outcome log(expected) offset. can also specify number random simulations null hypothesis run order calculate empirically p-value.surprisingly, Moran’s statistic virtually identical got previous example (\\(=0.21\\)). empirical p-value \\(p=0.002\\) still significant \\(\\alpha = 0.05\\), substantially larger p-value calculated Moran’s test run assumption normality. reflects variation precision among counties, now least partially accounted .","code":"\nmoranI.test(VLBW ~ offset(log(expected)), \n                  data = vlbw,\n                  model = 'poisson',\n                  R = 499,\n                  listw = q_listw,\n                  n = 159,\n                  S0 = Szero(q_listw))## Moran's I test of spatial autocorrelation \n## \n##  Type of boots.: parametric \n##  Model used when sampling: Poisson \n##  Number of simulations: 499 \n##  Statistic:  0.2141972 \n##  p-value :  0.002"},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"global-morans-i-after-empirical-bayes-smoothing","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3.2.3 Global Moran’s I after Empirical Bayes smoothing","text":"Given concern relatively rare outcome, births counties, might also concerned whether rates well-estimated. moranI.test() function helped address unequal variance resulting sparse compared populous counties, directly address reliability estimate. However possible combine disease mapping tool Empirical Bayes smoothing Moran’s statistic get reliable estimate global spatial auto-correlation. EBImoran.mc() function spdep package us.case, Moran’s statistic remains similar, adding confidence belief test value simply function extreme outliers due sparse data. mentioned , relatively larger p-value derive (appropriate) use Poisson rather Gaussian distribution, also limits precision permutation empirical p-values compared asymptotic p-values.","code":"\nebm <- EBImoran.mc(n = vlbw$VLBW,\n                   x = vlbw$TOT,\n                   listw = q_listw,\n                   nsim = 499)\nprint(ebm)## \n##  Monte-Carlo simulation of Empirical Bayes Index (mean subtracted)\n## \n## data:  cases: vlbw$VLBW, risk population: vlbw$TOT\n## weights: q_listw\n## number of simulations + 1: 500\n## \n## statistic = 0.20882, observed rank = 500, p-value = 0.002\n## alternative hypothesis: greater"},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"local-morans-i","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3.3 Local Moran’s I","text":", interest importance global tests spatial autocorrelation describing overall structure spatial dataset, diagnosing confirming underlying assumptions (e.g. assumption spatial independence). However identification (mapping) specific local clusters regions represent hot spots (grouped-areas high risk) cold spots (grouped-areas low risk) often motivation spatial cluster analysis; purview local tests spatial auto-correlation.Local Moran’s simply decomposition global test individual values part summation. Unfortunately, many caveats comes using Local Indicators Spatial Association (LISA) statistics.\\[I_i = \\frac{(Y_i-\\bar Y)}{\\sum\\limits_{j=1}^{n}(Y_j-\\bar Y)^2 / (n-1)} \\sum\\limits_{j=1}^{n}w_{ij}(Y_j-\\bar Y)\\]formula simply specification \\(\\) statistic sub-region study area. Together, \\(I_i\\) sum \\(\\). However, additional wrinkles break overall \\(\\) statistic sub-parts.First, global tests, parametric assumptions variance plausible, assuming reasonably large number regions (e.g. n=159). However local test, want make inference separately region (county example), case trying test whether 1 county correlated handful neighbors. words, N quite small test, making asymptotic (e.g. ‘large-number’) tests invalid.addition, testing county turn, raising concerns multiple comparisons corresponding Type error (e.g. whether false positives terms statistical significance).One thing keep mind LISA statistics often selected identify disease clusters. p-value null-hypothesis statistical test whether clustering sufficiently unusual called significant. efficient way test whether clustering global tests; use local tests therefore seen light descriptive exploratory, although shall see, adjustments multiple comparisons reduce concern Type error.","code":""},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"local-morans-i---localmoran","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3.3.1 Local Moran’s I - localmoran()","text":"first approach use basic localmoran() function spdep. method treats variable interest normally distributed continuous variable; reasonable exposure covariate measures, cases reasonable disease rates values relatively similar levels variance (e.g. regions sufficiently large population sizes).previous version spdep built-option calculate multiple comparison corrections local moran’s . current version, function removed localmoran(), can still looking help documentation function ?p.adjustSP.contents returned localmoran() function matrix five columns separate values \\(n=159\\) rows (representing counties data).Ii local Moran’s test statistic.E.Ii expected value Moran’s statistic null. Note values , close zero.Var.Ii variance local Moran’s statistic.Z.Ii standardized deviation (e.g. ‘z-score’) local Moran’s statisticPr(Z>0) p-value (case adjusted multiple comparison using false discovery rate method).","code":"\nlm1 <- localmoran(x = vlbw$rate,\n                  listw = q_listw) \n\n\ndim(lm1)      # dimensions of object returned## [1] 159   5\nsummary(lm1)  # summary of object returned##        Ii                E.Ii                 Var.Ii               Z.Ii        \n##  Min.   :-1.55827   Min.   :-0.09571301   Min.   :0.0000016   Min.   :-2.8908  \n##  1st Qu.:-0.03078   1st Qu.:-0.00651091   1st Qu.:0.0120165   1st Qu.:-0.3002  \n##  Median : 0.06534   Median :-0.00204478   Median :0.0656044   Median : 0.4763  \n##  Mean   : 0.21420   Mean   :-0.00632911   Mean   :0.1805685   Mean   : 0.4952  \n##  3rd Qu.: 0.34255   3rd Qu.:-0.00047898   3rd Qu.:0.1983378   3rd Qu.: 1.2909  \n##  Max.   : 2.69958   Max.   :-0.00000003   Max.   :2.6484359   Max.   : 4.5459  \n##  Pr(z != E(Ii))     \n##  Min.   :0.0000055  \n##  1st Qu.:0.1376159  \n##  Median :0.3463825  \n##  Mean   :0.4130179  \n##  3rd Qu.:0.7236363  \n##  Max.   :0.9972931"},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"plotting-a-cluster-map","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3.3.2 Plotting a Cluster Map","text":"many options visualizing results including plotting Local Moran’s statistic value \\(I_i\\) county, mapping p-value, combination. However common approach implemented free GeoDa software, adopted ESRI ArcGIS software produce map representing county respect four quadrants Moran’s plot.\nFIGURE 8.1: LISA scatterplot typology\nHigh-High regions statistically significant positive values local \\(I_i\\) statistic, higher--average rates index regions neighborsLow-Low regions statistically significant positive values local \\(I_i\\) statistic (positive values \\(I_i\\) represent similar values, whether similarly high similarly low), lower--average rates index regions neighborsHigh-Low regions statistically significant negative values local \\(I_i\\) statistic (\\(I_i\\) negative index neighbors inversely related), index region higher--average rates, neighbors lower--average ratesLow-High counties statistically significant negative values local \\(I_i\\) statistic, index county lower--average rates, neighbors higher--average ratesNon-significant regions non-significant values \\(I_i\\) statistic symbolized neutral color.practice, can simply use p-value localmoran() test identify specific regions statistically significantly different expectations null. among ‘significant’ regions can assign quadrant based two aspects risk rate:standardized (e.g. z-score value) VLBW rate countyThe weighed average standardized rate county’s neighbors, weights function neighbor definition. function lag.listw() computation us; accepts vector measures plus listw class description neighbors. returns vector ‘spatially lagged’ averaged values variable interest.spatial lag?prior examples used spatial neighbor definitions create spatial weights matrix. weights matrix used form spatial lag incorporation information adjacent units compare index unit.example, lag.listw() simply gives us tools spatially lag variable interest. Specifically, use goes county county, index region, uses spatial weights calculate average neighbors region. useful now using information rate index county rate neighbors assign quadrants.future examples use lag.listw() function test whether exposure neighboring regions correlated outcome index county, question spatial diffusion.summarize, code accomplished following steps:Assign new column vlbw dataset named lag. variable (produced lag.listw() function) average VLBW prevalence neighbors (defined Queen contiguity case) countyAssign new column vlbw datasets named lm_pv (local moran’s p-value). done needed information one object, vlbw data set.Within chained series dplyr steps produce new data object, vlbw_lm, following steps occur:\nStandardize raw rate zero mean value unit represents one standard deviation\nStandardize spatially lagged rate\nRecode every county according three factors: ) VLBW risk county larger smaller average (e.g. higher lower zero)?; ii) VLBW risk among spatial neighbors county larger smaller average?; iii) local Moran’s test statistic significant \\(\\alpha = 0.05\\) adjusting multiple comparisons false discovery rate method?\nNote recode, line TRUE/FALSE statement. answer FALSE option final option county assigned value 'Non-significant'.\nStandardize raw rate zero mean value unit represents one standard deviationStandardize spatially lagged rateRecode every county according three factors: ) VLBW risk county larger smaller average (e.g. higher lower zero)?; ii) VLBW risk among spatial neighbors county larger smaller average?; iii) local Moran’s test statistic significant \\(\\alpha = 0.05\\) adjusting multiple comparisons false discovery rate method?Note recode, line TRUE/FALSE statement. answer FALSE option final option county assigned value 'Non-significant'.Using case_when() multi-category recodeIn code use dplyr function called case_when(). function efficient way work typically contained long chain ifelse() statements. syntax test left-hand-side, tilde (~) connecting, new assigned variable test TRUE right-hand-side.created re-coded variable, can now plot results. palette used custom specification colors, color represented HEX value (alphanumerical system cataloging color spectrum). colors roughly produced GeoDa ArcGIS, colors suitable categorical data (compared sequential data) appropriate.\napparent ‘High-High’ clusters, single county ‘Low-Low’, counties categorized spatial outliers (e.g. ‘High-Low’ ‘Low-High’).‘cluster’ 1?map unitary ‘clusters’ low-low high-high.Remember cluster defined intersection whether region unusual whether neighbors unusual. process carried separately county, quite plausible single region situated among similar-risk regions categorized local cluster, ’s neighbors may sufficient evidence say neighbors also similarly extreme.","code":"\n# For convenience, give an easier name to the 5th column, representing p-value\nnames(lm1)[5] <- 'pvalue'\n\n# create lagged local raw_rate - in other words the average of the queen neighbors value\nvlbw$lag <- lag.listw(q_listw, var = vlbw$rate)\nvlbw$lm_pv <- lm1[,5]\n\n# Create a new dataset that includes standardized values, and then creates a new variable\n# 'lm_quad' which takes on the above categorical values.\nvlbw_lm <- vlbw %>%\n  mutate(raw_std = as.numeric(scale(rate)), # scale means standardize to mean 0, 1 SD\n         lag_std = as.numeric(scale(lag)),\n         lm_quad = factor(case_when(  # All of this is assigning labels based on values\n           raw_std >= 0 & lag_std >= 0 & lm_pv < 0.05 ~ 'High-High',\n            raw_std <= 0 & lag_std <= 0 & lm_pv < 0.05 ~ 'Low-Low',\n            raw_std <= 0 & lag_std >= 0 & lm_pv < 0.05 ~ 'Low-High',\n            raw_std >= 0 & lag_std <= 0 & lm_pv < 0.05 ~ 'High-Low',\n            lm_pv >= 0.05 ~ 'Non-significant'),\n           levels = c('High-High','Low-Low','Low-High','High-Low','Non-significant')))\ntm_shape(vlbw_lm) +\n  tm_fill('lm_quad',\n          style = 'cat',\n          palette = c(\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\", \"#ffffb3\"),\n          title = 'Cluster category') +\n  tm_borders()"},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"local-morans-i---localmoran.exact","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3.3.3 Local Moran’s I - localmoran.exact()","text":"Another approach local spatial auto-correlation question, use exact tests account small-n problem test.approach, must supply regression model object test done model-fitted values. help documentation specifies must lm() object (e.g. linear model), offsets allowed.One way get linear regression account difference population--risk weight observation (e.g. region) weight proportion total state population births risk. Also note, localmoran.exact() uses spatial neighbors objects, qnb, rather previously used spatial neighbors weights list, q_listw.localmoran.exact() function returns complex object localmoran(), ’ll see use summary(). print() function extracts useful information plotting:Now object ready plotting using strategy categorizing counties LISA typology, mapping. introduce one last strategy, plot three versions side side .","code":"\n# Create a vector of weights that reflect the relative population (births) size in each county\nwts <- vlbw$TOT / sum(vlbw$TOT) * 159\n\n# Fit a weighted linear regression model of the raw (observed) rates\nreg1 <- lm(rate ~ 1, \n           data = vlbw,\n           weights = wts)\n\n# Carry out the Exact Moran's I test on the model object\nlm2 <- localmoran.exact(reg1, nb = qnb)\n# This just converts the output to a more useful 'matrix' format\nlm2 <- print(lm2)\n\n# Assign the exact p-value to the vlbw data object\nvlbw_lm$pvalExact <- lm2[,3]"},{"path":"spatial-structure-and-clustering-i-morans-i-and-lisa.html","id":"local-morans-i-with-constant-risk-assumption","chapter":"Week 8 Spatial Structure and Clustering I: Moran’s I and LISA","heading":"8.3.3.4 Local Moran’s I with constant risk assumption","text":"functions built spdep provide helpful approaches describing local spatial auto-correlation, reason extend (Global Moran’s tests ) allow Poisson-distributed case event counts population risk. words don’t allow us use constant risk hypothesis number events can expected based simply reference rate local population risk.functions automated, Bivand, et al2 provide code adapted Waller & Gotway, adapted .assume already variable expected product overall rate times population county reflects expected count VLBW babies county given number births county followed constant risk (e.g. state overall average).designation steps CR refers constant risk Poisson based hypothesis local autocorrelation.hand-made local Moran’s constant risk Poisson assumption, values \\(I_i\\), variance, indicator significance. constant risk assumption, can simulate null hypothesis generating random Poisson counts, based local expected values, comparing observed event counts simulated distribution.conduct simulation must set parametersThe code simulates distribution null hypothesis constant risk. words, simulated geographic location value, instead amount variation rate county expected simply due chance small numbers.step testing significance local dependence, instead test evidence observed rates substantively ‘unusual’ compared expectation.next steps serve organize observed simulated null results order characterize whether counties \\(I_i\\) statistic calculated Step 3 significantly different expected Poisson variation.Now combine p-values different strategies single dataset facilitate mapping.plot compares three versions:can see overlap among methods, also notable variations, noticeable difference final simulated version constant Poisson risk. standard exact approach remain time run analysis, random simulation, constant risk approach change instance, particular, changes increase nsim parameter simulations null hypothesis.Overall seems relatively consistent evidence clustered high risk Southwest Georgia, although exact counties included varies. One argue third map best accounts dual concerns data sparseness leading spurious conclusions, concerns extreme clusters need , ‘unusual’ statistically.also evidence (especially exact constant risk maps) clustered low-risk counties North Georgia. Although discrepancy among methods disconcerting two things kept mind interpreting local clustering analysis.Local cluster detection exploratory process useful description, hypothesis generation, guiding investigation. Caution taken strong inference individual counties versus hotspots, instead using tools identify evidence regional extremes.LISA statistics (cluster strategies) rely heavily null-hypothesis significance testing. know epidemiologists much weight can put arbitrary achievement test statistic moving across largely meaningless threshold value. align maps past maps Disease Mapping modules, counties highlighted high low surprising. differences among strategies relies statistical efficiency, power detect ‘true’ deviations, alignment real world statistical assumptions. Therefore, use results tools understanding cautious using strong decision tools.","code":"\n# Step 1 - Create the standardized deviation of observed from expected\nsdCR <- (vlbw$VLBW - vlbw$expected) / sqrt(vlbw$expected)\n\n# Step 2 - Create a spatially lagged version of standardized deviation of neighbors\nwsdCR <- lag.listw(q_listw, sdCR)\n\n# Step 3 - the local Moran's I is the product of step 1 and step 2\nvlbw_lm$I_CR <- sdCR * wsdCR\n# Step 4 - setup parameters for simulation of the null distribution\n\n# for a random simulation, we often set a seed for random number generator\nset.seed(123)\n\n# Specify number of simulations to run\nnsim <- 499\n\n# Specify dimensions of result based on number of regions\nN <- length(vlbw$expected)\n\n# Create a matrix of zeros to hold results, with a row for each county, and a column for each simulation\nsims <- matrix(0, ncol = nsim, nrow = N)\n# Step 5 - Start a for-loop to iterate over simulation columns\nfor(i in 1:nsim){\n  y <- rpois(N, lambda = vlbw$expected) # generate a random event count, given expected\n  sdCRi <- (y - vlbw$expected) / sqrt(vlbw$expected) # standardized local measure\n  wsdCRi <- lag.listw(q_listw, sdCRi) # standardized spatially lagged measure\n  sims[, i] <- sdCRi * wsdCRi # this is the I(i) statistic under this iteration of null\n}\n# Step 6 - For each county, test where the observed value ranks with respect to the null simulations\nxrank <- apply(cbind(vlbw_lm$I_CR, sims), 1, function(x) rank(x)[1])\n\n# Step 7 - Calculate the difference between observed rank and total possible (nsim)\ndiff <- nsim - xrank\ndiff <- ifelse(diff > 0, diff, 0)\n\n# Step 8 - Assuming a uniform distribution of ranks, calculate p-value for observed\n# given the null distribution generate from simulations\nvlbw_lm$pvalCR <- punif((diff + 1) / (nsim + 1))\nvlbw_lm2 <- vlbw_lm %>%\n  mutate(lm_quad_exact = factor(case_when(\n    # First, recode the LISA quadrant values using exact test\n           raw_std >= 0 & lag_std >= 0 & pvalExact < 0.05 ~ 'High-High',\n            raw_std <= 0 & lag_std <= 0 & pvalExact < 0.05 ~ 'Low-Low',\n            raw_std <= 0 & lag_std >= 0 & pvalExact < 0.05 ~ 'Low-High',\n            raw_std >= 0 & lag_std <= 0 & pvalExact < 0.05 ~ 'High-Low',\n            pvalExact >= 0.05 ~ 'Non-significant'),\n           levels = c('High-High','Low-Low','Low-High','High-Low','Non-significant')),\n    # Now recode the LISA quadrant values using the constant-risk simulation\n          lm_quad_CR = factor(case_when(\n           raw_std >= 0 & lag_std >= 0 & pvalCR < 0.05 ~ 'High-High',\n            raw_std <= 0 & lag_std <= 0 & pvalCR < 0.05 ~ 'Low-Low',\n            raw_std <= 0 & lag_std >= 0 & pvalCR < 0.05 ~ 'Low-High',\n            raw_std >= 0 & lag_std <= 0 & pvalCR < 0.05 ~ 'High-Low',\n            pvalCR >= 0.05 ~ 'Non-significant'),\n           levels = c('High-High','Low-Low','Low-High','High-Low','Non-significant')))\ntm_shape(vlbw_lm2) + \n  tm_fill(c('lm_quad', 'lm_quad_exact', 'lm_quad_CR'),\n          style = 'cat',\n          palette = c(\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\", \"#ffffb3\"),\n          title = c('Standard LISA', 'Exact LISA', 'Constant Risk')) +\n  tm_borders() +\n  tm_layout(legend.position = c('RIGHT','TOP'),\n            inner.margins = c(0.02,.02,0.1, 0.1))"},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"spatial-structure-and-clustering-ii-spatial-scan-statistics","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","text":"","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"getting-ready-7","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.1 Getting Ready","text":"","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"learning-objectives-8","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"additional-resources-8","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.1.2 Additional Resources","text":"Vignette scanstatistics package RSatScan website, supported National Cancer Institute","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"important-vocabulary-8","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.1.3 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 9","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"spatial-thinking-in-epidemiology-conceptual-tools-for-thinking-about-clusters","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.2 Spatial Thinking in Epidemiology: Conceptual tools for thinking about ‘clusters’","text":"Last week formalized two essential questions spatial epidemiology:spatial heterogeneity variation intensity disease within study area?spatial dependency autocorrelation disease rate among local sub-regions overall study area?two questions recalled contrast made prior weeks global patterns local patterns:Global: characterization patterns intensity autocorrelation entire study region.Local: characterization specific deviations expectation sub-regions study area.combine two constructs, describing global heterogeneity ask, “areas meaningfully different constant risk null hypothesis?” Describing global autocorrelation ask, “average values region correlate values neighboring regions?”extension, describing local heterogeneity detect existence local extreme (e.g. intensity significantly /meaningfully higher lower expected assumptions constant risk). Similarly, describing local autocorrelation detect specific sub-regions unusually similar (perhaps unusually unlike) neighbors.","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"the-clustering-conundrum","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.2.1 The clustering conundrum","text":"preceding summary points covered past weeks gives us starting point analytically statistically detecting describing spatial clustering disease. discussed last week, interest statistically (e.g. rule bias error sources unusual patterning), epidemiologically (e.g. characterize occurrence disease part surveillance etiologic research), policy public health perspective (e.g. inform public health prevention, regulation, policy).constructs help detection unusual patterns, explaining (e.g. terms causes, processes, exposures) often ultimate goal. exactly ‘disease clustering’ statistical evidence global local heterogeneity dependency tell us generation causes clustering?turns notion explaining ‘clustering’ tricky one conceptually analytically.Conceptually, might like distinguish least two kinds processes:Factors context population results greater smaller intensity disease one place versus another. example environmental toxicants air pollution arsenic water plausibly affect live area, resulting higher disease prevalence. UV-exposure varies latitude, partly explains differences skin cancer Vitamin D deficiency. Spatial variation caused changes underlying property places populations called 1st order effects.Processes spread, contagion, diffusion suggest interaction people (institutional influence within places) result spread transmission disease. intuitive infectious disease, transmission function proximity. contagion diffusion can occur non-infectious outcomes well, seen behavioral contagion social norms shared within networks (e.g. acceptability smoking, expectations body size, etc). Spatial variation clustering caused interaction individuals entities called 2nd order effects.\nFIGURE 2.1: Image source: https://mgimond.github.io/Spatial/index.html\nfigure illustrates 1st 2nd order spatial effects context ecology. core concept relevant epidemiology 1st order effects assume patterns differences mean intensity, whereas 2nd order effects focuses differences covariation correlation.might recognize, therefore, parallel two essential questions heterogeneity mean intensity versus autocorrelation among local sets regions. may also note analytic strategies framed context distinctions: tests spatial heterogeneity built null hypothesis constant risk sound like evaluating 1st order effects; contrast tests spatial autocorrelation comparing pairs regions sound like evaluating 2nd order effects.Analytically great test distinguishes clearly competing explanations unusual spatial pattern generated produced. second tricky issue. 1st 2nd order processes can produce patterns disease detected either tests heterogeneity tests spatial autocorrelation. Said another way, tests analytically distinguish unusual pattern caused; instead complementary ways describe magnitude location patterns.end day, spatial epidemiologists must design studies select tools best serve needs question hand. detecting describing clustering primary goals (e.g. surveillance description), combination disease mapping cluster detection may beginning end work.However, characterizing underlying causes processes important – either scientific understanding effective public health action – tools just steps way. Disease mapping cluster detection may generate hypotheses, lead additional investigation, used triangulate data build fuller picture.","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"spatial-scan-statistics","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3 Spatial Scan Statistics","text":"family scan statistics commonly used identifying localized spatial clusters disease. specific examples statistical approaches broad category attributed Besag & Newell, Openshaw (Geographical Analysis Machine), Kulldorff & Nagarwalla.discussion focuses primarily implementation latter set tests. Many users take advantage stand-alone free software called SaTScan carry tests. software can downloaded : https://www.satscan.org/download_satscan.html. site also rich set tutorial technical documentation resources.part widespread use SaTScan software, less development scan statistics R. reason, functions used examples many helper functions wrappers ’ve previous examples. result, tutorial mix spatial analysis hacking way output R coding.","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"packages-data-2","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.1 Packages & Data","text":"use several familiar, one new package lab: scanstatistics.2021 scanstatistics updated R4.0Unfortunately developer package moved onto things continued maintain package versions R evolved. Thus package longer hosted CRAN repositories. lab install older version directly Github. older version seems perform well, least examples.terms data, using new dataset example. Specifically counts reported sexually transmitted infections (STIs; includes chlamydia, gonorrhea, chancroid, syphilis) county Georgia, along population count risk. data exist cross-sectional version, pooling counts 2018, n=159 rows 159 counties.However, also spatio-temporal dataset STIs county year 2010-2018. data long format means row every year county (e.g. repeated rows within counties).new data, simple map STI rate 2017-18 years pooled.","code":"\nlibrary(sf)        # manage sf class data \nlibrary(dplyr)     # facilitates data processing\nlibrary(tmap)      # for thematic mapping\nlibrary(SpatialEpi) # Functions including the kulldorff()\nlibrary(ggplot2)  # Create a ggplot visualization\n# cross-sectional STI data\nsti <- st_read('ga-std-2017-18.gpkg')\n\n# calculated overall global risk of STI\nr <- sum(sti$STD) / sum(sti$POP)\n\n# use the global risk and the population of each county to calculate local expected \nsti$expected <- r*sti$POP\n\n# longitudinal STI data\nsti_long <- st_read('ga-std-long.gpkg')\nsti %>%\n  mutate(rate = STD / POP * 1000) %>%\n  tm_shape() + \n  tm_fill('rate',\n          style = 'quantile',\n          palette = 'YlGnBu',\n          title = 'STI per 1000') +\n  tm_layout(main.title = 'STI per capita, Georgia, 2018',\n            inner.margins = c(0.1, 0.02, 0.02, 0.08)) +\n  tm_credits('Source: GA-OASIS, https://oasis.state.ga.us/') +\n  tm_borders()"},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"overview-of-kulldorff-nagarwalla-scan-statistic","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.2 Overview of Kulldorff & Nagarwalla scan statistic","text":"Scan statistics get name approach detection clusters flexible manner ‘scanning’ entire study region many different possible windows observation. basic analytic strategy Kulldorff scan statistic follows several steps:Define single location (e.g. centroid polygon, regularly-placed grid points across region)location, define radius window defining area local location. radius typically varied iteratively zero (e.g. single location included), something large (perhaps large necessary include 50% population). Scan statistics typically use circular window, although ellipses shapes possible.location \\(x\\), many possible window-radii (e.g. window surrounding reference point), aggregate count events population risk (alternatively expected count events constant risk hypothesis) inside (e.g. \\(p\\)) outside (e.g. \\(q\\)) window.Calculate likelihood ratio test whether rates/risks equal (\\(H_0: p=q\\)), risk inside window larger (\\(H_1: p>q\\)).Repeat four steps every iteration window-radius every location study region.first thing apparent, null assumption tested constant risk spatial homogeneity risk assumption, rather spatial independence assumption. clear fact assessing correlation values, magnitude risk/rate inside versus outside region. illustrates testing spatial autocorrelation (e.g. Moran’s LISA) way conceive clusters.also apparent results large number test statistics, raises concern multiple comparisons Type error. Kulldorff’s approach, however, suggests interested set statistically significant test statistics (many chance alone given number tests conducted), instead interested identifying single (perhaps ) -likely cluster(s).priori restricting interest -likely cluster, eliminate concern multiple comparison. possible -likely cluster fact statistically significant, (e.g. spatial homogeneity, likely still interesting!).-possible locations \\(x\\) window-radius tested, can also choose look secondary clusters, recognizing list unusual test statistics, greater risk Type error.strengths Kulldorff scan statistic flexibility respect defining ‘local’, straightforward evaluation whether risk within window versus outside. potential limitations clusters (e.g. along highways producing linear pattern) may readily detected circular elliptical search windows.","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"spatio-temporal-scan-statistics","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.3 Spatio-temporal scan statistics","text":"strategy described purely spatial scan, relatively straightforward extend strategy include spatio-temporal scans. Obviously work, data must available multiple time-periods within every region.spatio-temporal scans, simply add another dimension defined window. window centered given location (e.g. centroid county), given radius, include varying-number time-periods. instance, two iterations test centered spatial location, window-size, one might include single time-period, another includes two time-periods.comparison resulting test statistics tells us whether count events inside spatio-temporal window (spatio-temporal ‘column’) one time-period different spatio-temporal window two time-periods. Instead scanning circle traveling across map, might imagine scanning column tube height varying define different numbers time periods.example illustration , conventional cross-sectional scan statistic simply move two-dimensional window around map. spatio-temporal window third dimension reflecting maps stacked top one another.\nFIGURE 2.6: Image source: https://www.mdpi.com/1999-4907/11/4/454/htm\n","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"estimating-spatial-only-kulldorff-scan-statistics","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.4 Estimating spatial-only Kulldorff scan statistics","text":"kulldorff() function package, SpatialEpi relatively easy way implement spatial-scan statistics.","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"prepare-data","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.4.1 Prepare data","text":"Look help documentation function, kulldorff(). function currently written directly use spatial object R (e.g. object class sf sp), instead must supply matrix \\(x,y\\) coordinates representing centroids area region (county scenario). centroids used iterative specification study window, determining versus window whether county-centroid falls circle.First need \\(x,y\\) coordinates centroid county matrix form. following code achieves .","code":"\nsti_cent <- st_centroid(sti) %>%\n  st_coordinates()\nhead(sti_cent)##            X       Y\n## [1,] 1057695 1255760\n## [2,] 1363356 1098009\n## [3,] 1095793 1279833\n## [4,] 1367529 1010385\n## [5,] 1225492 1068308\n## [6,] 1279077 1097468"},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"call-kulldorff-function","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.4.2 Call kulldorff() function","text":"several decisions make using scan statistic. First, maximum size window wish search? can specified using contextual knowledge large small clusters anticipated . absence priori knowledge size, common practice allow windows vary zero size large enough include 50% population risk within window.addition, must set number Monte Carlo simulations null hypothesis (e.g. simulations distribution counts constant risk hypothesis). discussed previously, need adequately large number iterations approximate distribution happen chance alone null.precision (number significant digits) resulting p-value limited number iterations. specify n=499 null iterations, added single test observed data produces n=500 versions test. inference based unusual single test observed data relation 499 tests null.Finally, specification alpha.level dictates () secondary clusters retained. -likely cluster reported matter significance, secondary clusters retained smaller alpha threshold. purposes exploration set alpha 0.2.plot produced default (suppress plot specify plot = FALSE), shows histogram simulated null distribution log-likelihood ratios contrast rates inside versus outside assuming constant risk hypothesis.words, permutation simulation applied average risk actual population assuming simple Poisson distribution. histogram therefore includes information likelihood ratios \\(n=499\\) simulations null. addition simulated permutations \\(n=1\\) actual observed likelihood ratio single significant cluster identified -possible scans. case single significant cluster indicated red line. quite evident cluster highly unusual null assumption, empirical p-value = 0.002.","code":"\nk1 <- kulldorff(sti_cent, \n                cases = sti$STD,\n                population = sti$POP,\n                expected.cases = sti$expected,\n                pop.upper.bound = 0.5,\n                n.simulations = 499,\n                alpha.level = 0.2)"},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"summarize-results","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.4.3 Summarize results","text":"Unfortunately, handy function providing pretty summary, much information contained within object. examining specific aspects result can learn great deal. First, note object produced, k1 list meaning composed several sub-parts. can see names parts like :begin looking .likely.cluster component, several sub-parts:can see STI rate inside cluster two half times higher rate outside cluster (e.g. SMR contrasting rates inside versus outside 2.54).can also look similar information secondary clusters, clusters second-highest log-likelihood ratio.object secondary.clusters list, element list containing information just reviewed .likely.cluster. words can see p-value, SMR, list counties contributing secondary clusters. code-snippet use base-R function sapply() extract 5th element (SMR) 8th element (p.value) secondary cluster k1$secondary.clusters. (Alternatively used k1$secondary.clusters[[1]]$p.value, k1$secondary.clusters[[2]]$p.value get values). use format() function specify number digits print.First, notice log-likelihood ratios two secondary clusters substantially smaller -likely cluster (nearly 6000!). addition SMR’s p-values vary.","code":"\n# See the elements returned by function - explore them!\nnames(k1) ## [1] \"most.likely.cluster\" \"secondary.clusters\"  \"type\"               \n## [4] \"log.lkhd\"            \"simulated.log.lkhd\"\n# See the row-numbers for the counties in the most-likely cluster\nk1$most.likely.cluster$location.IDs.included##  [1] 138 142 111 137 135  76  23   8 159 130  24 123 131  90  85  46 157 136 141\n## [20] 129 122  99 145  83  29 104  74 153 152  96  35  95  45  78 100  62  93  88\n## [39] 105  69  98  32 128 140  38  49  12   5 158 119 151  34  75 125  53 102  91\n## [58] 144 132  18  25  89  81  80  72 124  48  26  19 139 120  52  63 154  97 121\n## [77] 117  84   6  36  37  50 134  79 156  59  68  82  66  94 147   1\n# See the SMR for the most-likely cluster\nk1$most.likely.cluster$SMR## [1] 1.255823\n# See the observed and expected cases inside cluster\nk1$most.likely.cluster$number.of.cases## [1] 55699\nk1$most.likely.cluster$expected.cases## [1] 44352.6\n# see how many additional clusters reported:\nlength(k1$secondary.clusters)## [1] 4\ntibble(\n  SMR = format(sapply(k1$secondary.clusters, '[[', 5), digits = 3), # this gets SMR's\n  lik = format(sapply(k1$secondary.clusters, '[[', 6), digits = 2), # this gets log-likelihoods\n  pval = format(sapply(k1$secondary.clusters, '[[', 8), digits = 3)) %>%# this gets p.values\n  kableExtra::kable(align = 'c')"},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"plotting-results","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.4.4 Plotting results","text":"Just isn’t handy function summarizing results, also isn’t handy function plotting results. result little work see clusters . step--step strategy creating variable Georgia county STI dataset indicates whether county cluster.First, initialize new variable called k1_cluster. initialize simply means create without values (e.g. set NA).Now fill value new variable according whether county given cluster. Recall row-numbers counties included -likely cluster contained vector k1$.likely.cluster$location.IDs.included. Therefore, can use row indices say counties assigned cluster 1 (-likely cluster).approach can used extract location.IDs.included secondary clusters. simply loop (using (1:length(x))) across however many secondary.clusters name .result mappable variable:several things apparent map. First foremost, STI rate portion state colored red substantially higher outside portion, cluster including counties largest likelihood ratio. addition huge cluster, handful secondary clusters reached threshold significance \\(\\alpha = 0.2\\).","code":"\nsti$k1_cluster <- NA\nsti$k1_cluster[k1$most.likely.cluster$location.IDs.included] <- 'Most likely cluster'\nfor(i in 1:length(k1$secondary.clusters)){\nsti$k1_cluster[k1$secondary.clusters[[i]]$location.IDs.included] <- paste(\n  'Secondary cluster ', i, sep = '')\n}\ntm_shape(sti) +\n  tm_fill('k1_cluster',\n          style = 'cat',\n          textNA = 'Not in cluster',\n          palette = 'Set1',\n          title = '') + \n  tm_borders() + \n  tm_layout(legend.outside = T)"},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"estimating-spatio-temporal-kuldorff-scan-statistic","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.5 Estimating spatio-temporal Kuldorff scan statistic","text":"R, currently one package readily permits spatio-temporal scan testing, scanstatistics. actually implements somewhat limited version temporal component: assess cluster duration varies, least current iteration, cluster duration go last period back. words data end 2017, consider 2017, 2017+2016, 2017+2016+2015. However (apparently) consider intervals time middle study period (e.g. 2015+2016 including 2017). unlike Kulldorff’s implementation free software Sat Scan.examples relatively complex looking R code. provided might try adapt code projects. However please note expect everyone ‘learn’ ‘remember’ code details. important high level concepts concern interpretation scan statistics.","code":""},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"preparing-data","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.5.1 Preparing data","text":"data sti_long spatio-temporal object include STI counts population risk every Georgia county year 2008-2017. words 10 rows every county.several ways present data scanstatistics functions, easiest probably sf data frame. However variables must follow specific naming protocol (see help documentation).","code":"\nsti_scan2 <- sti_long %>%\n  mutate(count = STD,       # event variable must be labeled 'count'\n         location = GEOID, # region id must be labeled 'location'\n         population = POP, # denominator must be labeled 'population\n         time = as.integer(YEAR))%>%  # time-period must be labeled 'time'\n  dplyr::select(time, location, count, population) "},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"prepare-geographic-window-zones","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.5.2 Prepare geographic window zones","text":"package also unique way defining areas contained within varying-sized windows, called zones context. approach first defines k-nearest neighbors order locate regions connected one another. using measures distance region k-nearest neighbors, varying-sized windows applied. result set zones consist county plus neighbors starting zero-distance (neighbors) maximum number neighbors defined.Although data long format (e.g. multiple years every geographic region, county), zones calculated object region represented .exactly function ? First try looking data class object length:object class list, can examine individual list elements better understand output (just randomly chose elements):Notice element list called zones vector row-id’s. words zones represents every iteration location x window-size calculated preceding procedure. become important comes time plot results.","code":"\n## 2022 - This code outdated -- changes in progress ##\nzones <- sti %>%\n  st_centroid() %>% # convert polygons to centroid\n  st_coordinates() %>% #converts sf object to matrix of x, y locations\n  spDists(x = ., y = ., longlat = FALSE) %>%\n  dist_to_knn(k = 50) %>% # distance up to the 50 nearest neighbors\n  knn_zones() # convert into zones needed for scanstatistics based on distances\n## 2022 - This code outdated -- changes in progress ##\nclass(zones)## [1] \"function\"\nlength(zones)## [1] 1\n## 2022 - This code outdated -- changes in progress ##\nzones[[34]]\nzones[[657]]"},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"estimate-spatio-temporal-scan-statistics","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.5.3 Estimate spatio-temporal scan statistics","text":"several scan statistic options package including reliance Poisson assumptions versus Negative binomial, allowing estimation population denominators, expected counts. implementation population-denominator version Poisson scan (note take minute run):basic summary information tells us row-ID’s likely spatio-temporal cluster, 9 year period, -likely duration cluster fact 9-years (e.g. 2010-2018). words cluster STI’s quite persistent time!","code":"\n## 2022 - This code outdated -- changes in progress ##\n\n#k2 <- scan_pb_poisson(sti_scan2, \n#                      zones = zones,\n#                      n_mcsim = 499)\n#print(k2)"},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"visualizing-most-likely-clusters","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.5.4 Visualizing most-likely clusters","text":"package scanstatistics function extracting -likely clusters, can visualize location, explore duration intensity .First, use function top_clusters() extract information. case asking top 5 clusters, specifying want non-overlapping.see top_clusters() produced, look object:expected 5 rows object, row tells us something respective cluster.first thing tells us spatial zone involved. Remember element object zones vector neighbors? zone number refers particular neighbor set highest likelihoods containing clustered deaths.addition object top5 tells us likely duration temporal dimension cluster. single likely spatio-temporal cluster described zone 154, 9 years duration; fifth likely cluster described spatially zones 501, temporally 8-years duration.get information zones sf object mapping can use functions package purrr efficient way process vectors contained lists. looking zones defined top5$zone, using number, extracting vector row-ids original zones object. produces list involved counties.new object, top5_counties list length 5, element list vector county names.Now can use list names populate new variable sf dataset. ’ll iterating elements list, top5_counties, apply cluster ID, cluster duration, cluster score, related log-likelihood.","code":"\n## 2022 - This code outdated -- changes in progress ##\ntop5 <- top_clusters(k2, zones, k = 5, overlapping = FALSE)\n## 2022 - This code outdated -- changes in progress ##\ntop5\n## 2022 - This code outdated -- changes in progress ##\n# First, get vector of county names\ncounty <- as.vector(sti$NAME)\n\n# Find the counties corresponding to the spatial zones of the 5 clusters.\ntop5_counties <- top5$zone %>%\n  purrr::map(get_zone, zones = zones) %>%\n  purrr::map(function(x) county[x])\n## 2022 - This code outdated -- changes in progress ##\n\nfor(i in 1:length(top5_counties)){\n  cluster <- top5_counties[[i]]\n  sti$cluster[sti$NAME %in% cluster] <- i\n  sti$c_score[sti$NAME %in% cluster] <- top5$score[i]\n  sti$c_duration[sti$NAME %in% cluster] <- top5$duration[i]\n}"},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"mapping-top-5-clusters","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.5.5 Mapping top-5 clusters","text":"4 clusters asked 5? investigation clear likely cluster second -likely fully overlap (despite option disallow overlapping clusters!).may also note cluster locations look quite different cross-sectional analysis 2018 data! search spatio-temporal clustering can turn distinct patterns observed single year. clusters rise top strongest evidence significantly unusual.wanted also produce visualization time-span clusters, use ggplot2 . First create variable start end time (end times assumed last year, 2017).clusters represent persistently high levels entire 9-year period, much distinction example. However, Cluster 5 stand apparent beginning study period.","code":"\n## 2022 - This code outdated -- changes in progress ##\n\ntm_shape(sti) + \n  tm_fill('cluster',\n          style = 'cat',\n          palette = 'Set1') +\n  tm_borders()\n## 2022 - This code outdated -- changes in progress ##\n\n# Assign a cluster number called 'order'\ntop5$order <- 1:nrow(top5)\n# Calculate start/end years from cluster duration\ntop5$end <- 2018\ntop5$start <- 2018 - top5$duration\n\n# Create ggplot\ng <- ggplot(top5, aes(x = start, y = order, col = as.factor(order)))\ng + geom_segment(aes(yend = order), xend = 2019, size = 2) + \n  geom_point(size = 3) + \n  labs(x = 'Cluster timing',\n       y = 'Cluster number',\n       col = 'Cluster') "},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"mapping-relative-scores","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.5.6 Mapping relative scores","text":"county varying probability versus cluster. function (takes long time run!) calculates, average statistic space-time window location included. words, averages statistic zones maximum duration. reason , quantify (visualize) relative likelihood location part cluster.Now can map relative score, interpreting relative likelihood county member cluster:shows , using 9-year time series, variation likelihood counties part true cluster, highest probability around likely cluster. notable patterns spatio-temporal clustering relatively distinct patterns point--time cross-sectional clustering using 2018 data .","code":"\n## 2022 - This code outdated -- changes in progress ##\n\n# Note: This step takes awhile...about 4-5 minutes on my computer\n#county_scores <- score_locations(k2, zones)\n## 2022 - This code outdated -- changes in progress ##\n\n# This part goes quicker - first just rename some stuff for merging\nsti_scan3 <- county_scores %>% \n  mutate(NAME = county) %>%\n  left_join(sti, by ='NAME') %>%\n  st_as_sf() # this just converts the new object back to 'sf'\n## 2022 - This code outdated -- changes in progress ##\n\ntm_shape(sti_scan3) +\n  tm_fill('relative_score') + \n  tm_borders()"},{"path":"spatial-structure-and-clustering-ii-spatial-scan-statistics.html","id":"concluding-thoughts","chapter":"Week 9 Spatial Structure and Clustering II: Spatial scan statistics","heading":"9.3.6 Concluding thoughts","text":"scan statistics represent one tool useful investigating existence location spatial clusters disease events. case scan statistics, clustering specifically local excess risk violation assumption spatial homogeneity constant risk. therefore distinct measures spatial auto correlation Moran’s statistic.Scan statistics useful finding statistically significant -likely cluster, exploring secondary clusters. extension spatio-temporal setting another feature. information can compliment learned tests spatial auto correlation, global local test discussed last week.","code":""},{"path":"spatreg1.html","id":"spatreg1","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","text":"","code":""},{"path":"spatreg1.html","id":"getting-ready-8","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.1 Getting Ready","text":"","code":""},{"path":"spatreg1.html","id":"learning-objectives-9","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"spatreg1.html","id":"important-vocabulary-9","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.1.2 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 10","code":""},{"path":"spatreg1.html","id":"spatial-thinking-in-epidemiology-7","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.2 Spatial Thinking in Epidemiology","text":"","code":""},{"path":"spatreg1.html","id":"multivariable-regression-for-exploring-spatial-data","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.2.1 Multivariable regression for exploring spatial data","text":"Multivariable regression magic. just fancy correlations estimation sample means. statistical tool takes noisy variable data, smooths reduces summary statistics hope interpretable trying make meaning raw data alone.Multivariable regression methods useful epidemiologists often want: smoothed summaries distill trends features (hopefully) give us clues true process.several motivations using multivariable regression spatial data:descriptive spatial analysis, may interested producing adjusted estimates conditioned multiple covariates (e.g. age strata, socioeconomic status, background/nuisance environmental features). possible produce indirectly adjusted estimates discussed Disease Mapping, becomes challenging higher data-dimensionality (variables). example use multiple variables predict disease count (rate) region function covariates.exploration diagnosis aspatial model performance, may want evaluate whether regression models explain spatial auto correlation, whether undiagnosed dependency (spatial autocorrelation) residuals apparent conventional aspatial residual diagnostic plots.etiologic spatial analysis, may interested estimating conditional associations interpreted causal effect estimates certain circumstances including adjustment causally confounded pathways. motivation aspatial modeling extends spatial least two scenarios:\nInterest association predictor outcome, conditional covariates, concern residual nuisance spatial auto correlation unaddressed bias estimates\nInterest association predictor outcome, conditional covariates, spatial interaction spillover causal process.\nInterest association predictor outcome, conditional covariates, concern residual nuisance spatial auto correlation unaddressed bias estimatesInterest association predictor outcome, conditional covariates, spatial interaction spillover causal process.","code":""},{"path":"spatreg1.html","id":"data-generating-process","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.2.2 Data Generating Process","text":"critical importance effective use regression aspect epidemiology, including spatial, theorizing hypothesizing data generating process. simply phrase describing (possibly unknown) mechanisms world collectively generated events gave rise data sampled observed. causal epidemiology, often use directed acyclic graphs (DAGs) illustrations models possible data generating processes.reason bring concept point, process spatial patterns generated central interest spatial epidemiology. health events (e.g. incident cancer, influenza, diabetes) homogeneous constant space – heterogeneous purely random independent one another – might see less value tools spatial epidemiology gain insight.However, observe spatial structure patterns (including extremes heterogeneity dependence) disease health status, unlikely patterns just ‘sprung ’ reason. words, rarely physical location specific latitude longitude sole explanation higher lower risk disease. Instead spatial patterns health due spatial patterns causes health, data generating process. Therefore often wish dig deeper understand causes, describe accurately possible data generating process.","code":""},{"path":"spatreg1.html","id":"model-residuals-are-not-just-mistakes","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.2.3 Model residuals are not just mistakes","text":"likely recall biostatistics given random variable can define statistical error deviation specific observation’s measured value expected value, true mean underlying population. rarely know true mean entire population, use mean specific sample approximation. Therefore model residual difference observation sample mean.use word ‘error’ used contexts deviations presumed represent unknown, random, mistakes estimation perhaps sampling error (e.g. took sub-sample full population) simple random chance (e.g. ‘measurement’ error). sometimes treat errors random mistakes, often assume errors follow certain random distributions.example, linear regression assume model residuals normally distributed, – average – value zero. words, assume average observation fact equal sample mean, therefore zero difference . addition, residuals exactly equal expected value, positive (e.g. observed value larger expected), negative (observed value smaller expected), amount variability summarized \\(\\sigma^2\\).classical theory, modeled ‘mistakes’ residuals assumed show pattern consistent random chance:independent one another;vary around zero;follow expected distribution (e.g. normal distribution simple linear regression)., , mean pattern residuals appear expected? trained examine residual diagnostics regression models.Regression models approximate data generating processSpatial variation disease can explained spatial variation causes predictors disease. somewhat simplistic strategy begin explore data generating process follows:predicted value unconditional mean regression model (e.g. model intercept predictors) every observation average (e.g. prediction observation equals intercept). model residuals represent error difference observation global average.Putting residuals map describes pattern error unexplained variation. autocorrelation, might hypothesize missing variables part data generating process.fit one several additional models add hypothesized predictor data generating process. model, can make new prediction value observation, incorporating \\(\\beta\\) coefficient measured value predictor.Putting residuals conditional adjusted models map describes whether still spatial structure, accounted parts data generating process attributable exposures predictors model.basic logic - model residuals ‘absorb’ ‘describe’ unexplained variation health beyond expectations specified model, leveraged many analytic strategies spatial epidemiology. can use patterns regression model residuals clues well approximating data generating process. Specifically, often test whether residuals spatially independent (expected null hypothesis model errors), spatially dependent auto-correlated.words, putting regression residuals map spatial context provides whole new lens think data.","code":""},{"path":"spatreg1.html","id":"spatial-analysis-in-epidemiology-6","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3 Spatial Analysis in Epidemiology","text":"","code":""},{"path":"spatreg1.html","id":"spatializing-aspatial-regression","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.1 Spatializing aspatial regression","text":"first three weeks considering application multivariable regression spatial data. logical starting point considering can interpret conventional aspatial regression models become familiar biostatistics epidemiologic modeling coursework.","code":""},{"path":"spatreg1.html","id":"data-packages","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.1.1 Data & Packages","text":"tutorial, use vlbw dataset low birthweight prevalence Georgia counties example. Although identical outcome used prior examples, dataset also several contextual variables covariates. contextual variables selected proxies one hypothesized data generating process. Specifically, now following variables measured county:MCD: categorical variable designating county Maternity Care Desert area inadequate access outpatient inpatient women’s health care servicesPctPov: continuous measure percent population living federal poverty line (ranges 0 1)isolation: measure county-level Black-White residential racial segregation using Isolation Index (ranges 0 segregation 1 complete segregation).FI: Food Insecurity indexSocCap: Social Capital indexpctNOIns_Fem: proportion women without health insuranceThese added variables may explain differences risk low birth weight. known vary spatially, related population-level lifecourse economic opportunity, health status access health care, plausible (least hypothesized) contributors generation spatial structure dependence population VLBW.","code":"\npacman::p_load(tidyverse, # General data processing\n               sf,        # Read/write sf data\n               spdep,     # Moran's I and spatial neighbors functions\n               tmap,      # Mapping\n               MASS)      # Statistical package including function for studentized residuals\nvlbw <- st_read('ga-vlbw-covar.gpkg')  %>%\n  mutate(rate = VLBW / TOT)"},{"path":"spatreg1.html","id":"fitting-unconditional-empty-model","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.1.2 Fitting unconditional (empty) model","text":"discussed extensively benefits modeling ‘rate’ data numerator denominator counts arising Poisson, binomial, negative binomial distribution. values normally distributed, variance may different (heteroskedastic) across regions due different population size risk.However, approximation convert numerator denominator counts continuous ‘rate’ ratio, model using linear regression. partially account heteroskedasticity, weight county relative population (e.g. number births risk VLBW).Fitting linear regression models R straightforward. purposes, might first fit empty unconditional mean model. means regression model intercept, predictors. model essentially decomposes outcome global mean value (expected value), residuals represent difference county overall mean expectation.Look summary model. results sparse include predictors. intercept estimate modeled expectation global average risk VLBW (e.g. 0.018 1.8%). summary results also report basic numerical range model residuals. put residuals map, first let’s fit one model.","code":"\n# Create a vector of weights that reflect the relative population (births) size in each county\nwts <- vlbw$TOT / sum(vlbw$TOT) * 159\n\n# Fit a weighted linear regression model of the raw (observed) rates\nm0 <- lm(rate ~ 1, \n         data = vlbw,\n         weights = wts)\n\nsummary(m0)## \n## Call:\n## lm(formula = rate ~ 1, data = vlbw, weights = wts)\n## \n## Weighted Residuals:\n##        Min         1Q     Median         3Q        Max \n## -0.0149727 -0.0035211  0.0003131  0.0029493  0.0186953 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 0.018189   0.000436   41.72   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.005498 on 158 degrees of freedom"},{"path":"spatreg1.html","id":"fitting-conditional-model","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.1.3 Fitting conditional model","text":"previous model interesting, might add predictor variables. consider two measures social material context influence women’s health (poverty rate, pctPOV residential racial segregation, isolation), well two indicators health access including prevalence women county uninsured (pctNOIns_Fem) whether county designated Maternity Care Desert (MCD), meaning access OB/GYN midwives, hospitals labor delivery capacity.case, appears (non-spatial results), socio-material variables poverty rate segregation strongly correlated VLBW, conditional measures, independent association either prevalence uninsured living maternity care desert. carry routine regression diagnostics evaluate extreme values, leverage, normality residuals.","code":"\nm1 <- lm(rate ~ pctPOV + isolation + pctNOIns_Fem + MCD,\n         data = vlbw,\n         weights = wts)\nsummary(m1)## \n## Call:\n## lm(formula = rate ~ pctPOV + isolation + pctNOIns_Fem + MCD, \n##     data = vlbw, weights = wts)\n## \n## Weighted Residuals:\n##        Min         1Q     Median         3Q        Max \n## -0.0103096 -0.0023745  0.0003931  0.0026713  0.0105881 \n## \n## Coefficients:\n##                      Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         0.0074464  0.0017699   4.207 4.39e-05 ***\n## pctPOV              0.0275036  0.0068742   4.001 9.80e-05 ***\n## isolation           0.0114604  0.0015850   7.231 2.15e-11 ***\n## pctNOIns_Fem        0.0032358  0.0120122   0.269    0.788    \n## MCDLimited Access 1 0.0008278  0.0009327   0.888    0.376    \n## MCDNo Access        0.0009929  0.0013909   0.714    0.476    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.004154 on 153 degrees of freedom\n## Multiple R-squared:  0.4471, Adjusted R-squared:  0.429 \n## F-statistic: 24.74 on 5 and 153 DF,  p-value: < 2.2e-16"},{"path":"spatreg1.html","id":"mapping-residuals","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.2 Mapping residuals","text":"purpose extend examination model performance fit simply aspatial spatial context. first easy step attach residuals model (m0 representing deviations county VLBW rate overall mean; m1 representing deviation county value predicted covariates) data object map . Specifically, used weighted linear regression account unequal variance estimates among counties, extract studentized residuals residuals normalized variance. function package MASS calculating studentized residuals (e.g. studres()).many ways visualize data. example might simply interested seeing spatial distribution residuals, extremes well near zero. Using style = \"quantile\" accomplishes objective.Residuals represented diverging color paletteRemember residuals : deviations average expected value. predicted value truly average deviations positive, negative, zero (e.g. predicted).map values central value (e.g. zero case 1 case ratio measure like RR SMR), best use diverging color palette. specify RdYlGn, diverging.midpoint = 0 specifies value neutral central color.maps appears spatial clustering residuals empty model (m0) conditional model (m1). specifically, appear clustered negative residuals North Georgia (e.g. places model predicted higher rate observed) positive residuals Southwest Georgia (e.g. places observed VLBW higher predicted model).contrast map right appears (qualitatively least) less clustering county values. counties prediction better (e.g. closer zero), others actually seems worse (e.g. zero).","code":"\nvlbw$m0_resids <- studres(m0)\nvlbw$m1_resids <- studres(m1)\ntm_shape(vlbw) +\n  tm_fill(c('m0_resids', 'm1_resids'),\n          style = 'quantile',\n          palette = 'RdYlGn',\n          midpoint = 0) +\n  tm_borders()  +\n  tm_layout(legend.position = c('RIGHT','top'),\n            inner.margins = c(0.02, 0.02, 0.02, 0.1),\n            legend.format = list(digits = 1))"},{"path":"spatreg1.html","id":"morans-i-tests-on-lm-models","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.3 Moran’s I tests on lm models","text":"Recall two weeks ago, used global Moran’s statistic test spatial auto correlation. reminder, auto correlation refers dependency (correlation) value measure one place values neighbors place. absence spatial structure clustering, expect spatial independence (e.g. null hypothesis), therefore evidence spatial auto correlation suggests departure independence.linear regression assume conditional global mean (intercept), mean slope covariates (beta estimates), residual error normally distributed independently distributed. assumption can checked aspatially plots residuals, check can extended space applying Moran’s statistic model residuals.","code":""},{"path":"spatreg1.html","id":"creating-spatial-neighbors","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.3.1 Creating spatial neighbors","text":"Just past exercises, definition spatial neighbors can critically important, results often sensitive choice (e.g. choice represents different alternative hypothesis). definition neighbors definition units likely interact depend others. contiguous adjacent units (implied Queen contiguity), units within certain sphere influence, inverse distance relationship continuous space?use Queen contiguity neighbor definition ) convenient intuitive; b) commonly used spatial analysis. clear, choice, analyst always consider whether better alternatives.code chunk combines several steps: first creates nb neighbor object; , takes nb object converts listw object needed Moran’s .","code":"\nqnb_listw <- vlbw %>%\n  poly2nb() %>%\n  nb2listw()"},{"path":"spatreg1.html","id":"lm.morantest-function","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.3.2 lm.morantest() function","text":"section detecting general autocorrelation, introduced several function calculating global Moran’s including moran.test() moranI.test(). appropriate evaluating observed data series like observed VLBW rate. instance, residuals modeled estimates thus require different approach.evaluate spatial auto correlation residuals model use function lm.morantest() derivatives.possible, incorrect, extract residuals model (mapping) apply moran.test() directly.several observations make results :Moran’s evaluating degree spatial auto-correlation among residuals unconditional model, m0 0.22 (p < 0.001). words moderate clustering spatial dependence VLBW.Moran’s evaluating residuals conditional model (e.g. m1, specifically adjusted 4 variables described) 0.02 (p = 0.29).Looking back model summary model m1 see adjusted \\(R^2\\) 0.43. say four variables ‘explain’ , , -county variation VLBW.Together results suggest spatial patterns clustering largely explained four variables, non-spatial patterns -county differences fully explained.Said another way, model m1 describes possible data generating process lead spatial dependence clustering, model m1 fully describe data generating process spatial heterogeneity risk among countiesCorrelation causationWhether statistical patterns preceding results represent causal patterns versus spurious associations due confounding, selection, misclassification, random error requires knowledge insight beyond model output.instance possible set variables “explains” spatial dependence heterogeneity variables proxies (possibly unmeasured) causal structures.process – sequential models different variables included compared – exploratory approach understanding relationship predictor outcome variables, spatial patterning relationships. Variables – adjusted – decrease spatial auto correlation tapping aspect (proxy) reason clustering first place.course fact four variables ‘explain’ spatial autocorrelation equivalent four variables causal data generating process. evaluate causation need fully investigate threats causal inference including individual-level, ecologic-level, cross-level confounding.","code":"\nlm.morantest(m0, listw = qnb_listw)## \n##  Global Moran I for regression residuals\n## \n## data:  \n## model: lm(formula = rate ~ 1, data = vlbw, weights = wts)\n## weights: qnb_listw\n## \n## Moran I statistic standard deviate = 4.6616, p-value = 1.569e-06\n## alternative hypothesis: greater\n## sample estimates:\n## Observed Moran I      Expectation         Variance \n##      0.223521496     -0.005214700      0.002407732\nlm.morantest(m1, listw = qnb_listw)## \n##  Global Moran I for regression residuals\n## \n## data:  \n## model: lm(formula = rate ~ pctPOV + isolation + pctNOIns_Fem + MCD,\n## data = vlbw, weights = wts)\n## weights: qnb_listw\n## \n## Moran I statistic standard deviate = 0.56403, p-value = 0.2864\n## alternative hypothesis: greater\n## sample estimates:\n## Observed Moran I      Expectation         Variance \n##      0.016003102     -0.011353497      0.002352452"},{"path":"spatreg1.html","id":"morans-i-tests-on-glm-models","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.4 Moran’s I tests on glm models","text":"mentioned previous section, focus linear regression, underlying Gaussian probability distribution, contrast focus distributional assumptions generalized linear exponential family including Poisson Negative Binomial. One reason many statistical tools spatial auto correlation, developed fields accustomed normally-distributed continuous data, rather count binomial data common epidemiology.possible use tools like Moran’s statistic residuals GLM models? Well certainly reason cautious. may learned biostatistics (EPI III), residuals GLM model (e.g. logistic regression) behave like residuals linear model, part normally distributed may homoskedastic. Even link scale (e.g. logit log scale), differences.glm model families fully meet assumptions linear model Moran’s tests, following section seen purely exploratory.However, possible examine deviance residuals glm models, assess degree spatial auto correlation, locations better worse model fit. begin, fit unconditional conditional Poisson model estimate prevalence low birthweight county.Note longer weighting population (e.g. TOT), incorporated directly Poisson model offset.examine summary results, recall Poisson regression, coefficients log scale. \\(e^{intercept}\\) background prevalence/risk VLBW, \\(e^\\beta\\) represents relative excess prevalence/risk VLBW 1-unit increase predictor variable.","code":"\ng0 <- glm(VLBW ~ 1 + offset(log(TOT)), \n          data = vlbw,\n          family = poisson())\ng1 <- glm(VLBW ~ pctPOV + isolation + pctNOIns_Fem + MCD +\n            offset(log(TOT)), \n          data = vlbw,\n          family = poisson())\nsummary(g0)## \n## Call:\n## glm(formula = VLBW ~ 1 + offset(log(TOT)), family = poisson(), \n##     data = vlbw)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -4.00696    0.01476  -271.4   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 409.27  on 158  degrees of freedom\n## Residual deviance: 409.27  on 158  degrees of freedom\n## AIC: 1081.6\n## \n## Number of Fisher Scoring iterations: 4\nsummary(g1)## \n## Call:\n## glm(formula = VLBW ~ pctPOV + isolation + pctNOIns_Fem + MCD + \n##     offset(log(TOT)), family = poisson(), data = vlbw)\n## \n## Coefficients:\n##                     Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)         -4.63412    0.08267 -56.058  < 2e-16 ***\n## pctPOV               1.49965    0.29511   5.082 3.74e-07 ***\n## isolation            0.64721    0.07094   9.123  < 2e-16 ***\n## pctNOIns_Fem         0.24054    0.54309   0.443    0.658    \n## MCDLimited Access 1  0.04657    0.04188   1.112    0.266    \n## MCDNo Access         0.06040    0.06128   0.986    0.324    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 409.27  on 158  degrees of freedom\n## Residual deviance: 220.93  on 153  degrees of freedom\n## AIC: 903.31\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"spatreg1.html","id":"mapping-glm-residuals","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.5 Mapping glm residuals","text":"First extract deviance residuals glm objects:map :saw lm() models results , appears qualitatively slightly clustering dependence left (unconditional, m0) map compared right (conditional adjusted model, m1). Unlike previous set maps, appears magnitude deviation counties predicted (e.g. min max residuals) smaller adjusted model.","code":"\nvlbw$g0_resids <- resid(g0, type = 'deviance')\nvlbw$g1_resids <- resid(g1, type = 'deviance')\ntm_shape(vlbw) +\n  tm_fill(c('g0_resids', 'g1_resids'),\n          style = 'quantile',\n          palette = 'RdYlGn',\n          midpoint = 0) +\n  tm_borders()  +\n  tm_layout(legend.position = c('RIGHT','top'),\n            inner.margins = c(0.02, 0.02, 0.02, 0.1),\n            legend.format = list(digits = 1))"},{"path":"spatreg1.html","id":"morans-i-for-glm","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.6 Moran’s I for glm","text":"turns lm.morantest() function actually accept glm model object. mean use Moran’s deviance residuals glm model interpretable way expected (e.g. hypothesis testing discouraged), caution useful exploratory tool.Reassuringly, thee results using Poisson model quite consistent weighted linear regression model magnitude statistical significance Moran’s test statistic.","code":"\nlm.morantest(g0, listw = qnb_listw)## \n##  Global Moran I for regression residuals\n## \n## data:  \n## model: glm(formula = VLBW ~ 1 + offset(log(TOT)), family = poisson(),\n## data = vlbw)\n## weights: qnb_listw\n## \n## Moran I statistic standard deviate = 4.5915, p-value = 2.201e-06\n## alternative hypothesis: greater\n## sample estimates:\n## Observed Moran I      Expectation         Variance \n##     0.2284486189    -0.0002192915     0.0024803253\nlm.morantest(g1, listw = qnb_listw)## \n##  Global Moran I for regression residuals\n## \n## data:  \n## model: glm(formula = VLBW ~ pctPOV + isolation + pctNOIns_Fem + MCD +\n## offset(log(TOT)), family = poisson(), data = vlbw)\n## weights: qnb_listw\n## \n## Moran I statistic standard deviate = 0.067635, p-value = 0.473\n## alternative hypothesis: greater\n## sample estimates:\n## Observed Moran I      Expectation         Variance \n##     0.0024999785    -0.0009614966     0.0026192872"},{"path":"spatreg1.html","id":"final-words","chapter":"Week 10 Spatial Regression I: Spatializing aspatial regression residuals","heading":"10.3.7 Final words","text":"directly tackled spatial regression week, illustrated easily conventional aspatial regression models can projected onto space, assuming units observation correspond geographic places. exploratory diagnostic approach greatly extends understanding model relationships can begin answer questions raised past two weeks health data came clustered space.next two weeks build formally incorporate spatial relationships model .","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatial-regression-ii-spatial-econometric-regression","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"Week 11 Spatial Regression II: Spatial econometric regression","text":"","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"getting-ready-9","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.1 Getting Ready","text":"","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"learning-objectives-10","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"important-vocabulary-10","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.1.2 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 11","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatial-thinking-in-epidemiology-8","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.2 Spatial Thinking in Epidemiology","text":"","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatial-econometric-models-putting-dependence-right-in-the-model","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.2.1 Spatial econometric models: Putting dependence right in the model","text":"Spatial Regression 1, examined spatial dependence left-(e.g. residual ) conventional aspatial linear regression. benefits multivariable regression answering epidemiologic questions ability summarize relationships conditional possibly highly-dimensional covariate patterns.use Moran’s statistics test presence spatial auto correlation residuals (errors) multiply-adjusted models strategy ) diagnosing violations model assumptions; b) iteratively improving model specification (variables selection) effort explain measured variables drivers clustered outcomes.However, mix aspatial regression spatial residual diagnostics limits explanatory power epidemiologically interesting spatial phenomenon. Specifically, limitations aspatial regression strategy :Assumes full effects exposures outcomes contained within boundaries unit aggregation (e.g. neither ‘spills ’ ‘spreads ’ neighboring regions affecting rates). related Stable Unit Treatment Value Assumption (SUTVA) causal inference, also referred interference.still produce biased estimates coefficients residual autocorrelation errors remains. words, add predictor variables , Moran’s statistic remains meaningfully high, regression coefficients likely biased.module, extend aspatial methods inserting representations data generating process produces explains auto correlation directly regression model. data generating process set interactions, relationships, effects among people environments give rise actual observed data.Therefore, articulating specific testable data generating process, trying explicitly model spatial processes play order recover unbiased (less biased) covariate association estimates, also statistically test quantify evidence spatial spillover.Spatial spillover refers phenomenon experiences neighboring units (e.g. outcome rates exposure/covariate values) direct indirect influence risks rates index region. Spillover therefore approximates processes contagion, diffusion, propagation occur infectious diseases, environmental contaminants, social processes including norms, knowledge, attitudes, behaviors.potential evident class spatial models – often termed spatial econometrics models development economics – comes cost many assumptions, increased importance thoughtfulness part epidemiologic analyst. thorny statistical issues involved, also perennial challenge overlaying statistical issues questions interests population sciences like epidemiology.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"comparing-spatial-econometric-models","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.2.2 Comparing spatial econometric models","text":"numerous regression models quantify spatially correlated data. module introduce just three seen use economics, sociology, political science, occasionally used epidemiology.Unfortunately, methods primarily developed linear regression outcomes (really errors conditional aspects model) assumed normally independently distributed. can use models spatial analysis epidemiologic data ‘events’ analyzed rates, risks, prevalence plausibly normally distributed (note can still use population weighting regions account heteroskedasticity variance due varying population size).interested, Bayesian extensions incorporate spatially-lagged predictors generalized linear regression.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatially-lagged-outcome-model","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.2.2.1 Spatially Lagged Outcome Model","text":"first econometric model introduced sometimes called ‘spatial lag model’ includes spatially lagged value neighbors outcome (e.g. weighted average values outcome spatial neighbors) predictor model.words suggests disease risk, rate, prevalence neighboring regions direct influence risk, rate, prevalence inside index region. easiest metaphor contagious infectious disease, number infected neighbors directly cause infection incidence rise due contagion. previously talked social contagion understanding spatial spread disease, seems plausible next model, exposures predictors lagged.Spatial Lag Model can represented like :\\[Y_i=\\rho WY_{-} + \\beta X_i + e_i\\]\ndescribe model words :health event outcome, \\(Y_i\\) value risk, rate, prevalence \\(^{th}\\) regionThe spatial correlation coefficient, \\(\\rho\\) quantifying strength magnitude correlation average outcome neighbors outcome index county, \\(\\), conditional variables model. spatial correlation coefficient, \\(\\rho\\) interpreted correlation coefficients range \\(-1\\) \\(+1\\). Note value \\(\\rho=0\\) implies zero correlation (net covariates model) neighbors outcome index region outcome; setting term effectively drops model zero, resulting conventional aspatial linear regression model.spatial weights matrix, \\(W\\), spatial weights construct used spatial Empirical Bayes Moran’s analyses. words mathematical summarization regions neighbors . Note use row-standardized weights aid interpreting spatial regression coefficient.\\(\\rho W\\) specific values outcomes \\(Y_{-}\\). words regions \\(\\), specifically neighbors, defined \\(W\\) (e.g. non-neighbors \\(W=0\\) thus drop ).\\(\\beta X_i\\) \\(e_i\\) interpretation conventional regression. Specifically, \\(\\beta\\) regression coefficient change outcome \\(Y\\) 1-unit change \\(X\\), conditional spatially lagged outcome, \\(\\rho WY_{-}\\)","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatial-durbin-model","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.2.2.2 Spatial Durbin Model","text":"natural extension Spatial Lag model outcome dependent variable allowed spillover one region another, effects exposure independent covariates similarly allowed spillover.Conceptually, model proxies data generating process wherein exposures effect health constrained fall within boundaries region region’s outcome, also diffuse spread influence outcome neighboring regions. relatively easy imagine environmental service-access predictors whose influence unlikely arbitrarily end boundaries.example may hospital county, one just across border next county. Therefore exposed hospital access service spills county.statistical description Spatial Durbin model looks much like Spatial Lag, one addition:\\[Y_i=\\rho WY_{-} + \\beta X_i + \\gamma WX_{-} + e_i\\]model added components can described words like :spatial weights matrix, \\(W\\) identical locations: \\(\\rho WY_{-}\\) \\(\\gamma WX_{-}\\). instances simply describes regions neighbors (\\(W>0\\)) neighbors (\\(W==0\\)).two sets covariates, \\(X\\) model. first (\\(\\beta X_i\\)) refers measured covariates, \\(X\\) \\(^{th}\\) region. contrast second reference \\(\\gamma WX_{-}\\), refers values \\(X\\) \\(^{th}\\) region, indicated \\(X_{-}\\) ‘regions \\(\\)’. specifically lagged (averaged) values neighbors regions -also \\(W>0\\) (e.g. among neighbors).\\(\\gamma\\) coefficient quantifying magnitude direction association spatially-lagged covariates outcome.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatial-error-model","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.2.2.3 Spatial Error Model","text":"final model alternative specification underlying reason observed data patterns, data generating process. model, assumption reason observed spatial patterns autocorrelation outcomes /residuals one spillover (contagion) outcomes one reason next, spillover diffusion effects exposures one region outcomes neighbors.Instead, model simply states reason important missing variables spatially clustered. Therefore – can included model – model residuals also clustered auto correlated. words model assertion starting point Spatial Regression . However, different Spatial Error model articulates statistical way estimate valid unbiased coefficients spite (conditional ) dependent errors residuals.done manner familiar worked correlated data. strategy explicitly model account auto correlation nuisance term order terms model can estimated conditionally unbiased way. model form looks like :\\[Y_i=\\beta X_i+ u_i\\]\n\\[u_i=\\lambda Wu_{-} + e_i\\]Essentially model give place correlation modeled (e.g. residual correlation coefficient, \\(\\lambda\\)), order remaining residual, \\(e_i\\) can assumed distributed \\(N(0, \\sigma^2)\\)).interpretation correlation coefficient \\(\\lambda\\) similar spatial lag correlation coefficient, \\(\\rho\\), range \\(-1\\) \\(+1\\), zero suggesting correlation (e.g. units spatially independent, conditional variables model).different, however, assumed spatially correlated. spatial lag model, \\(\\rho\\) quantifies strength correlation outcome county \\(\\) average outcome set ‘neighbor’ counties. contrast, spatially lagged correlation \\(\\lambda\\) quantifies strength correlation ‘error’, presumed represent mix random error, contribution unmeasured unobserved drivers spatial structure outcome.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"comparing-and-selecting-spatial-econometric-models","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.2.3 Comparing and selecting spatial econometric models","text":"Given set three competing models described , spatial epidemiologist choose? best strategy mix substantive knowledge theory specific scenario question hand, careful cautious use statistical testing compare model fit competing models.First, several assertions made LeSage & Pace (2009) consider consequences mis-match whatever true data generating process might (e.g. really happening world gave rise data), versus data generating process approximated choice one models.true data generating process Spatial Error, three models produce unbiased estimates coefficients, although Durbin Spatial Lag models may less statistically efficient (e.g. produce unnecessarily large standard errors). answer , average, valid regardless model choice, true reason spatial clustering residuals missing unobserved variables.true data generating process Spatial Lag, analyst selects Spatial Error model, model coefficients biased misleading. words ‘contagious’ spread outcome producing dependence risk rate disease, instead appropriately fitting spatial Lag model, spatial error model fit, estimates may biased.Finally, true data generating process concerns direct spatial spread outcome spatial spillover exposures predictors (e.g. suggested Durbin model), either spatial Error Spatial Lag model biased. words, failing explicitly model spatial spillover covariates can produce biased estimates.take home message assertions Spatial Durbin model logical baseline starting point absence specific substantive theoretical reason prefer models. comparing spatial Durbin model others, possible test whether fact statistical evidence spatial spillover covariates.use statistical testing compare models reviewed next tutorial section. However, briefly, possible use likelihood ratio testing comparison AIC competing model forms determine whether model fits data better one strategy another.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatial-analysis-in-epidemiology-estimating-spatial-econometrics-models","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3 Spatial Analysis in Epidemiology: Estimating spatial econometrics models","text":"","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"package-data","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.1 Package & Data","text":"packages largely usual suspects one addition, package spatialreg. package actually subsumed many functions spdep. still use spdep neighbor & weights creation, load package directly many function conflicts spatialreg. use spdep:: notation call needed., use Atlanta-area BRFSS 500-cities project census-tract-based estimates self-reported health behaviors outcomes, merged area-based indicators.full data dictionary Spatial Regression II section eBook. Briefly, variables focus tutorial example:","code":"\npacman::p_load(tidyverse,   # For general data manipulation\n               sf,          # For defining and handling spatial data\n               tmap,        # For mapping\n               spatialreg)  # For spatial econometric regression\n# Read in the dataset and remove tracts with missing values...\natl <- st_read('BRFSS_Atl.gpkg') %>%\n  na.omit() "},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"defining-the-study-question","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.2 Defining the study question","text":"follows abbreviated summary specific questions (statistical epidemiologic), motivation. real-world scenario might include additional measures, consideration sources bias include misclassification exposure outcome, selection bias, conventional confounding, ecologic bias related modifiable areal-unit problem.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"the-idealized-ovarching-question","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.2.1 The idealized ovarching question","text":"idealized overarching question whether living place characterized concentrated poverty housing instability causes (e.g. increases decreases) poor mental health.specifically focus variables hypothesized contribution mental health:Poverty_std standardized poverty rate within tract. know spatial concentration poverty associated numerous contextual processes including services, stigma, aspects built environment including walkability, food, social environment. therefore might hypothesize higher poverty associated higher prevalence poor health. spatial patterning poverty relation investment built environment services, might also hypothesize variable also spatial spillover effect mental health prevalence. words poverty tract, living area many adjacent tracts high poverty causes harm.InstabilityStress captures degree housing instability (e.g. vacancy foreclosures) occur neighborhood. Increased housing instability directly stressful individuals, also produce indirect anxiety depression residents area.ParkProximity_Std represents relative access green space parks. Evidence mixed whether proximity green space causally related mental health, might hypothesize relationship exists, protective. also might hypothesize predictor ‘spatial spillover’.PHYSHLTH captures prevalence poor physical health tract, plausibly direct cause poor mental health. might include possible confounder effect interest, although also plausible physical mental health complex time-varying feedback relationship.question public health relevance extent spatial concentration poverty, housing policy, urban design decisions reproduced transportation, zoning, development, housing policies (theoretically quite modifiable), identification place-based determinants health can imply place-based targeted intervention including mental health screening mental health services.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"the-more-realistically-answered-overarching-question","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.2.2 The (more) realistically answered overarching question","text":"Generally, answer idealized overarching question single study design modality. Given reliance cross-sectional, self-reported BRFSS 500-cities project, realistic overarching question might whether ecologic census-tract prevalence self-reported poor mental health associated poverty rates sub-region Atlanta metropolitan region, conditional ecologically-measured confounders.realistic question differs idealized version de-emphasis recovering causal effect estimates, acknowledgment ecologic (case cross-sectional) measurement exposure, outcome, covariates, restriction possibly unique sub-section metropolitan region.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"specific-questions","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.2.3 Specific questions","text":"Within framework, can generate several specific questions anticipate specific answers. distinction overarching question (may discrete quantitative answers, often qualitative answers), specific questions important. former clearly frames motivates latter, without latter (specific questions) may get lost sea statistical results without clarity can want know.just possible questions can answer data. divide (perhaps simplistically) statistical epidemiologic questions simply highlight different emphasis field. reality clearly overlap statistical epidemiologic questions!","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"statistical-questions","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.2.4 Statistical questions","text":"data generating process (spatial error, lagged outcome, lagged outcome covariates) best fits data?residual auto correlation beyond explained spatial auto regression terms?spatial weights matrix best fits data?","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"epidemiologic-questions","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.2.5 Epidemiologic questions","text":"tract-level poverty housing instability associated poor mental health, adjusting candidate causal confounders detailed ?evidence confounding target relationships candidate confounders using change--estimates approach?Assuming census tracts reasonable definition local neighborhoods, spatial clustering poor mental health primarily function missing variables spatial contagion spillover processes?association () poverty housing instability poor mental health primarily defined local effects (e.g. within-neighborhood) poverty housing instability one place affect mental health neighboring places?spillover effects contextual covariates, important direct compared indirect effects?spatial relationships contextual covariates poor mental health depend (vary ) definition local inter-connectedness places?","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"defining-competing-definitions-of-local","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.3 Defining competing definitions of local","text":"emphasized several points throughout semester one fundamentally important decision spatial analysis specification spatial weights matrix mathematical encoding belief spatial relationships study units observation. spatial neighbors geographic units believed share experience interact one another, differentiated units unlikely .Whether regional units (really inhabitants regions) interact depends question hand. instance experience sharing access primary health care center likely occurs relatively small scale (small set neighborhoods primarily depend clinic). contrast, looking access liver transplantation services highly-specialized referral care, scope local quite different, much geographically far-flung range places willing access health resource.question, spatial conceptualization concentration poverty housing instability important. quantify variables scale individual household, buildings (e.g. families living multi-family unit), census tract, conglomerations census tracts, forth.Poverty concentrated larger areas (e.g. regions city including multiple neighborhoods) likely looks different terms social environmental context micro-pockets poverty (e.g. specific city blocks apartment complexes), therefore may different impacts mental health. reasonable definition much census tracts share interact spread experience poverty mental health?","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"two-extremes","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.4 Two extremes","text":"Queen contiguity definition neighbors probably represents lower bound connectedness possible given set geographic boundaries (e.g. can’t estimate apartment building values census tracts).might interactions occur beyond bounds contiguous census tracts? Perhaps. Might interactions likely occur 10 miles away 1 mile away? Unlikely (impossible). K-nearest neighbors definition lets us flexibly scale size spatial connection network.quick review, let’s create several competing spatial weights matrices competing definitions spatial neighbors. mentioned , package use spatial regression (spatialreg) several conflicts package containing functions creating neighbors (spdep). reason load package spdep , instead call explicitly using double-colon notation .Queen contiguity weights:somewhat extreme alternative, create k-nearest neighbor spatial weights assuming interaction occurs tract 15 nearest neighbors!Now, let’s plot two versions visualize range connectivity hypothesized (code borrowed Disease Mapping 1):Clearly k-nearest neighbors provides denser connectivity matrix neighbors Queen contiguity.","code":"\n# Create a Queen Contiguity weights objects\nqueen <- atl %>% # start with sf object\n  spdep::poly2nb() %>%      # convert polygons to neighbor object\n  spdep::nb2listw()         # convert nb object to spatial weights\n\n# NOTE: nb2listw() creates row-standardized weighted by default\n# That is what we want for spatial regression\n# Create a 15 tract k-nearest neighbor weights object\nknn15 <- atl %>%               # start with sf object\n  st_centroid() %>%            # get the centroid of each tract\n  st_coordinates() %>%         # create matrix of x,y coordinates for that centroid\n  spdep::knearneigh(k=15) %>%  # use spdep to find 15 nearest neighbors\n  spdep::knn2nb() %>%          # convert that into a formal nb object\n  spdep::nb2listw()            # and finally convert the nb into a spatial weights object\n# Create centroids object\natl.cent <- st_centroid(st_geometry(atl))\n\npar(mfrow = c(1,2))\nplot(st_geometry(atl), main = 'Queen', border = 'grey')\nplot(queen, coords = atl.cent, add = T)\nplot(st_geometry(atl), main = 'KNN 15', border = 'grey')\nplot(knn15, coords = atl.cent, add = T)"},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"fitting-multivariable-regression-models","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.5 Fitting multivariable regression models","text":"","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatial-or-aspatial","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.5.1 Spatial or aspatial?","text":"different model previous weeks, assume aspatial model necessarily poor choice. Fitting summarizing results – including assessment spatial auto correlation residuals– always starting point moving spatial econometric regression.Review results take note magnitude direction associations. Remember primary question focuses poverty housing instability.NOTE:One measure model fit reported: \\(R^2=0.9\\). extremely high may related way BRFSS 500-cities small area data created: small area modeling process right, included variables may overly explain one another jointly considered.ignore fact purposes exercise, point suggest caution optimistic looking fit statistics!diagnose spatial auto correlation model residuals repeat procedures two weeks ago. try procedure Queen contiguity weights 15-nearest neighbors. can extend intermediate specifications.Two things suggested results :relatively large spatial autocorrelation prevalence poor mental health unconditional mean model regardless neighbor choice. cases, inclusion predictor variables reduces magnitude Moran’s statistic, meaning variables explain portion spatial auto correlation.spatial autocorrelation appears modestly stronger empty adjusted models Queen contiguity compared 15 K-nearest neighbors. suggest spatial scale dependence auto-correlation smaller local highs lows slightly masked attenuated pool tracts together relatively larger KNN definition.","code":"\n# Fit unconditional mean (empty) model using conventional regression\nm0 <- lm(MENTALHLTH ~ 1,\n         data = atl)\n# Fit conventional aspatial linear regression with predictors\nm1 <- lm(MENTALHLTH ~ Poverty_std + InstabilityStress + ParkProximity_std  + PHYSHLTH,\n         data = atl)\nsummary(m1)## \n## Call:\n## lm(formula = MENTALHLTH ~ Poverty_std + InstabilityStress + ParkProximity_std + \n##     PHYSHLTH, data = atl)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.7934 -0.7675 -0.1878  0.6420  3.9312 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)        5.57191    0.33031  16.869   <2e-16 ***\n## Poverty_std        1.12808    0.10492  10.752   <2e-16 ***\n## InstabilityStress -0.05725    0.05310  -1.078   0.2824    \n## ParkProximity_std  0.53791    0.25446   2.114   0.0359 *  \n## PHYSHLTH           0.46897    0.02883  16.269   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.198 on 180 degrees of freedom\n## Multiple R-squared:  0.9004, Adjusted R-squared:  0.8982 \n## F-statistic: 406.8 on 4 and 180 DF,  p-value: < 2.2e-16\n# First compare the empty model to the adjusted model using Queen neighbors\nspdep::lm.morantest(m0, listw = queen)## \n##  Global Moran I for regression residuals\n## \n## data:  \n## model: lm(formula = MENTALHLTH ~ 1, data = atl)\n## weights: queen\n## \n## Moran I statistic standard deviate = 16.515, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Observed Moran I      Expectation         Variance \n##      0.725418358     -0.005434783      0.001958399\nspdep::lm.morantest(m1, listw = queen)## \n##  Global Moran I for regression residuals\n## \n## data:  \n## model: lm(formula = MENTALHLTH ~ Poverty_std + InstabilityStress +\n## ParkProximity_std + PHYSHLTH, data = atl)\n## weights: queen\n## \n## Moran I statistic standard deviate = 4.2186, p-value = 0.00001229\n## alternative hypothesis: greater\n## sample estimates:\n## Observed Moran I      Expectation         Variance \n##      0.167610084     -0.015323958      0.001880378\n# Now compare the empty model to the adjusted model using KNN15 \nspdep::lm.morantest(m0, listw = knn15)## \n##  Global Moran I for regression residuals\n## \n## data:  \n## model: lm(formula = MENTALHLTH ~ 1, data = atl)\n## weights: knn15\n## \n## Moran I statistic standard deviate = 26.168, p-value < 2.2e-16\n## alternative hypothesis: greater\n## sample estimates:\n## Observed Moran I      Expectation         Variance \n##     0.6245066933    -0.0054347826     0.0005795126\nspdep::lm.morantest(m1, listw = knn15)## \n##  Global Moran I for regression residuals\n## \n## data:  \n## model: lm(formula = MENTALHLTH ~ Poverty_std + InstabilityStress +\n## ParkProximity_std + PHYSHLTH, data = atl)\n## weights: knn15\n## \n## Moran I statistic standard deviate = 4.9199, p-value = 4.33e-07\n## alternative hypothesis: greater\n## sample estimates:\n## Observed Moran I      Expectation         Variance \n##     0.0956392273    -0.0129261292     0.0004869422"},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatial-error-model-1","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.6 Spatial Error Model","text":"Spatial Error Model (SEM) posits measured covariates relate (act ) outcome within region independently (e.g. don’t spillover act across boundaries). However, model allows something unmeasured (within areal unit census tract) predicts outcome spatially patterned (spatially random).consequence spatial auto correlation nuisance dependency arising missing covariate. intentionally explicitly modeling error (e.g. specifying /estimating form dependency part model fitting process), can achieve two objectives:Quantify estimates residual dependency form correlation coefficient \\(\\lambda\\) (lambda); andEstimate regression coefficients measured covariates hope longer biased spatial dependence.Remember, assumed data generating process Spatial Error Model follows:\\[Y_i=\\beta X_i+u_i\\]\\[u_i=\\lambda Wu_{-}+e_i\\]:\\(Y\\) vector outcomes (prevalence poor mental health case \\(^{th}\\) census tract);\\(X\\) matrix covariates (e.g. n = 185 census tracts x k = 1 intercept + 4 predictors);\\(\\beta\\) vector coefficients, \\(\\beta_0\\) intercept; \\(u\\) error term.SEM, error term, \\(u\\), sum two components: first spatially independent portion, \\(e_i\\); second spatially auto correlated portion induced spatially patterned missing variable(s). expressed strength correlation, \\(\\lambda\\) spatially-lagged measures \\(u\\) indicated spatial weights matrix \\(Wu\\).","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"fitting-the-spatial-error-model","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.6.1 Fitting the Spatial Error Model","text":"code fitting SEM model using Queen contiguity weights. can re-fit model alternative spatial weights matrices. function, errorsarlm() stands spatial autoregressive (SAR) linear model, requires formula defining model, dataset, definition spatial neighbors object.","code":"\nsem <- errorsarlm(MENTALHLTH ~ Poverty_std + InstabilityStress + ParkProximity_std  + PHYSHLTH,\n                  data = atl,\n                  listw = queen)\n\nsummary(sem)## \n## Call:errorsarlm(formula = MENTALHLTH ~ Poverty_std + InstabilityStress + \n##     ParkProximity_std + PHYSHLTH, data = atl, listw = queen)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.72726 -0.76110 -0.12242  0.69509  4.21695 \n## \n## Type: error \n## Coefficients: (asymptotic standard errors) \n##                    Estimate Std. Error z value Pr(>|z|)\n## (Intercept)        5.315774   0.386667 13.7477   <2e-16\n## Poverty_std        0.970541   0.106880  9.0807   <2e-16\n## InstabilityStress -0.026098   0.055274 -0.4722   0.6368\n## ParkProximity_std  0.429410   0.273470  1.5702   0.1164\n## PHYSHLTH           0.499118   0.033517 14.8913   <2e-16\n## \n## Lambda: 0.41472, LR test value: 13.904, p-value: 0.00019236\n## Asymptotic standard error: 0.093931\n##     z-value: 4.4152, p-value: 0.000010092\n## Wald statistic: 19.494, p-value: 0.000010092\n## \n## Log likelihood: -286.4547 for error model\n## ML residual variance (sigma squared): 1.2512, (sigma: 1.1186)\n## Number of observations: 185 \n## Number of parameters estimated: 7 \n## AIC: NA (not available for weighted model), (AIC for lm: 598.81)"},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"interpreting-sem-output","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.6.2 Interpreting SEM output","text":"several bits information one can glean output. summary points:regression coefficients suggest Poverty_std PHYSHLTH significantly positively associated prevalence poor mental health. ParkProximity_std relatively large magnitude association statistically significant, InstabilityStress associated.interpretation coefficient Poverty_std – conditional covariates error term – 1-standard deviation increase tract poverty rate, 1% increase prevalence poor mental health, association statistically significant.moderately strong (statistically significant) spatial auto correlation errors, evidenced estimated \\(\\lambda=0.41, p < 0.001\\).log-Likelihood AIC reported, comparison AIC Spatial Error Model (\\(AIC=587\\)) compared aspatial linear model (\\(AIC=599\\)) suggest better fit SEM (e.g. smaller AIC better fit).","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"hausman-test-of-coefficients","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.7 Hausman test of coefficients","text":"One statistical test specific use Spatial Error Model Hausman Test. Recall one concern naively using aspatial regression presence underlying spatial auto correlation regression coefficients aspatial regression may biased. Hausman Test test consistency regression coefficients Spatial Error Model compared aspatial version. test spatial autocorrelation; instead tests whether coefficients consistent.significant test suggests regression coefficients SEM likely inconsistent coefficients aspatial model; words evidence aspatial model coefficients biased compared spatial error model.","code":"\nHausman.test(sem)## \n##  Spatial Hausman test (asymptotic)\n## \n## data:  NULL\n## Hausman test = 14.781, df = 5, p-value = 0.01134"},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatial-lag-model","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.8 Spatial Lag Model","text":"spatial lag model shifts source spatial auto correlation error term (random effects) mean (fixed effects) portion model. particular, Spatial Lag Model posits reason spatial auto correlation dependency outcome one region average outcome neighboring regions.dependency implies form spread outcome, expected infectious contagious processes (influenza affects risk influenza), imagined terms social contagion (e.g. smoking behavioral outcome smoking affects sense normalcy around smoking).However clear whether spatially-lagged prevalence poor mental health contagious. Perhaps place everyone depressed affects depression?statistical estimation guarantee contagion play real world, notion spatially-lagged outcomes interesting compelling epidemiologic public health perspective, suggests something spread diffusion outcome, may imply intervention needed change environmental risk factors, break propagation one person (unit) another.Remember, data generating process SLM follows:\\(Y_i=\\rho WY_{-}+\\beta X_i+e_i\\)\\(Y\\) \\(X\\beta\\) SEM , \\(e_i\\) spatially-independent residual error term. new coefficient model \\(\\rho\\), correlation coefficient estimating degree mean outcomes one’s neighbors (captured spatially via spatial weights matrix \\(WY_{-}\\)) predictor local outcome.Assuming spatial weights matrix, \\(W\\) row-standardized, \\(-1<\\rho<+1\\) \\(\\rho=0\\) meaning spatial auto correlation outcomes.function lagsarlm() package spatialreg fits Spatial Lag Model well Spatial Durbin Model (). use Durbin = FALSE distinguishes SLM, spatially-lagged outcomes, covariates.","code":"\nslm <- lagsarlm(MENTALHLTH ~ Poverty_std + InstabilityStress + ParkProximity_std  + PHYSHLTH,\n                data = atl,\n                listw = queen,\n                Durbin = F)\n\nsummary(slm)## \n## Call:lagsarlm(formula = MENTALHLTH ~ Poverty_std + InstabilityStress + \n##     ParkProximity_std + PHYSHLTH, data = atl, listw = queen,     Durbin = F)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.95397 -0.77812 -0.14108  0.68593  3.41057 \n## \n## Type: lag \n## Coefficients: (asymptotic standard errors) \n##                    Estimate Std. Error z value Pr(>|z|)\n## (Intercept)        4.595551   0.507521  9.0549  < 2e-16\n## Poverty_std        1.056327   0.103725 10.1839  < 2e-16\n## InstabilityStress -0.102273   0.055872 -1.8305  0.06718\n## ParkProximity_std  0.430945   0.249558  1.7268  0.08420\n## PHYSHLTH           0.419326   0.036375 11.5280  < 2e-16\n## \n## Rho: 0.14362, LR test value: 5.6967, p-value: 0.016996\n## Asymptotic standard error: 0.060921\n##     z-value: 2.3574, p-value: 0.018403\n## Wald statistic: 5.5574, p-value: 0.018403\n## \n## Log likelihood: -290.5585 for lag model\n## ML residual variance (sigma squared): 1.3492, (sigma: 1.1616)\n## Number of observations: 185 \n## Number of parameters estimated: 7 \n## AIC: 595.12, (AIC for lm: 598.81)\n## LM test for residual autocorrelation\n## test value: 6.9746, p-value: 0.0082677"},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"interpreting-slm-output","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.8.1 Interpreting SLM Output","text":"similarities differences output spatial lag compared spatial error models:regression coefficients appear substantially different magnitude sign SEM, possible exception housing instability appears larger coefficient smaller (albeit still non-significant) p-value.Instead \\(\\lambda\\) coefficient, model now reports spatial lag term, \\(\\rho=0.14, p = 0.017\\). suggests modest spatial dependence outcome (small, statistically significant \\(\\rho\\)).AIC SLM smaller AIC aspatial linear model, suggesting improvement fit including spatial auto correlation.significant, residual spatial auto correlation errors (e.g. \\(p=0.008\\)), conditional covariates model spatially-lagged \\(\\rho\\) coefficient.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"model-fit-statistics","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.8.2 Model fit statistics","text":"can use Likelihood Ratio Tests compare fit pairs models. can formally test two questions:spatial lag model fit better aspatial regression model?ANSWER: Yes. (p-value difference fit = 0.017)spatial lag model fit better spatial error model?ANSWER: spatial error model appears slightly smaller AIC spatial lag model suggesting (spatial error model) fits better.","code":"\nLR.Sarlm(m1, slm)## \n##  Likelihood ratio for spatial linear models\n## \n## data:  \n## Likelihood ratio = -5.6967, df = 1, p-value = 0.017\n## sample estimates:\n##  Log likelihood of m1 Log likelihood of slm \n##             -293.4069             -290.5585\n# instead we can compare AIC for each model\nAIC(sem, slm)##     df      AIC\n## sem  7 586.9095\n## slm  7 595.1170"},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"spatial-durbin-model-1","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.9 Spatial Durbin Model","text":"Spatial Durbin Model extends Spatial Lag Model allowing outcome, also covariates spillover one region next.notion spatial lag suggests contagion outcomes causing outcomes, spillover effect covariates broadly applicable. implication external effects covariates simply impact felt beyond (possibly arbitrary) boundaries geographic regions. occur environmental drift (e.g. via air water), spatial mobility individual home location nearby locations.Remember, mathematical form data generating process extends SLM adding spatially-lagged \\(X\\) well \\(Y\\) variables:\\[Y_i=\\rho WY_{-}+\\beta X_i + \\gamma WX_{-} + e_i\\]\\(\\beta X_i\\) models within-region effects covariate \\(X\\), \\(\\gamma WX_{-}\\) models spillover spatially-lagged effects \\(X_{-}\\) (e.g. spatially lagged average \\(X\\) values region \\(\\)) spatial neighbors indicated \\(W\\).","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"fitting-the-spatial-durbin-model","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.9.1 Fitting the Spatial Durbin Model","text":"code fitting Spatial Durbin Model differs SLM code changing model Durbin = TRUE.","code":"\nsdm <- lagsarlm(MENTALHLTH ~ Poverty_std + InstabilityStress + ParkProximity_std  + PHYSHLTH,\n                data = atl,\n                listw = queen,\n                Durbin = T)\n\nsummary(sdm)## \n## Call:lagsarlm(formula = MENTALHLTH ~ Poverty_std + InstabilityStress + \n##     ParkProximity_std + PHYSHLTH, data = atl, listw = queen,     Durbin = T)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -3.065250 -0.702557 -0.077417  0.528628  4.017753 \n## \n## Type: mixed \n## Coefficients: (asymptotic standard errors) \n##                        Estimate Std. Error z value  Pr(>|z|)\n## (Intercept)            4.369455   0.787175  5.5508 2.844e-08\n## Poverty_std            0.909306   0.113223  8.0311 8.882e-16\n## InstabilityStress     -0.023942   0.063633 -0.3763   0.70673\n## ParkProximity_std      0.275745   0.310209  0.8889   0.37406\n## PHYSHLTH               0.542647   0.043181 12.5667 < 2.2e-16\n## lag.Poverty_std        0.430300   0.262114  1.6417   0.10066\n## lag.InstabilityStress -0.227807   0.117749 -1.9347   0.05303\n## lag.ParkProximity_std -0.096726   0.563610 -0.1716   0.86374\n## lag.PHYSHLTH          -0.294494   0.071865 -4.0979 4.169e-05\n## \n## Rho: 0.34143, LR test value: 10.337, p-value: 0.0013041\n## Asymptotic standard error: 0.098766\n##     z-value: 3.457, p-value: 0.00054623\n## Wald statistic: 11.951, p-value: 0.00054623\n## \n## Log likelihood: -280.3087 for mixed model\n## ML residual variance (sigma squared): 1.1849, (sigma: 1.0885)\n## Number of observations: 185 \n## Number of parameters estimated: 11 \n## AIC: 582.62, (AIC for lm: 590.95)\n## LM test for residual autocorrelation\n## test value: 0.19933, p-value: 0.65526"},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"interpreting-sdm-output","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.9.2 Interpreting SDM output","text":", similarities differences output compared preceding model forms.coefficient table includes within-region fixed effects ; \\(\\beta X_i\\) values data generating process .coefficient table now also includes spatially-lagged versions covariate, representing \\(\\gamma WX_{-}\\). associations spatially lagged averaged neighboring units \\(X\\) covariates -unit outcomes, controlling covariates. results indicated prefix lag.xxx.\\(\\rho\\) coefficient estimated model, now value \\(\\rho = 0.34, p=0.001\\), suggesting , conditional spatially-lagged covariates, now stronger correlation outcomes neighboring regions -region outcomes.AIC 583 SDM smaller AIC 591 aspatial regression (also smaller SLM SEM models)significant residual auto correlation (e.g. \\(p=0.65\\))qualitative terms, two example interpretations:Poverty direct effect spillover effect, case associated higher prevalence poor mental healthPrevalence poor physical health positively correlated poor mental health within census tract, somewhat offset inverse association neighboring tracts prevalence poor physical health.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"model-fit-statistics-1","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.9.3 Model fit statistics","text":", can check Likelihood Ratio Tests assess model fit.Durbin model fit better aspatial regression?ANSWER: Yes. spatial Durbin model fits better.Durbin model fit better SEM?ANSWER: Yes. Spatial Durbin model fits least somewhat better SEM.Durbin model fit better SLM?ANSWER: Yes. spatial Durbin model appears fit best models.NOTE:particular case Spatial Durbin model appears best fitting. fact, combined fact anticipate Durbin model provides best opportunity return statistically unbiased coefficient estimates meaningful.However, note results function particular variables included. alternate model specifications data, Durbin model fit best. assume Durbin always best fitting model.","code":"\nLR.Sarlm(m1,sdm)## \n##  Likelihood ratio for spatial linear models\n## \n## data:  \n## Likelihood ratio = -26.196, df = 5, p-value = 0.00008174\n## sample estimates:\n##  Log likelihood of m1 Log likelihood of sdm \n##             -293.4069             -280.3087\nLR.Sarlm(sdm, sem)## \n##  Likelihood ratio for spatial linear models\n## \n## data:  \n## Likelihood ratio = 12.292, df = 4, p-value = 0.01531\n## sample estimates:\n## Log likelihood of sdm Log likelihood of sem \n##             -280.3087             -286.4547\nLR.Sarlm(sdm, slm)## \n##  Likelihood ratio for spatial linear models\n## \n## data:  \n## Likelihood ratio = 20.5, df = 4, p-value = 0.0003978\n## sample estimates:\n## Log likelihood of sdm Log likelihood of slm \n##             -280.3087             -290.5585"},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"comparing-models","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.10 Comparing Models","text":"many things hold mind comparing spatial econometric models:one model better conceptually represents question interest process play, noteworthy. example, many epidemiologists argue selecting covariates based plausible role contributing bias confounding (e.g. determined prior knowledge, DAG), rather using model fit statistics.underlying data generating process clear, best chance unbiased estimates regression coefficients comes Spatial Durbin Model.model fit statistics provide ancillary information well different specifications data generating process fit observed data. fit statistics may go , important sources information. However, note fit statistics specific inputs provided, including specification model covariates, choice spatial weights. Thus, single comparison fit statistics necessarily settle issue.","code":""},{"path":"spatial-regression-ii-spatial-econometric-regression.html","id":"impact-assessments","chapter":"Week 11 Spatial Regression II: Spatial econometric regression","heading":"11.3.11 Impact Assessments","text":"setting either Spatial Lag Model Spatial Durbin Model interest, important note coefficients interpreted conventional manner accustomed regression.reason presence spatial spillover represented either \\(\\rho\\) coefficient spillover outcome, \\(\\gamma\\) coefficients spillover covariates, means change \\(X\\) variable affect \\(Y\\) within single region. Instead direct indirect impacts spread (propagate ripple) system defined spatial weights matrix.quantify effects context echo feedback, formal definitions statistical impacts defined capture reverberations effects transmitted system function strength correlation coefficients.results decompose impact changes \\(X\\) covariate Direct Indirect component.average Direct Impact effect, averaged across regions, change predictor covariate (e.g. exposure) \\(region_i\\) outcome \\(region_i\\), accounting fact changing local exposure might spillover change neighbors outcomes, might echo back affect local outcomes.average Indirect Impact effect, averaged across regions, change neighbors (e.g. \\(region_j\\)) predictor covariate local outcome \\(region_i\\).Total Impact sum direct indirect impacts.results , can see Total Impact Poverty_std 2, direct impact 0.96 (higher poverty tract predicts higher prevalence poor mental health), indirect impact 1.07 (higher poverty neighbors tracts predicts additional higher prevalence poor mental health).impact significant? can use Bayesian Markov Chain Monte Carlo simulations quantify bounds impacts. , must simulate impacts() large number times (actually run medium number times, R=199, time efficiency; increase number prediction):can see direct indirect impacts Poverty_std PHYSHLTH appear significantly different zero (least ran …specific numbers depend random seed number samples drawn). Furthermore, indirect effect InstabilityStress appears significantly different zero based 95% confidence intervals overlapping zero.","code":"\nimpacts(sdm, listw = queen)## Impact measures (mixed, exact):\n##                        Direct     Indirect      Total\n## Poverty_std        0.96236964  1.071752764  2.0341224\n## InstabilityStress -0.04084631 -0.341421083 -0.3822674\n## ParkProximity_std  0.27556056 -0.003729526  0.2718310\n## PHYSHLTH           0.53482319 -0.158016271  0.3768069\nsdm.impact <- impacts(sdm, listw = queen, R = 199)\nsummary(sdm.impact)## Impact measures (mixed, exact):\n##                        Direct     Indirect      Total\n## Poverty_std        0.96236964  1.071752764  2.0341224\n## InstabilityStress -0.04084631 -0.341421083 -0.3822674\n## ParkProximity_std  0.27556056 -0.003729526  0.2718310\n## PHYSHLTH           0.53482319 -0.158016271  0.3768069\n## ========================================================\n## Simulation results ( variance matrix):\n## Direct:\n## \n## Iterations = 1:199\n## Thinning interval = 1 \n## Number of chains = 1 \n## Sample size per chain = 199 \n## \n## 1. Empirical mean and standard deviation for each variable,\n##    plus standard error of the mean:\n## \n##                       Mean      SD Naive SE Time-series SE\n## Poverty_std        0.96251 0.10581 0.007500       0.007500\n## InstabilityStress -0.04662 0.06286 0.004456       0.003432\n## ParkProximity_std  0.28057 0.30998 0.021974       0.021974\n## PHYSHLTH           0.53430 0.04421 0.003134       0.003134\n## \n## 2. Quantiles for each variable:\n## \n##                      2.5%      25%     50%       75%   97.5%\n## Poverty_std        0.7666  0.88422  0.9559  1.032996 1.18095\n## InstabilityStress -0.1653 -0.09160 -0.0476 -0.000647 0.07481\n## ParkProximity_std -0.2984  0.07701  0.2785  0.474769 0.88039\n## PHYSHLTH           0.4404  0.50533  0.5380  0.562835 0.61496\n## \n## ========================================================\n## Indirect:\n## \n## Iterations = 1:199\n## Thinning interval = 1 \n## Number of chains = 1 \n## Sample size per chain = 199 \n## \n## 1. Empirical mean and standard deviation for each variable,\n##    plus standard error of the mean:\n## \n##                       Mean      SD Naive SE Time-series SE\n## Poverty_std        1.08853 0.31877 0.022597       0.022597\n## InstabilityStress -0.34170 0.17722 0.012563       0.010240\n## ParkProximity_std -0.03314 0.81912 0.058066       0.058066\n## PHYSHLTH          -0.15996 0.07515 0.005327       0.005327\n## \n## 2. Quantiles for each variable:\n## \n##                      2.5%     25%      50%     75%     97.5%\n## Poverty_std        0.4773  0.8775  1.09563  1.2806  1.681331\n## InstabilityStress -0.6830 -0.4401 -0.34472 -0.2336  0.044363\n## ParkProximity_std -1.6036 -0.5719 -0.01542  0.5044  1.530825\n## PHYSHLTH          -0.3230 -0.2020 -0.16205 -0.1197 -0.003548\n## \n## ========================================================\n## Total:\n## \n## Iterations = 1:199\n## Thinning interval = 1 \n## Number of chains = 1 \n## Sample size per chain = 199 \n## \n## 1. Empirical mean and standard deviation for each variable,\n##    plus standard error of the mean:\n## \n##                      Mean      SD Naive SE Time-series SE\n## Poverty_std        2.0510 0.32964 0.023368       0.022907\n## InstabilityStress -0.3883 0.17627 0.012495       0.012495\n## ParkProximity_std  0.2474 0.75094 0.053232       0.053232\n## PHYSHLTH           0.3743 0.05942 0.004212       0.004212\n## \n## 2. Quantiles for each variable:\n## \n##                      2.5%     25%     50%     75%    97.5%\n## Poverty_std        1.4791  1.8296  2.0323  2.2092 2.691911\n## InstabilityStress -0.7136 -0.5020 -0.3813 -0.2766 0.004567\n## ParkProximity_std -1.3465 -0.2295  0.2087  0.6789 1.832168\n## PHYSHLTH           0.2415  0.3383  0.3730  0.4106 0.480879"},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"spatial-regression-iii-geographically-weighted-regression","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"Week 12 Spatial Regression III: Geographically Weighted Regression","text":"","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"getting-ready-10","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.1 Getting Ready","text":"","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"learning-objectives-11","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.1.1 Learning objectives","text":"TABLE 1.1:  Learning objectives weekly module","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"important-vocabulary-11","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.1.2 Important Vocabulary","text":"TABLE 1.2:  Vocabulary Week 12","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"spatial-thinking-in-epidemiology-9","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.2 Spatial Thinking in Epidemiology","text":"","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"non-stationarity-and-spatially-varying-relationships","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.2.1 Non-stationarity and spatially varying relationships","text":"latter half course focused estimate, describe, test epidemiologically relevant spatial structure spatial heterogeneity population health data. Briefly topics covered following introductory Epidemiologic Cartography module:Disease mapping spatial heterogeneity describe epidemiologically meaningful patterns disease space, possibly even presence sparse data reduces certainty precision estimates. tools disease mapping focused stabilizing smoothing parameter estimates, possibly formal testing statistical deviation expectations, contrasting local rates global rates (e.g. SMRs).Global local spatial auto correlation ways describe tendency univariate data (e.g. health outcomes) spatially dependent. tests described degree (case local tests, location ) outcomes clustered space. Spatial auto correlation fundamental characteristic spatial structure, thus useful exploring describing data. Importantly, testing spatial auto correlation explain population events spatially dependent clustered.Spatial scan statistics class tests designed alternative (complementary) tool testing presence spatially-clustered health events. spatial auto correlation tested explicitly spatial-dependence locations, spatial scan statistics tested statistically unusual extreme rates risks disease within arbitrarily small/large search windows. way spatial scan statistics describe spatial structure lens spatial homogeneity (constant risk) heterogeneity. However, spatial scan statistics share spatial auto correlation statistics lack explanatory insight disease intensity varies space.Spatial auto correlation tests aspatial regression residuals first step towards describing spatial structure function multi-dimensional (multiple variable) predictors. fitting multivariable regression models include predictors might explain reason spatial structure, approach began intersection multivariable regression spatial structure. approach spatializing aspatial regression useful exploratory diagnostic step, result insight understand spatial structure generated. However, often residual spatial auto correlation residuals exists inclusion important measured variables, question remains: produces residual dependence?brings us geographically weighted statistics. order topics covered written somewhat hierarchical linear-sequential manner (step follows expands preceding), course analysis always way. particular, geographically weighted statistics culmination spatial analysis date, instead represent alternative way view data generating process.data generating process ideally refers biological, social, ecologic mechanisms processes collectively give rise generate data observed.often use statistical models approximate believe underlying process generated data. extent correctly describe data generating process gave rise data, can efficiently (validly) carry analysis observed data.","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"introducing-geographically-weighted-regression","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.2.2 Introducing geographically weighted regression","text":"Stationarity assumption statistic parameter estimate , average, constant homogeneous across samples. extension, spatial stationarity assumption statistic given study region constant regardless spatial location.Examples spatial non-stationarity previously introduced geographically weighted summary statistics (e.g. Disease Mapping 3), generally idea spatially heterogeneous mean intensity disease, tested spatial scan statistic Poisson tests excess risk. notion places higher places lower risk rates suggests single global statistic (e.g. overall risk rate) adequate describing health population; instead set local statistics needed.Spatially non-stationary disease intensity (e.g. spatial heterogeneity) example first order spatial variation. context, ‘first order’ refers first statistical moment mean expected value. first order spatial variation variation mean disease rate.may also interested second order spatial variation, refers mean, covariance among variables. Therefore, second order spatial processes reflect interest whether relationships among variables constant (global; stationary) heterogeneous (local; non-stationary).Geographically weighted regression (GWR) natural extension geographically weights summary statistics wish go single variable mean intensity (first order process) describe multivariable covariance patterns (second order process) space. GWR quite simply multivariable regression carried iteratively restrictions specific geographic sub-sets entire study region, interest estimating just single set regression coefficients, \\(\\beta\\), instead set coefficients one geographic sub-region (e.g. \\(\\beta_{x,y}\\)}).Just tools statistically testing whether spatial intensity (e.g. rate, prevalence) disease homogeneous versus heterogeneous, can similarly use statistical tests compare fit global (stationary) regression model fit local (non-stationary) model. evidence better fit, even penalty multiple comparisons, can proceed interpreting results GWR.Geographically weighted regression therefore powerful tool characterizing understanding data generating process. provides insight spatial relationships :permitting local analysis multivariable relationships; andstatistically testing whether single global model fits data better worse ensemble local models, even penalizing multiple comparison.tool represents significant step forward ability spatial epidemiologists understand spatial variation population health – even accounting confounding heterogeneity effects – methods several well-described limitations collectively lead us treat GWR exploratory tool:Local multi-collinearity: Regression highly colinear (highly correlated) predictor variables can produce statistically unstable regression coefficients due variance inflation. single global dataset may sufficient distinction among variables avoid problem. However – covariates may spatially clustered – common collinearity greater problem restricting specific spatial sub-regions. Thus, GWR estimates suffer variance inflation instability beta coefficients.Multiple comparison: analytic strategy GWR re-fit regression model multiple times, sub-region study area. quite reasonably raises concerns multiple comparisons, particularly statistical hypothesis tests conducted.Model overfitting: known concern regression modeling coefficients can biased model begins describe ‘random error’ rather underlying relationships, reducing generalizability coefficients. can occur including many covariates given number observations. Therefore process restricting geographic region model fitting GWR can lead local overfitting.Local sensitivity outliers: global regression, aware possible single extreme observations undue influence estimation regression coefficient. linear regression, residual diagnostics focus influence statistics diagnose problem. GWR, refitting model multiple subsets data increases risk least local fits unduly influenced outlier observations.Bandwidth selection: Just discussed introducing kernel density estimation Disease Mapping, key driver results using kernel density functions smooth weight data bandwidth. Bandwidth radius kernel search area. GWR, kernel bandwidth defines many observations (weight) included local regression. larger bandwidth include total data, limit amount spatial variation coefficients. contrast smaller bandwidth include less data fit maximize possible identification local spatial variation.","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"making-epidemiologic-meaning-of-spatially-varying-coefficients","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.2.3 Making epidemiologic meaning of spatially varying coefficients","text":"Although spatial heterogeneity theme throughout course, may obvious exactly heterogeneity regression coefficients means tells us epidemiologists. One analogy – clear imperfect analogy exact correspondence – conceive spatially varying regression coefficients akin statistical interaction non-spatial models.epidemiology, powerful important concept describing causal effects exposures interventions health outcomes, effect measure modification. idea causal effect exposure outcome homogeneous (e.g. constant; stationary), instead varies depends value second covariate (e.g. heterogeneous non-stationary). example effect measure modification non-spatial epidemiology magnitude causal effect drug treatment preventing death given disease larger women compared men.common analysts epidemiologic data incorporate interaction terms multivariable regression effort identify estimate effect measure modification, important recall statistical interaction always (perhaps rarely) equivalent causal effect measure modification. However, statistical interactions tests idea heterogeneity variation magnitude (e.g. additive multiplicative scale depending model fit) relationship association predictor outcome.Therefore, interpreting spatially varying coefficients geographically weighted regression seen similar interpreting heterogeneous correlation evidenced statistical interaction non-spatial model: describes degree association exposure outcome differs varies.model identified causal data generating process, statistical interaction indicate presence form spatial effect measure modification, identification something location modifies relationship exposure outcome.","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"distinguishing-variation-in-prevalence-from-variation-in-correlation","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.2.4 Distinguishing variation in prevalence from variation in correlation","text":"One final conceptual challenge interpreting spatially varying coefficients (interpreting statistical interactions general) clear role spatially varying covariate prevalence compared role spatially varying correlation covariate outcome.GWR describes geographic regions magnitude association predictor/covariate outcome larger smaller. However, confused overall prevalence predictor areas.example, examine maps illustrating prevalence smoking poor physical health Atlanta census tracts (predictors exposures; top two panels) compared strength association covariate outcome, poor mental health (bottom two panels).\nFIGURE 2.1: GWR: Prevalence vs. Correlation\ninterpretation regression coefficients bottom two maps “change \\(Y\\) one-unit change \\(X\\)’. Therefore, see prevalence poor mental health increases every 1-unit increase smoking prevalence southern part map compared northern. hand prevalence poor mental health increases every 1-unit increase prevalence poor physical health census tracts northern part map.However, combined prevalence information, can see meaning ‘one-unit change \\(x\\)’ put context variation covariates . magnitude association poor physical health poor mental health appears strongest north, census tracts actually quite low prevalence poor physical health.point illustration consider whether interest relative variation magnitude association (e.g. interaction non-stationarity association), population impact related baseline prevalence. Either relevant, considering prevalence correlation/association provides context.","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"spatial-analysis-in-epidemiology-7","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3 Spatial Analysis in Epidemiology","text":"","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"estimating-geogrpahically-weighted-regression-models","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.1 Estimating geogrpahically weighted regression models","text":"","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"packages-and-data-1","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.1.1 Packages and Data","text":"least four packages provide functions geographically weighted regression: McSpatial, gwrr, spgwr, GWModel. using latter package, GWModel much functionality others, additional diagnostic model form capabilities.example tutorial uses data CDC PLACES project. data developed collaboration Robert Wood Johnson Foundation CDC effort increase availability small-area health data. Briefly, project uses data state’s Behavioral Risk Factor Surveillance System (BRFSS) survey estimate small area (census tract) prevalence limited set BRFSS indicators. estimation process employed CDC create dataset uses restricted-access geocodes, well model-interpolation assumptions.data available urbanized locations. included exposure measures public-use data sets indicated Data Dictionary .data geographic level census tracts (although 500-cities actually estimates census block group). values measure (e.g. poor mental health, smoking, etc) tract-specific prevalence. words purely ecologic data, thus regression ecologic analysis. true exercises semester, particularly important keep fact mind begin conducting multivariable regression.","code":"\npacman::p_load(tidyverse,   # For data workflow\n               sf,          # For managing spatial data class sf\n               sp,          # For managing spatial data class sp\n               GWmodel,     # For geographically-weighted statistics\n               raster,      # For converting spatial grid to raster in final section\n               tmap)        # For mapping results\natl <- st_read('BRFSS_Atl.gpkg') %>%\n  na.omit() # na.omit() drops 2 tracts with missing variables"},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"brfss-data","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.1.2 Data Dictionary","text":"","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"preparing-for-using-gwmodel","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.2 Preparing for using GWModel","text":"Please note already introduced GWModel package use geographically weighted summary statistics Disease Mapping 3. might revisit section (Part B lab Disease Mapping 3), review using kernel density weighting produce locally smoothed mean intensity correlation statistics.GWModel package updated accommodate sf class spatial data R yet. reason, necessary us convert sf data sp class. Furthermore, way kernel density estimator evaluates regions versus ‘local’ region using matrix distances centroid every pair census tracts (e.g. simply different way defining neighbors used spdep package).function GWModel can calculate distance matrix fly, needed many times, can computationally efficient calculate distance matrix , simply provide matrix function needs .code creates sp object, large matrix \\(185 X 185\\), cell representing Euclidean distance pairs tract centroids.default, distance calculated every pair observations. default case kernel density function centered every single observation. However, analyzing much larger dataset (e.g. thousands points), computationally inefficient points close together might add much new information.therefore possible pre-define locations kernel function centered adequately cover region efficient manner. points often defined along grid (e.g. every 1000 meters, example), define subset locations fit kernel.illustration approach final section tutorial, please note adaptation useful many settings. However, immediate purposes accept default behavior.","code":"\n# Create a copy of data in the 'sp' format for use in some functions\natl.sp <- atl %>%\n  as('Spatial')\n\n# Create distance matrix from centroids\natl.DM <- gw.dist(dp.locat = coordinates(atl.sp))"},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"defining-local-optimizing-kernel-bandwidths","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.3 Defining local: optimizing kernel bandwidths","text":"Remember considered alternative definitions local using kernel density estimates changing bandwidth kernel function? Recall bandwidth radius search window, kernel typically bell-shaped curve (e.g. Gaussian bi-square similar).GWModel package many functions using cross-validation (AIC) find optimal (defined best fitting) bandwidth given dataset. Unfortunately best fit completely depends statistic estimated. reason may need find different bandwidth statistical test.Just previous applications kernel density functions, can choose fixed adaptive bandwidth. Fixed bandwidth refers single kernel density radius use parts study region. contrast, adaptive bandwidth strategy allow radius adjusted maintain consistent amount information, irrespective changes population area.case GWModel, fixed bandwidths defined terms linear units map (e.g. meters case). contrast adaptive bandwidths defined number units; therefore kernel grows shrinks order maintain constant number units ‘’ kernel function.Disease Mapping 3, used function bw.gwss.average() find optimal bandwidth specific calculation geographically weighted summary statistics. However, optimal bandwidth can different statistic interest, therefore procedure finding optima geographically weighted regression different.function bw.gwr() uses cross-validation find optimum bandwidth geographically weighted regression rather summary statistics.optimal bandwidth specific regression model either fixed radius 12082 meters, else adaptive bandwidth always maintains 59 census tracts within local region, regardless relative size (e.g. akin k-nearest neighbors).","code":"\nh.fixed <- bw.gwr(MENTALHLTH ~ SMOKING + PHYSHLTH, \n             data = atl.sp, \n             dMat = atl.DM)\n\nh.adapt <- bw.gwr(MENTALHLTH ~ SMOKING + PHYSHLTH, \n             data = atl.sp, \n             adaptive = T,\n             dMat = atl.DM)\nh.fixed## [1] 12082.4\nh.adapt## [1] 59"},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"geographically-weighted-regression","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.4 Geographically weighted regression","text":"GWModel offers wide range modeling tools including geographically weighted linear regression, Poisson regression, binomial regression, even local fitting options including adjustment local heteroscedasticity, local ridge regression robust collinearity.outcome, MENTALHLTH continuous relatively normally distributed, basic linear geographically weighted regression work.fitting basic (meaning linear) GWR models via function gwr.basic(). Just many functions GWModel begin bw.x estimating bandwidths, many functions begins gwr.x fitting variety models. Look help documentation.procedure produces lot output! Examine output , review key points interest:initial test presence significant non-stationarity evidence (much complex) GWR fits better simpler global model. checking whether local (GWR) model fits better global (aspatial) model comparing AICc global model top output AICc GWR bottom. AICc fit statistics penalizes additional parameters, including many parameters estimated repeating model many times. Therefore find evidence non-stationarity AICc GWR smaller AICc global model (typically difference AICc 3 greater sufficient say local model fits better). case GWR fits much better global, suggesting significant non-stationarity relationships model.Next compare magnitude regression coefficients global fit (top) range coefficients GWR (lower portion output). variables appear vary ? Using interquartile comparison (e.g. comparing Q1 Q3 range coefficients), absolute difference greatest PHYSHLTH, although SMOKING also substantial variation.Look variance explained summarized adjusted \\(R^2\\) value. global GWR models explain great deal variance, GWR higher \\(R^2\\) 0.93.","code":"\nm <- gwr.basic(MENTALHLTH ~ SMOKING + PHYSHLTH, \n             data = atl.sp, \n             bw = h.adapt,\n             adaptive = T,\n             dMat = atl.DM)\nprint(m)##    ***********************************************************************\n##    *                       Package   GWmodel                             *\n##    ***********************************************************************\n##    Program starts at: 2023-10-11 14:41:58.516849 \n##    Call:\n##    gwr.basic(formula = MENTALHLTH ~ SMOKING + PHYSHLTH, data = atl.sp, \n##     bw = h.adapt, adaptive = T, dMat = atl.DM)\n## \n##    Dependent (y) variable:  MENTALHLTH\n##    Independent variables:  SMOKING PHYSHLTH\n##    Number of data points: 185\n##    ***********************************************************************\n##    *                    Results of Global Regression                     *\n##    ***********************************************************************\n## \n##    Call:\n##     lm(formula = formula, data = data)\n## \n##    Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.4988 -0.9689 -0.1638  0.6708  5.7530 \n## \n##    Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n##    (Intercept)  3.13908    0.28497  11.015  < 2e-16 ***\n##    SMOKING      0.16286    0.02830   5.754 3.64e-08 ***\n##    PHYSHLTH     0.50325    0.04137  12.166  < 2e-16 ***\n## \n##    ---Significance stars\n##    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n##    Residual standard error: 1.451 on 182 degrees of freedom\n##    Multiple R-squared: 0.8523\n##    Adjusted R-squared: 0.8507 \n##    F-statistic: 525.1 on 2 and 182 DF,  p-value: < 2.2e-16 \n##    ***Extra Diagnostic information\n##    Residual sum of squares: 383.1576\n##    Sigma(hat): 1.446982\n##    AIC:  667.704\n##    AICc:  667.9262\n##    BIC:  516.4669\n##    ***********************************************************************\n##    *          Results of Geographically Weighted Regression              *\n##    ***********************************************************************\n## \n##    *********************Model calibration information*********************\n##    Kernel function: bisquare \n##    Adaptive bandwidth: 59 (number of nearest neighbours)\n##    Regression points: the same locations as observations are used.\n##    Distance metric: A distance matrix is specified for this model calibration.\n## \n##    ****************Summary of GWR coefficient estimates:******************\n##                    Min.    1st Qu.     Median    3rd Qu.   Max.\n##    Intercept -1.4633747 -0.0771617  3.0706879  3.9759400 8.0903\n##    SMOKING   -0.0074907  0.0599293  0.1369631  0.3200178 0.6134\n##    PHYSHLTH  -0.1574946  0.1773786  0.5613934  0.9628009 1.1627\n##    ************************Diagnostic information*************************\n##    Number of data points: 185 \n##    Effective number of parameters (2trace(S) - trace(S'S)): 27.10305 \n##    Effective degrees of freedom (n-2trace(S) + trace(S'S)): 157.8969 \n##    AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 568.6092 \n##    AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 540.1906 \n##    BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 441.6136 \n##    Residual sum of squares: 179.7807 \n##    R-square value:  0.9307003 \n##    Adjusted R-square value:  0.9187292 \n## \n##    ***********************************************************************\n##    Program stops at: 2023-10-11 14:41:58.524207"},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"checking-local-multi-collinearity","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.5 Checking local multi-collinearity","text":"collinearity possible concern regression model, importance GWR heightened fact model re-fit geographic subsets data variables anticipated higher correlation study region large (e.g. like things tends near like things). Multicollinearity can lead variance inflation may produce statistically unreliable (imprecise) estimates.can first carry collinearity diagnostics model results, find evidence concerning multicollinearity either alter model (remove culprit covariates) consider fitting model forms robust collinearity including local ridge regression available via function gwr.lcr().now begin diagnostics. function gwr.collin.diagno() returns Condition Number Variance Inflation Factor (VIF) model fit .neither print() summary() function output function. However see output summarized spatial object can look names collin$SDF:see overall Condition Number (local_CN), well Variance Inflation Factors (VIF) Variance Decomposition Proportions (VDP) variable. evaluating concerning collinearity, can map values understand whether (, ) evidence collinearity.instance might start looking local Condition Number; rule thumb suggests value 30 evidence important multicollinearity.looks like portion southwest Atlanta concerning CN’s. Now can look VIF variables see culprits ().can see places VIF variable 4; reduces concern meaningful local variance inflation model. continue explore diagnostics, now satisfactory finding.","code":"\ncollin <- gwr.collin.diagno(MENTALHLTH ~ SMOKING + PHYSHLTH,\n                            data = atl.sp,\n                            adaptive = T,\n                            dMat = atl.DM,\n                            bw = h.adapt)\nnames(collin$SDF)## [1] \"SMOKING_VIF\"             \"PHYSHLTH_VIF\"           \n## [3] \"local_CN\"                \"Intercept_VDP\"          \n## [5] \"SMOKING_VDP\"             \"PHYSHLTH_VDP\"           \n## [7] \"Corr_Intercept.SMOKING\"  \"Corr_Intercept.PHYSHLTH\"\n## [9] \"Corr_SMOKING.PHYSHLTH\"\ntm_shape(collin$SDF) +\n  tm_fill('local_CN',\n          style = 'fixed',\n          breaks = c(0,15,25,30,35),\n          palette = '-PRGn')+\n  tm_borders(alpha = 0.2) \ntm_shape(collin$SDF) +\n  tm_fill(c('SMOKING_VIF', 'PHYSHLTH_VIF'),\n          style = 'fixed',\n          breaks = c(0,1, 2, 3, 4),\n          palette = '-PRGn') +\n  tm_borders(alpha = 0.2) "},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"adjusting-for-multiple-comparisons","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.6 Adjusting for multiple comparisons","text":"First, let’s look closely output call gwr.basic() . Specifically, let’s look names m$SDF portion output.surprisingly, see values estimate local intercept, regression coefficients predictor. addition standard error coefficient, well t-value test statistic relates significance. simply local version information global regression output , provides information necessary determine whether covariate statistically significantly associated outcome, interest.However, presence local test statistics significance highlight another critique GWR, repetition tests across study region. repeating tests centered county turn reflect fully independent tests (lot information new window kernel), clearly concern making inference (example via tests statistical significance) multiple testing occurred.However can adjust test statistic account multiple comparison using function gwr.t.adjust(). function takes gwrm.Obj object output gwr.basic() function input, returns set p-values adjusted multiple comparisons different ways.function calculates adjusted p-values four alternative schemes. One familiar Bonferroni adjustment, almost surely conservative, assumes every test location completely independent sample (definition GWR false). approaches various versions False Discovery Rate algorithms try assess degree dependence tests determine effective number tests conducted (e.g. equivalent number unique, independent tests accounting overlapping information). work, least one review suggest Benjamini-Yekutieli method good middle ground.look results returned function see adjusted p-values provided method Benjamini-Yekutieli indicated *_by* suffix.","code":"\nnames(m$SDF)##  [1] \"Intercept\"     \"SMOKING\"       \"PHYSHLTH\"      \"y\"            \n##  [5] \"yhat\"          \"residual\"      \"CV_Score\"      \"Stud_residual\"\n##  [9] \"Intercept_SE\"  \"SMOKING_SE\"    \"PHYSHLTH_SE\"   \"Intercept_TV\" \n## [13] \"SMOKING_TV\"    \"PHYSHLTH_TV\"   \"Local_R2\"\nt.adj <- gwr.t.adjust(m)\nnames(t.adj$SDF)##  [1] \"Intercept_t\"    \"SMOKING_t\"      \"PHYSHLTH_t\"     \"Intercept_p\"   \n##  [5] \"SMOKING_p\"      \"PHYSHLTH_p\"     \"Intercept_p_by\" \"SMOKING_p_by\"  \n##  [9] \"PHYSHLTH_p_by\"  \"Intercept_p_fb\" \"SMOKING_p_fb\"   \"PHYSHLTH_p_fb\" \n## [13] \"Intercept_p_bo\" \"SMOKING_p_bo\"   \"PHYSHLTH_p_bo\"  \"Intercept_p_bh\"\n## [17] \"SMOKING_p_bh\"   \"PHYSHLTH_p_bh\""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"visualizing-model-inference","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.7 Visualizing model inference","text":"Finally, ready visualize results GWR model. many ways visualize significance parameter. one approach. code chunk following things:Convert sp sf can take advantage data manipulation sf objects using dplyr() verbs pipes.Recode adjusted p-value coefficient (e.g. Intercept, SMOKING, PHYSHLTH) indicate counties respective coefficient p-value <0.05, naming resulting variable pvalGroup counties whether covariate coefficient statistically significant grouping variable pvalUse summarise() function aggregate 185 census tracts either significant .Finally, use filter() exclude non-significant regions. Note removing actual coefficients used choropleth map! Instead just selecting outlines census tracts statistically significant adjusting multiple comparisons.complete, can now map geographically weighted regression coefficient (along adjusted test significance) association SMOKING, conditional variables, MENTALHLTH. tmap code relatively familiar point, exception couple additions:use function tm_add_legend() manually specify want legend include red lines labeled ‘p-value < 0.05’.R different ways get Greek letters symbols plots. tmap one found work use Unicode values. unique codes every possible symbol. Google ‘unicode Greek letters’ find lists. smoke.coefficients map specify title legend \\u03B2 value specific Greek \\(\\beta\\).map demonstrates several things relationship local prevalence smoking prevalence poor mental health (outcome) census tract level.First, reminder, map observed prevalence (predictor poor mental health) left hand map. Smoking prevalence higher South Atlanta.evident strength relationship smoking poor mental health stationary, instead quite non-stationary location-dependent. see evidenced fact coefficient ranges right-hand panel essentially zero (e.g. north) 0.6 southwest.Finally, global model suggested smoking statistically significant predictor poor mental health, appears right-hand map true part study area. words, smoking significantly associated poor mental health regions outside zone delineated red line.can examine three coefficients together like :","code":"\n# Create a spatial object delineating statistically significant INTERCEPT coefficients\nintercept.p <- t.adj$SDF %>%\n  st_as_sf() %>%\n  mutate(pval = ifelse(SMOKING_p_by <0.05, 1, 0)) %>%\n  group_by(pval) %>%\n  summarise(count = n()) %>%\n  filter(pval ==1)\n\n\n\n# Create a spatial object delineating statistically significant SMOKING coefficients\nsmoke.p <- t.adj$SDF %>%\n  st_as_sf() %>%\n  mutate(pval = ifelse(Intercept_p_by <0.05, 1, 0)) %>%\n  group_by(pval) %>%\n  summarise(count = n()) %>%\n  filter(pval ==1)\n\n\n# Create a spatial object delineating statistically significant PHYHLTH coefficients\nphyshlth.p <- t.adj$SDF %>%\n  st_as_sf() %>%\n  mutate(pval = ifelse(PHYSHLTH_p_by <0.05, 1, 0)) %>%\n  group_by(pval) %>%\n  summarise(count = n()) %>%\n  filter(pval ==1)\n# Map of observed prevalence of smoking\nsmoke.prev <- tm_shape(atl) +\n  tm_fill('SMOKING',\n          style = 'quantile',\n          palette = 'BuPu',\n          title = '%') +\n  tm_borders() +\n  tm_layout(title = 'Prevalence of smoking',\n          legend.format = list(digits = 1))\n\n# Map for spatially varying SMOKING coefficients\nsmoke.coefficients <- tm_shape(m$SDF) +\n  tm_fill('SMOKING',\n          style = 'quantile',\n          title = '\\u03B2') +\n  tm_borders(alpha = 0.2) +\ntm_shape(smoke.p) +\n  tm_borders(col = 'red', \n             lwd = 1) +\n  tm_layout(title = 'Coefficients for SMOKING',\n          legend.format = list(digits = 2)) +\ntm_add_legend(type = 'line',\n              labels = 'p-value < 0.05',\n              col = 'red')\n\ntmap_arrange(smoke.prev, smoke.coefficients)\n# Map for spatially varying intercepts\nint.coefficients <- tm_shape(m$SDF) +\n  tm_fill('Intercept',\n          style = 'quantile',\n          title = '\\u03B2',\n          palette = 'OrRd') +\n  tm_borders(alpha = 0.2) +\ntm_shape(intercept.p) +\n  tm_borders(col = 'red', \n             lwd = 1) +\n  tm_layout(title = 'Intercepts',\n          legend.format = list(digits = 2)) +\ntm_add_legend(type = 'line',\n              labels = 'p-value < 0.05',\n              col = 'red')\n\n# Map for spatially varying PHYSHLTH coefficients\nphyshlth.coefficients <- tm_shape(m$SDF) +\n  tm_fill('PHYSHLTH',\n          style = 'quantile',\n          title = '\\u03B2',\n          palette = 'YlGnBu') +\n  tm_borders(alpha = 0.2) +\ntm_shape(physhlth.p) +\n  tm_borders(col = 'red', \n             lwd = 1) +\n  tm_layout(title = 'Coefficients for PHYSHLTH',\n          legend.format = list(digits = 2)) +\ntm_add_legend(type = 'line',\n              labels = 'p-value < 0.05',\n              col = 'red')\n\ntmap_arrange(int.coefficients, smoke.coefficients, physhlth.coefficients)"},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"extensions","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.8 Extensions","text":"Unlike tools ’ve learned class, GWR quite widely adaptable variety situations. example possible conduct GWR analysis points polygons, demonstrated linear regression, also use Poisson Binomial regression.example, individual level outcome data associated exposures (individual contextual) linked street-level geocodes (e.g. points) carry spatial logistic regression, estimating spatially-varying association predictors outcome.Alternatively aggregated count data might common surveillance-type data relied Disease Mapping, use Poisson estimate rates rather rely linear regression.","code":""},{"path":"spatial-regression-iii-geographically-weighted-regression.html","id":"data-models","chapter":"Week 12 Spatial Regression III: Geographically Weighted Regression","heading":"12.3.9 Data models","text":"One final option useful using gridded points pre-specify evaluation location kernel density estimation. Recall default, preceding procedures place kernel estimator every single feature (census tract polygon centroid case). However efficient describe regular grid points covers whole region. computationally efficient example number grid points smaller number features (observations). Note pre-specified evaluation points don’t change data; instead change kernel window centered estimation.strategy described generalized time wanted return raster surface kernel density smoothing (e.g. using gwss() gwr.xx() models) points polygons. otherwords using gridded evaluation points converts answer polygon/point raster surface.approach follows steps:Extract bounding box spatial extent original data. min max \\(x,y\\) coordinate locations obesity spatial object.Define spatial resolution grid points. specified \\(1000\\) meters 1 km. far evaluation point.Define dimensions grid cover spatial extent data using specified resolution. many rows columns needed (specified resolution) cover target area.Finally, create SpatialGrid object sp target dimensions.plot grid original data can visualize evaluation points (grid points) represent region. grid single location GWR model centered. case clearly evaluation points number census tracts. way strategy actually saving computational time, instead simply step towards producing raster surface GWR results.Next, recalculate distance matrix. Recall originally describing distance pair county centroids (inform weights derived kernel density functions). Now, combine information census tract (weighting, described data point, dp.locat), evaluation point (locating kernel , described regression points, rp.locat).Finally, can refit original model, now using SpatialGrid definition model calibration points, rather assuming default region SpatialPolygons represents calibration model-fitting location.make visualizing easier can convert output model (SpatialPixelDataFrame) raster object. Typically raster object R single attribute (e.g. one statistic variable pixel values representing local value statistic).However, case want convert pixel values coefficients plus intercept. create called raster brick, just stack rasters. grid coverage well outside bounds study region, can also mask (e.g. trim crop) raster show results areas data.visualize results:demonstrates pre-defined spatial grid can recover patterns, instances number observations extremely large, spatial grid efficient approach. Unfortunately (reasons understand), standard errors t-values seem returned spatial grid, significance testing available (far can tell time writing).","code":"\nbb <- bbox(atl.sp)\nres <- 1000\nc.dim <- c(ceiling((bb[1,2] - bb[1,1]) / res),\n           ceiling((bb[2,2] - bb[2,1]) / res))\n\ngrd <- SpatialGrid(GridTopology(cellcentre.offset = bb[,1], \n                                cellsize = c(res,res), \n                                cells.dim = c.dim))\nplot(grd, col = 'red')\nplot(atl.sp, add = T, col = adjustcolor('navyblue',alpha.f=0.3))\natl.DM2 <- gw.dist(dp.locat=coordinates(atl.sp),\n              rp.locat=coordinates(grd))\n# Refit model at specified gridded points\nm2 <- gwr.basic(MENTALHLTH ~ SMOKING + PHYSHLTH, \n                data = atl.sp,\n                dMat = atl.DM2, \n                regression.points = grd,\n                bw = h.adapt,\n                adaptive = T)\n# Create brick and crop it to the county data\nm2.raster <- brick(m2$SDF) %>%\n  mask(atl)\nraster.int <- tm_shape(m2.raster$Intercept) +\n  tm_raster('Intercept',\n            style = 'cont',\n            palette = 'OrRd')\n\nraster.smoke <- tm_shape(m2.raster$SMOKING) +\n  tm_raster('SMOKING',\n            style = 'cont',\n            palette = 'BuPu')\n\nraster.phyhlth <- tm_shape(m2.raster$PHYSHLTH) +\n  tm_raster('PHYSHLTH',\n            style = 'cont',\n            palette = 'YlGnBu')\n\ntmap_arrange(raster.int, raster.smoke, raster.phyhlth)"},{"path":"reproducibility-and-projects-in-r.html","id":"reproducibility-and-projects-in-r","chapter":"Week 13 Reproducibility and Projects in R","heading":"Week 13 Reproducibility and Projects in R","text":"","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"additional-resources-9","chapter":"Week 13 Reproducibility and Projects in R","heading":"Additional Resources","text":"R Markdown CheatsheetComprehensive guide using R Markdown\nChapter within R Markdown guide specific Notebooks\nChapter within R Markdown guide specific NotebooksEpidemiologist R Handbook - Working projectsEpidemiologist R Handbook - R markdownWorking Projects RR Data Science - Workflow Projects","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"the-benefits-of-code-reproducibility","chapter":"Week 13 Reproducibility and Projects in R","heading":"13.1 The benefits of code reproducibility","text":"Reproducibility refers capacity process create fully independently replicable either future another person. Non-reproducibility scientific findings cited leading problem problem comes ad-hoc thus non-reproducible conduct data preparation analysis.Spatial epidemiology requires intensive data preparation, cleaning, management, often complex sequence analytic steps. words difficult another analyst future version repeat process exactly way unless perfect record done. reason, reproducibility analysis emphasized required course.analysis reproducible sure data stays paired code, (many possible) steps change manipulate data written scripts rather done ‘hand’ (e.g. Excel editor).","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"workflows-to-enhance-reproducibility","chapter":"Week 13 Reproducibility and Projects in R","heading":"13.2 Workflows to enhance reproducibility","text":"R RStudio often used data preparation, analysis, reporting, fundamental importance reproducibility (making analytic processes transparent, interpretable repeatable) built-many features. Appendix introduces several strategies important reproducibility broadly, also important work course.First, brief introduction projects RStudio, slightly -depth description specific file format, rmarkdown can used create Notebooks.","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"using-projects-in-r","chapter":"Week 13 Reproducibility and Projects in R","heading":"13.2.1 Using Projects in R","text":"project R organizes work much might use folders computer sort separate logical scheme. words, place put multiple documents files related one another.instance, might choose single project week class, perhaps separate project assignment. project directory (folder) store data, scripts code, outputs (e.g. saved maps saved objects) specific week assignment.advantage creating formal project RStudio (rather just regular folder, example), RStudio projects certain benefits coding workflow.open project, working directory (e.g. root directory file path R looks files import) automatically set inside project folder. means keep data inside project, never worry broken links incorrect file paths occur data moved.Projects remember environmental settings RStudio, may customize something specific project remembered time open project.ever work version control system Github, projects natural strategy contain repositoryYou avoid using setwd() R! function changes working directory may taught make easier. bad whatever pathname put inside setwd() amost never work another computer. means code fragile specific computer, probably computer specific point time.find relying setwd() strategy hard code file pathnames, please consider learning projects. help make code less fragile robust sharing reproducing.create new project:1.Look upper-right corner RStudio blue-ish R symbol likely say ‘Project’. Click pull-menu select New Project\n2. see Project Wizard open three options:\n+ yet created folder computer project, choose New Directory\n+ already folder (e.g. perhaps named ‘Week1’), choose Existing Directory\n+ forking checking repository Github, GitLab system, choose Version Control\n3. Navigate location want new folder , else location existing folder already \n4. Name project click Create ProjectOnce project created, can navigate via finder folder. notice new file extension .Rproj. double-click file, project open, including whatever files settings already worked .Get habit opening R double-clicking xxx.Rproj icon project folder. makes sure working directory set helps maintain relative rather absolute file pathnames within project folder.","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"organizing-projects","chapter":"Week 13 Reproducibility and Projects in R","heading":"13.3 Organizing projects","text":"projects analyses simple perhaps involve single script document use built-data. projects complex , involving dataset(s), one files code scripts, possibly output including datasets well images saved figures, markdown files reports. good practice standard strategy organizing .","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"make-scripts-that-do-discrete-tasks","chapter":"Week 13 Reproducibility and Projects in R","heading":"13.3.1 Make scripts that do discrete tasks","text":"may used one file hundreds even thousands lines code every part analysis. isn’t inherently wrong, can make difficult find particular snippets code defined recoded variable, carried descriptive analyses. larger projects, consider creating separate scripts discrete steps. many different R scripts given project, consider storing sub-folder perhaps labeled code/. might break work separate scripts like :script data preparation. allows quickly return process retrieving preparing data make changes.Scripts descriptive analysis. may want revisit descriptives future separate makes easier.Scripts (one ) complex analyses including modeling, figure preparation, simulation.script informative name project-x-data-prep.R project-x-create-final-maps.R.","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"always-store-data-with-code-and-output","chapter":"Week 13 Reproducibility and Projects in R","heading":"13.3.2 Always store data with code and output","text":"creating maps, raw (possibly post-processed, intermediate) data supports maps stored inside project folder. way guarantee can return year recreate map exactly. multiple data files, might consider putting content sub-folder, possibly labeled data/.","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"maintain-all-output-files-figures-cleaned-datasets-etc","chapter":"Week 13 Reproducibility and Projects in R","heading":"13.3.3 Maintain all output files (figures, cleaned datasets, etc)","text":"Just want store code data together, also plan store output content main project folder possibly one sub-folders (e.g. images/ reports/). several kinds outputs might generated including:Images figuresMapsCleaned prepared datasets (either stored .xlsx .csv possibly stored R binary format .rds)Reports (e.g. rendered R-markdown either html pdf)","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"use-the-here-package-to-maintain-robust-relative-pathnames","chapter":"Week 13 Reproducibility and Projects in R","heading":"13.4 Use the here package to maintain robust relative pathnames","text":"many reasons keep work organized, one maintain known constant relationship data code stored. discussed , use setwd() creates rigid absolute pointer file (e.g. data might C:\\MyDocuments\\EPI563\\Week1) stored. changed computers changed file structure current computer, absolute path likely fail making code non-reproducible (code find data)!Instead, please try preference relative pathnames. way describing something relative given starting point. case projects R-studio, starting point always folder containing project. Thus, location dataset stored sub-folder called data : data/mydataset.xlsx; assumed folder data sub-folder parent project folder. long keep project self-contained folder (e.g. copy/paste folder share folder contents), relative location robust.package developed try make bit easier. package named also function named () (know feels bit repetitive!). function, () serves describe hierarchical nesting folders locates file location desire (e.g. import dataset save figure ). examples use ():Importing data: mydata <- read.csv(('data', 'wave1', 'wave1_data.csv)). code, create new object (named mydata) results using function read.csv(). data located within project folder relative path location: data/wave1/wave1_data.csv.Saving output: ggsave(('figures', 'figure1.png')). code, save ggplot() figure computer location within overall project folder: figures/figure1.png.Caution: work Windows OS environment, careful designate file pathnames. R uses notation similar Unix OS, also one adopted Mac OS, define set nested folders forward slash : H:/mkram02/gis-file . Unfortunately opposite Windows describes pathnames (e.g. Windows use back slash like : H:\\mkram02\\gis-file). Using packages avoids confusion.","code":""},{"path":"reproducibility-and-projects-in-r.html","id":"specify-a-relative-location-outside-the-working-directory","chapter":"Week 13 Reproducibility and Projects in R","heading":"13.4.1 Specify a relative location outside the working directory","text":"one folder entire course, inside separate project directory week. working project Week2, might wish load file saved previously Week1. words sub-folder, actually outside current directory. use setwd() function change location, creates possibly fragile absolute pathname can dangerous. Instead create robust relative pathname referring file relation current location.Using two dots pathname tells R go level directory. georgia.csv file referred Week1 directory, currently woring Week2 :means “go level, look data folder, death-data folder, load georgia.csv file”. need go two () levels, simply repeat: ../../data/death-data.georgia.csv","code":"\ndd <- read.csv('../data/death-data/georgia.csv')"},{"path":"introduction-to-rmarkdown.html","id":"introduction-to-rmarkdown","chapter":"Week 14 Introduction to rmarkdown","heading":"Week 14 Introduction to rmarkdown","text":"","code":""},{"path":"introduction-to-rmarkdown.html","id":"what-is-markdown-and-why-do-we-need-it","chapter":"Week 14 Introduction to rmarkdown","heading":"14.1 What is markdown and why do we need it?","text":"markdown language approach easily creating formatted text use .html formats (.e.g PDF, Word) simple text-editor. Within R, rmarkdown package allows implementation general markdown language.assignments course, least portion deliverable fully-functional, annotated R Markdown document. benefit formats lets analyst create human-readable report easily combines R code, output results code (e.g. including tabular results well figures like maps), interspersed text formatted like might word processing document. example eBook, many resources course created using rmarkdown related packages bookdown.R Notebooks specific instance case markdown incorporated R Studio nice features applied data analyst.rmarkdown allows type text explains , decisions making, interpret findings, note areas need exploration. similar usual commenting might familiar , makes easy narrative expansive comments.rmarkdown contain functional R code interspersed narrative comments, code, comments output results can seen one continuous document.R-Studio, rmarkdown can result interactive results (e.g. output shows chunk), can even see rendered version look choosing Visual (instead Source) top editor pane. means coding working can see results document. save document text, code results saved!reason using rmarkdown provides means clear annotation documentation combined ready reproducibility. Reproducibility means someone else (future !) come back get result .benefit advantages , recommend gain familiarity basic (perhaps optional) formatting described . also recommend develop knack rich annotation documentation, just brief (often cryptic) comments used writing SAS code! Document plan . Document . Document results means. Document else needs done.R Markdown handy serve like ‘lab notebooks’ documenting thinking go. great reports want share others (future self). still ok use regular R-scripts analyses require extensive documentation. example writing functions data-cleaning scripts may appropriate simple scripts extension my_code.R rather my_code.Rmd (e.g. notebook markdown).rmarkdown vs. quartoRecently company develops supports R Studio developed new alternative markdown language R: Quarto. can read quarto , see comparison rmarkdown quarto .may learned use quarto another class. required use rmarkdown class. Instead, simply expected produce lab homework deliverables human-readable documents produced either markdown quarto.","code":""},{"path":"introduction-to-rmarkdown.html","id":"important-r-markdown-functions","chapter":"Week 14 Introduction to rmarkdown","heading":"14.2 Important R Markdown functions","text":"","code":""},{"path":"introduction-to-rmarkdown.html","id":"the-yaml","chapter":"Week 14 Introduction to rmarkdown","heading":"14.2.1 The YAML","text":"create new R Markdown file within R Studio (e.g. via File > New File > R Markdown), ‘YAML’ automatically created top script delineated three dash lines ---. YAML stands “yet another markup language” set instructions finished document look structured. can accept default YAML structure (course modifying title) copy/paste YAML top script. can also read online additional customizations YAML, none necessary course.However, YAML can tricky sometimes. general tips:Keywords (e.g. title, date output) end colon comes ‘argument’ ‘setting’ keyword.‘argument’ ‘setting’ keyword takes multiple lines, can hit , case output:.\nHowever, note sub-arguments (e.g. html_document:) parent must indented 2 spaces.\nsub-arguments (e.g. number_sections: yes specific setting html_document:) must indented additional 2 spaces. indentations represent organization connect multiple settings correct parent keyword.\nHowever, note sub-arguments (e.g. html_document:) parent must indented 2 spaces.sub-arguments (e.g. number_sections: yes specific setting html_document:) must indented additional 2 spaces. indentations represent organization connect multiple settings correct parent keyword.Modify YAML working quartoQuarto perhaps designed render files stay together set. example, documents eBook rendered disseminated group rather individuals files. However, work class asked submit .html versions work stand-alone file. Unfortunately, default behavior Quarto friendly “stand alone” dissemination (e.g. emailing uploading html Canvas). , formatting images may altered lost.solution?need tell R-studio Quarto want stand-alone version. , renders, necessary style, image information self-contained single .html file thus portable. adding details YAML like :","code":"---\ntitle: \"Title of your notebook\"\nauthor: \"Your Name Here\"\ndate: \"Submission date here\"\noutput:\n   html_document:\n    number_sections: yes\n    toc: yes\n    toc_float: yes\n------\ntitle: \"Untitled\"\nformat:\n  html:\n    embed-resources: true\neditor: visual\n---"},{"path":"introduction-to-rmarkdown.html","id":"typing-text","chapter":"Week 14 Introduction to rmarkdown","heading":"14.3 Typing text","text":"utility rmarkdown ability completely document thinking process carry analyses. necessary wordy just sake taking space, opportunity clearly delineate goals, steps, data sources, interpretations, etc.can just start typing text script serve purpose. text formatting functions summarized later document, Cheat sheets online resources linked elsewhere.","code":""},{"path":"introduction-to-rmarkdown.html","id":"adding-r-code","chapter":"Week 14 Introduction to rmarkdown","heading":"14.4 Adding R Code","text":"rmarkdow let write R code within Markdown file, run code, seeing results appear right code (rather Console, usually appear).2 ways add new chunk R code:Click green C-Insert button top editor panel R Studio. top option R code.Use keyboard short cut:\nMac Command + Shift + \nWindows Ctrl + Alt + \nMac Command + Shift + IWindows Ctrl + Alt + INotice R code chunks delineated three back-ticks (sort like apostrophes)…back-ticks typically key tilde (~) upper left keyboards. space sets 3 back-ticks R code goes called code chunk.see syntax color change things type inside R chunk (e.g. delineated ```), versus outside. Everything inside follows syntax rules R. Everything outside printed final report, run R code.want run code inside code chunks, can either:Place cursor line click Ctrl+enter (Windows) CMD+Return (Mac), can click Run button top editor pane R Studio.run code within chunk click green Run Current Chunk button upper-right code chunk.code corresponding results.way can iterate analytic process…switching running code, viewing output, documenting free text.","code":"\nhead(mtcars)##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nplot(cars)"},{"path":"introduction-to-rmarkdown.html","id":"workflow","chapter":"Week 14 Introduction to rmarkdown","heading":"14.5 Workflow","text":"recommend workflow:Click File>New File>R Markdown create new file. Edit YAML (stuff top) correct title, author, etc. template created example code. Delete generic code YAML. Save file project folder.Use space YAML type objective purpose analysis, introduction background useful.Carry analysis, inserting code chunks, running , documenting free text go.wish, can see results look HTML clicking Visual button top .Sometimes go back re-run code different order, else delete code without re-running entire script. means code reproducible objects created longer code support . final check reproducibility (assurance code self-contained dependent steps outside script) recommend always end clicking RUN button top panel. Specifically, choose Restart R Run Chunks. runs sure look results! step erases data objects memory starts running script top. error , something missing code. Try figure make changes code script everything expect.","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"formatting-markdown-and-notebooks","chapter":"Week 15 Formatting Markdown and Notebooks","heading":"Week 15 Formatting Markdown and Notebooks","text":"","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"optional-functions","chapter":"Week 15 Formatting Markdown and Notebooks","heading":"15.1 Optional functions","text":"list formatting functions long. include couple find useful (mandatory) :","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"customizing-your-yaml","chapter":"Week 15 Formatting Markdown and Notebooks","heading":"15.2 Customizing your YAML","text":"default YAML perfectly fine, YAML top script includes added functions including:Specify table contents - works use headersSpecify section numberingSpecify table contents ‘floating’ means html visible even scroll. PDF rendering, ‘float’ option.","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"simple-formatting-of-your-notebook","chapter":"Week 15 Formatting Markdown and Notebooks","heading":"15.3 Simple formatting of your Notebook","text":"generally helpful organize document using headers separate tasks steps code. can easily create headers using hashtag/pound sign #. Specifically…# beginning line denotes top-level (level-1) header large bold.## beginning line denotes level-2 header### unsurprisingly level-3 header!Make sure space # textAlways leave blank line (return/enter) header text ‘regular’ text.can also make numbered bulleted lists helpful. line begins either asterisk (*) number begin bulleted numbered list.Headers populated table contents, specified.","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"text-formatting","chapter":"Week 15 Formatting Markdown and Notebooks","heading":"15.4 Text formatting","text":"R Markdown Cheatsheets lots examples formatting. Three things use frequently bold, italics, numbered bulleted lists.Numbered lists start number, line must end 2 space (blank line ).Instead numbers can use lettersBulleted lists can initiated asterisk +, also must 2 spaces (blank carriage return) end item.","code":""},{"path":"formatting-markdown-and-notebooks.html","id":"making-tables","chapter":"Week 15 Formatting Markdown and Notebooks","heading":"15.5 Making tables","text":"required, may want summarize data table R Markdown. packages devoted creating tables, can create quick--dirty table just using keyboard symbols.First start making header row. Separate column name ‘pipe’ symbol, |Put continuous line dashes (-----) column name, separating columns pipe symbol (|)Now type text corresponding row column. Separate columns pipe (|) separate rows carriage return/EnterSo following text typed directly Markdown file (e.g. inside code chunk):produce following output:limited useful additional customizations table. instance can alter width column changing relative number dashes pipes. can also specify whether contents columns left right justified, whether centered using colons (:) inside line dashes.","code":"Column 1  | Column 2 | Column 3\n----------|----------|-----------\nText 1    | Text 2   | Text 3\nNext line | Next line 2 | Next line 3Column 1 | Left justified | Centered | Right justified |\n---------|:-------------|:---------:|-----------:|\nRow 1  | 1,024,477 | Johnson & Johnson | Dekalb County |\nRow 2 | 4,321 | Frederick | Mercer  |"},{"path":"formatting-markdown-and-notebooks.html","id":"final-note","chapter":"Week 15 Formatting Markdown and Notebooks","heading":"15.6 Final Note","text":"Remember final step think done project, Click Restart R Run Chunks, save/preview Notebook sure expect.","code":""},{"path":"sf-overview.html","id":"sf-overview","chapter":"Week 16 Tips for working with sf data class","heading":"Week 16 Tips for working with sf data class","text":"Simple Features (sf) cheat sheet","code":""},{"path":"sf-overview.html","id":"st_set_geom","chapter":"Week 16 Tips for working with sf data class","heading":"16.1 st_set_geom()","text":"One unique characteristic sf class data special column containing geometry information (often labeled geom) different variables. Specifically sticky. Stickiness variable means manipulate sf data object, geom column almost always sticks rest data even try remove .Imagine happen regular data.frame typed code console mvc[1, 1:2]. Typically kind numerical indexing cause R return row 1 columns 1 2. However, try R sf object happens:Notice get first row, first second column also got geom column even though didn’t request . stickiness generally desirable, important keep geographic/geometry data connected attribute data. However times want drop information. several ways , explicit way:literally erases sets NULL geometry column. retrieved without going back original data.","code":"\nlibrary(sf)\nlibrary(tidyverse)\n\nmvc <- st_read('../SpatialEpi-2021/DATA/GA_MVC/ga_mvc.gpkg')## Reading layer `ga_mvc' from data source \n##   `/Users/mkram02/Library/CloudStorage/OneDrive-EmoryUniversity/EPI563-Spatial Epi/SpatialEpi-2021/DATA/GA_MVC/ga_mvc.gpkg' \n##   using driver `GPKG'\n## Simple feature collection with 159 features and 17 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -85.60516 ymin: 30.35785 xmax: -80.83973 ymax: 35.00066\n## Geodetic CRS:  WGS 84\nmvc[1, 1:2]## Simple feature collection with 1 feature and 2 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -82.55071 ymin: 31.46925 xmax: -82.04858 ymax: 31.96618\n## Geodetic CRS:  WGS 84\n##   GEOID                    NAME                           geom\n## 1 13001 Appling County, Georgia MULTIPOLYGON (((-82.55069 3...\nmvc2 <- st_set_geometry(mvc, NULL)\n# look at the class of the original and the modified object\nclass(mvc)## [1] \"sf\"         \"data.frame\"\nclass(mvc2)## [1] \"data.frame\"\n# look at the first row and 1-2nd column after NULLing geom\nmvc2[1, 1:2]##   GEOID                    NAME\n## 1 13001 Appling County, Georgia"},{"path":"sf-overview.html","id":"st-as-sf","chapter":"Week 16 Tips for working with sf data class","heading":"16.2 st_as_sf()","text":"also times , inextricably, data set seems like sf object, get error message data right type geometry information sf.can happen data manipulation step strip away sf data class even though geom column still exists. happens can reinstate class status calling st_as_sf(). Essentially formal way declaring object sf explicitly defining spatial component.","code":""},{"path":"sf-overview.html","id":"st_crs","chapter":"Week 16 Tips for working with sf data class","heading":"16.3 st_crs()","text":"Spatial coordinate reference systems (CRS) projections critically important managing visualizing spatial data. spatial information sf object determined values coordinates contained geom geometry column, values assume known defined coordinate system. instance unprojected data typically measured degrees latitude longitude, even units can vary depending geodetic system datum used.know ’re working ? function st_crs() return whatever information stored object CRS/projection.recent version sf, returned st_crs() two pieces information:first piece labeled User input: case reads WGS 84, suggesting object based datum CRS.second piece information labeled wkt: stands Well-Known Text. standardized structured format describing annotating coordinate/projection information. detail probably want structure WKT CRS . short includes features:base datum underlying ellipsoid, case WGS 84Specific parameters including prime meridian, coordinate systemThe ID, often represented EPSG code.fact object mvc EPSG code 4326 suggests simple, unprojected, WGS-84 CRS (e.g. see ).Occasionally WKT complex, perhaps previous transformations stored metadata encoded WKT. case, closer examination WKT may needed identify CRS/projection. instance TARGETCRS mentioned? may current CRS.Another trick may get quickly data specifically need (usually know EPSG code data, one applied) code:","code":"\nst_crs(mvc)## Coordinate Reference System:\n##   User input: WGS 84 \n##   wkt:\n## GEOGCRS[\"WGS 84\",\n##     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n##         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n##         MEMBER[\"World Geodetic System 1984 (G730)\"],\n##         MEMBER[\"World Geodetic System 1984 (G873)\"],\n##         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n##         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n##         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n##         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n##         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n##             LENGTHUNIT[\"metre\",1]],\n##         ENSEMBLEACCURACY[2.0]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"geodetic latitude (Lat)\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"geodetic longitude (Lon)\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     USAGE[\n##         SCOPE[\"Horizontal component of 3D system.\"],\n##         AREA[\"World.\"],\n##         BBOX[-90,-180,90,180]],\n##     ID[\"EPSG\",4326]]\n# Extract only the CRS ID code from the object\nst_crs(mvc)$srid## [1] \"EPSG:4326\""},{"path":"dplyr.html","id":"dplyr","chapter":"Week 17 Tips for using dplyr","heading":"Week 17 Tips for using dplyr","text":"handy resources detail wrangling dplyr tidyr:dplyr overviewData transformation dplyr cheat sheetEpidemiologists R Handbook - tidy dataAs case many software packages, always one way get something done R! Base-R tools can accomplish kinds tasks, sometimes cumbersome inefficient.use R epidemiologists focused tools data science, might find diverse continually evolving tidyverse great toolbox explore. Originated Hadley Wickham (founder RStudio), packages constituting tidyverse now contributed lots different people. common interest handling data tidy ways. R Data Science authoritative guide tidy data, many tools constituting tidyverse including ggplot2, dplyr .appendix brief introduction dplyr package set data manipulation functions. words epidemiologists’ go-package data manipulation, recoding, preparation R.two high-level observations use dplyr semester:dplyr functions can thought verbs. means one tool act data, producing change. question “want change?”Functions dplyr (many parts tidyverse matter) can stand alone code. alternatively can chained together sequence. chaining (called piping tool connect chain steps called pipe looks like : %>%) can make code easier humans read, also helps run sequence steps efficiently.examples , use Georgia motor vehicle crash mortality dataset unit observation (e.g. content one row data) Georgia county, columns variable names. dataset also explicitly spatial meaning includes geography information regarding boundaries county, contained geom column, typical sf class data R.first rows dataset looks like (minus geom column):","code":"## Reading layer `ga_mvc' from data source \n##   `/Users/mkram02/Library/CloudStorage/OneDrive-EmoryUniversity/EPI563-Spatial Epi/SpatialEpi-2021/DATA/GA_MVC/ga_mvc.gpkg' \n##   using driver `GPKG'\n## Simple feature collection with 159 features and 17 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -85.60516 ymin: 30.35785 xmax: -80.83973 ymax: 35.00066\n## Geodetic CRS:  WGS 84##   GEOID                     NAME   variable estimate          County\n## 1 13001  Appling County, Georgia B00001_001     1504  Appling County\n## 2 13003 Atkinson County, Georgia B00001_001      875 Atkinson County\n## 3 13005    Bacon County, Georgia B00001_001      945    Bacon County\n## 4 13007    Baker County, Georgia B00001_001      390    Baker County\n## 5 13009  Baldwin County, Georgia B00001_001     2943  Baldwin County\n## 6 13011    Banks County, Georgia B00001_001     1767    Banks County\n##   MVCDEATHS_05 MVCDEATHS_14 MVCDEATH_17 TPOP_05 TPOP_14 TPOP_17\n## 1            4            4          10   17769   18540   18521\n## 2            5            1           3    8096    8223    8342\n## 3            7            5           0   10552   11281   11319\n## 4            1            1           1    3967    3255    3200\n## 5            6            8          13   46304   45909   44906\n## 6            4            8           6   16683   18295   18634\n##   NCHS_RURAL_CODE_2013    nchs_code     rural MVCRATE_05 MVCRATE_14 MVCRATE_17\n## 1                    6     Non-core     Rural   22.51111   21.57497   53.99276\n## 2                    6     Non-core     Rural   61.75889   12.16101   35.96260\n## 3                    6     Non-core     Rural   66.33813   44.32231    0.00000\n## 4                    4  Small metro non-Rural   25.20797   30.72197   31.25000\n## 5                    5 Micropolitan non-Rural   12.95784   17.42578   28.94936\n## 6                    6     Non-core     Rural   23.97650   43.72779   32.19921"},{"path":"dplyr.html","id":"select","chapter":"Week 17 Tips for using dplyr","heading":"17.1 select()","text":"first verb dplyr called select() useful want remove select specific columns/variables. instance, mentioned dataset 17 attribute columns plus geom column. perhaps need three variables, decided easier exclude unneeded variables? can select() want (inversely can select don’t want).three useful tips using select() spatial data:select variables keep simply list (e.g. select(data, var1, var2, var3))easier omit specific variables (e.g. perhaps 100 variables want drop 3), place negative sign name (e.g. select(data, -var5, -var6)).Finally, something specific working sf spatial data geometry column (typically named geom geometry) sticky. means ’s hard get rid . ’s actually good thing. usually want geometry stick attribute data. occasionally might want convert spatial sf data object aspatial data.frame. must first set geometry null like : aspatial.df <- st_set_geometry(spatial.df, NULL). See additional info .Let’s motor vehicle crash data.","code":"\n# First we read in the dataset, which is stored as a geopackage\nmvc <- st_read('GA_MVC/ga_mvc.gpkg')\n\n# Look at column names\nnames(mvc)\n\n# For this example we do not want the geom column because it is too big to view\nmvc2 <- st_set_geometry(mvc, NULL)\n\n# Creating a new object with only 4 attributes\nmvc2 <- select(mvc2, GEOID, NAME, rural, MVCRATE_05, MVCRATE_17)\n\n# look at column names\nnames(mvc2)##  [1] \"GEOID\"                \"NAME\"                 \"variable\"            \n##  [4] \"estimate\"             \"County\"               \"MVCDEATHS_05\"        \n##  [7] \"MVCDEATHS_14\"         \"MVCDEATH_17\"          \"TPOP_05\"             \n## [10] \"TPOP_14\"              \"TPOP_17\"              \"NCHS_RURAL_CODE_2013\"\n## [13] \"nchs_code\"            \"rural\"                \"MVCRATE_05\"          \n## [16] \"MVCRATE_14\"           \"MVCRATE_17\"           \"geom\"## [1] \"GEOID\"      \"NAME\"       \"rural\"      \"MVCRATE_05\" \"MVCRATE_17\""},{"path":"dplyr.html","id":"mutate","chapter":"Week 17 Tips for using dplyr","heading":"17.2 mutate()","text":"Another frequently needed verb called mutate() might guess changes data. Specifically mutate() function creating new variable, possibly recode older variable. mvc data object 159 rows (one n=159 counties).Let’s imagine wanted create map illustrated magnitude change rate death motor vehicle crashes 2005 2017. want create two new variables name delta_mr_abs (absolute difference rates) delta_mr_rel (relative diference rates).look help documentation mutate() ’ll see first argument input dataset, case mvc. anywhere one zillion different ‘recode’ steps can included inside parentheses, separated comma. , created two new variables, one representing absolute representing relative difference rates two years.can look first rows selected columns see new variables:","code":"\n# Now we make a new object called mvc2\nmvc3 <- mutate(mvc2, \n               delta_mr_abs = MVCRATE_05 - MVCRATE_17,\n               delta_mr_rel = MVCRATE_05 / MVCRATE_17)\nhead(mvc3)##   GEOID                     NAME     rural MVCRATE_05 MVCRATE_17 delta_mr_abs\n## 1 13001  Appling County, Georgia     Rural   22.51111   53.99276   -31.481650\n## 2 13003 Atkinson County, Georgia     Rural   61.75889   35.96260    25.796294\n## 3 13005    Bacon County, Georgia     Rural   66.33813    0.00000    66.338135\n## 4 13007    Baker County, Georgia non-Rural   25.20797   31.25000    -6.042034\n## 5 13009  Baldwin County, Georgia non-Rural   12.95784   28.94936   -15.991517\n## 6 13011    Banks County, Georgia     Rural   23.97650   32.19921    -8.222703\n##   delta_mr_rel\n## 1    0.4169284\n## 2    1.7173090\n## 3          Inf\n## 4    0.8066549\n## 5    0.4476038\n## 6    0.7446303"},{"path":"dplyr.html","id":"filter","chapter":"Week 17 Tips for using dplyr","heading":"17.3 filter()","text":"select() choosing columns keep drop, filter() choosing rows keep drop. familiar SAS, filter() statement might .Imagine wanted map urban counties, omit rural counties. defining filtering rule. rule logical statement (e.g. relationship can tested data return TRUE FALSE).create new dataset, mvc4 created mvc3 restricted non-Rural counties:can see original object (mvc3) 159 rows, filtered object (mvc4) 102, reflecting number non-Rural counties Georgia.Although example used one filtering rule (e.g. keep rows rural == 'non-Rural'), construct complex filter including several different logical tests within filter() function, separated comma. instance filter non-rural counties population 100,000 specified region state, assuming variables indicating values.","code":"\nmvc4 <- filter(mvc3, rural == 'non-Rural')\n\n\ndim(mvc3) # dimensions (rows, columns) of the mvc3 object## [1] 159   7\ndim(mvc4) # dimensions (rows, columns) of the restricted mvc4 object## [1] 102   7"},{"path":"dplyr.html","id":"arrange","chapter":"Week 17 Tips for using dplyr","heading":"17.4 arrange()","text":"Occasionally might want sort dataset, perhaps find lowest highest values variable, group like values together. Sorting dplyr uses arrange() verb. default, data arranged ascending order (either numerical alphabetical character variables), can also choose descending order :","code":"\nmvc5 <- arrange(mvc3, desc(MVCRATE_17))\n\nhead(mvc5)##   GEOID                     NAME     rural MVCRATE_05 MVCRATE_17 delta_mr_abs\n## 1 13307  Webster County, Georgia     Rural   38.81988  115.16315    -76.34327\n## 2 13269   Taylor County, Georgia     Rural   22.57336   73.69197    -51.11860\n## 3 13165  Jenkins County, Georgia     Rural   47.00353   57.03205    -10.02853\n## 4 13001  Appling County, Georgia     Rural   22.51111   53.99276    -31.48165\n## 5 13087  Decatur County, Georgia non-Rural   18.17455   52.40305    -34.22851\n## 6 13191 McIntosh County, Georgia non-Rural   16.11863   49.62427    -33.50564\n##   delta_mr_rel\n## 1    0.3370859\n## 2    0.3063205\n## 3    0.8241598\n## 4    0.4169284\n## 5    0.3468223\n## 6    0.3248135"},{"path":"dplyr.html","id":"pipe-operator","chapter":"Week 17 Tips for using dplyr","heading":"17.5 %>% Pipe operator","text":"Everything ’ve done now one step time, created five different datasets avoid overwriting original. one source coding efficiency R comes careful chaining piping together multiple steps.every verb required input dataset first argument, chain steps, functions take output previous step input current step. example code chunk everything one step:practice, takes experience write whole chain steps want. often go iteratively, adding one step time checking step expected.","code":"\nmvc6 <- mvc %>%\n  st_set_geometry(NULL) %>%                             # remove geom column\n  select(GEOID, NAME, rural, MVCRATE_05, MVCRATE_17) %>%# select target variables\n  mutate(delta_mr_abs = MVCRATE_05 - MVCRATE_17,        # recode variables\n        delta_mr_rel = MVCRATE_05 / MVCRATE_17) %>%\n  filter(rural == 'non-Rural') %>%                      # filter (restrict) rows\n  arrange(desc(MVCRATE_17))                             # sort by MVCRATE_17\n\ndim(mvc6)## [1] 102   7\nhead(mvc6)##   GEOID                      NAME     rural MVCRATE_05 MVCRATE_17 delta_mr_abs\n## 1 13087   Decatur County, Georgia non-Rural   18.17455   52.40305   -34.228506\n## 2 13191  McIntosh County, Georgia non-Rural   16.11863   49.62427   -33.505640\n## 3 13033     Burke County, Georgia non-Rural   34.87510   39.96093    -5.085824\n## 4 13189  McDuffie County, Georgia non-Rural   18.67501   37.21276   -18.537756\n## 5 13055 Chattooga County, Georgia non-Rural   11.76194   36.33428   -24.572337\n## 6 13227   Pickens County, Georgia non-Rural   29.32874   34.82335    -5.494612\n##   delta_mr_rel\n## 1    0.3468223\n## 2    0.3248135\n## 3    0.8727301\n## 4    0.5018442\n## 5    0.3237147\n## 6    0.8422147"},{"path":"dplyr.html","id":"group_by-and-summarise","chapter":"Week 17 Tips for using dplyr","heading":"17.6 group_by() and summarise()","text":"dplyr verb can incredibly important spatial epidemiology combination group_by() summarise(). two used aggregate summarize data. instance data arranged individual persons unit analysis (e.g. 1 person = 1 row data), wanted aggregate got counts per census tract, use group_by() arrange rows groups defined census tract, use summarise() calculation (e.g. count, mean, sum, etc) separately group.fact, can enter multiple variables group_by() argument result cross-classification grouping. example, imagine COVID-19 line listing data row single person, rows include age (young old) well county residence. enter group_by(county, age) data stratified age within county. used summarise(count = n()), get count individual cases age group county. use make map density young old cases county.important feature sf data objects operated dplyr verbs, built functionality handle geography/geometry data. instance, imagine wanted create map aggregated rural counties separately non-rural counties.can see (might predicted), aggregation changed dataset 159 rows 2 rows: one row rural one non-rural. Let’s see spatial data first mapping original data, mapping aggregated data. Read qtm()) tmap functions.dplyr verbs (e.g. mutate(), select(), filter()), constrained using group_by() single variable. Returning example individual observations nested within census tracts, use new_data <- individ_data %>% group_by(gender, year, tract) create file row data unique stratum gender * year * census tract.may see message R console run group_by() followed summarise() says something like summarise() ungrouping output override .groups argument. telling R automatically ungrouped data. specifically removed grouping last group_by() variable, avoid unintended consequences persistent grouping. group_by() two variables drops last grouping*, grouping may persist. grouping can removed adding ungroup() summarise().","code":"\nmvc7 <- mvc %>%\n  group_by(rural) %>%\n  summarise(avg_mr_17 = mean(MVCRATE_17))\n\nmvc7 %>% st_set_geometry(NULL)## # A tibble: 2 × 2\n##   rural     avg_mr_17\n## * <chr>         <dbl>\n## 1 Rural          29.2\n## 2 non-Rural      18.8\n# Using the qtm() function from tmap to create map of original data\nm1 <- qtm(mvc, 'MVCRATE_17')\n\n# Using the qtm() function from tmap package to create a map\nm2 <- qtm(mvc7, 'avg_mr_17')\n\ntmap_arrange(m1, m2)"},{"path":"dplyr.html","id":"join","chapter":"Week 17 Tips for using dplyr","heading":"17.7 join()","text":"Merging data common epidemiology, also prone many unintended consequences important pay attention options successful merges ‘table joins’ called parlance relational databases.challenges getting desired output merge join(), dplyr set verbs including: left_join(), right_join(), inner_join(), full_join() . documentation many ways join!join simply way merge two tables common key ID variable. purposes class want focus several key features joining important spatial analysis.explanation work two separate files: aspatial data.frame motor vehicle crash data called mvc.df; sf object U.S. counties, called us. unit analysis county, common ID key variable county FIPS code (named GEOID), different number rows:expected, mvc.df \\(n=159\\) counties makeup Georgia. However spatial/geography information \\(n=3220\\) rows, corresponding number U.S. counties territories.First, difference xxxx_join() two tables relate one another. purposes left_join() right_join() trick. difference: left_join() starts first object joins second. contrast right_join() starts second object joins first. mean?learn?number rows output dataset dictated two things:order objects written (e.g. case mvc.df always first us always second, contained within join())direction join. left_join() merge begins mvc.df, limits output 159 rows. contrast right_join() merge begins us, limits output 3220 rows.class output data depends object first. Notice left_join(), started aspatial data.frame, output also aspatial data.frame (although geom column now incorporated!). contrast right_join() put sf object us first, class sf.means merging joining think whether want rows data go output, ; think whether () can make sf object first.scenario , want \\(n=159\\) rows, thus want exclude non-Georgia counties. means must mvc.df first. Therefore, force object class sf like (also see info ):Joining Key/ID variables different namesSometimes common variable, county FIPS code, variable names different. example code , column storing unique county ID mvc.df named GEOID. However column sf object us stores unique county ID named FIPS. still possible use join() verbs relating (inside c() concatenation) order datasets introduced.","code":"\ndim(mvc.df)  # this is dimensions for the aspatial attribute data## [1] 159  17\ndim(us)      # this is dimensions for the spatial county polygon sf data## [1] 3221   13\n# left join, starting with mvc.df as the first object, us as the second\ntest.left <- mvc.df %>%\n  left_join(us, by = 'GEOID')\n\ndim(test.left)## [1] 159  29\nclass(test.left)## [1] \"data.frame\"\n# right join, starting with mvc.df as the first object\ntest.right <- us %>%\n  left_join(mvc.df, by = 'GEOID')\n\ndim(test.right)## [1] 3221   29\nclass(test.right)## [1] \"sf\"         \"data.frame\"\ntest.left <- mvc.df %>%\n  left_join(us, by = 'GEOID') %>%\n  st_as_sf()\n\ndim(test.left)## [1] 159  29\nclass(test.left)## [1] \"sf\"         \"data.frame\"\n# if us had the FIPS code stored in the column named 'FIPS'\ntest.left <- mvc.df %>%\n  left_join(us, by = c('GEOID' = 'FIPS'))"},{"path":"dplyr.html","id":"pivot_","chapter":"Week 17 Tips for using dplyr","heading":"17.8 Reshaping (transposing) data","text":"numerous intermediate advanced data manipulation options available dplyr tidyverse, outside scope course. One final verb represents sophisticated kind data change, however useful preparing spatial data. tools transpose reshape rectangular dataset wide long vice versa. Transposing useful , example, column disease rate several years (data wide), want dataset single column contains rate separate column indicates year (data long). article introduces notion pivoting data; can also review section R Data ScienceTwo related verbs help pivot tidy data one direction :","code":""},{"path":"dplyr.html","id":"pivot_longer-for-going-from-wide-to-long","chapter":"Week 17 Tips for using dplyr","heading":"17.8.1 pivot_longer() for going from wide to long","text":"Reviewing article linked previous paragraph (searching help documentation) give detail. example look take current mvc dataset, contains motor vehicle crash mortality rate county three different years (2005, 2014, 2017) separate columns (e.g. wide):mapping time-series often beneficial data long, say want data single column mvc_rate separate column year, can choose create map subset (defined year) data.First let’s look results, ’ll walk steps code chunk :can see, now 3 rows Appling County (GEOID 13001): one three years, different MVCRATE . long dataset. code work? step--step code chunk :first step create new object, mvc_long outcome steps piped together. input pipe original dataset, mvc.use as_tibble() current work around annoying ‘feature’ pivot_* functions don’t play well sf data classes. use as_tibble() essentially removing class designation, (making tibble tidy object); importantly different st_set_geometry(NULL) actually omits geometry column (e.g. see additional detail ).used select() pull variables interest, although leave variables desired.pivot_longer() can called several ways. way call , first specified columns pivot defining cols = argument variables start phrase MVCRATE. starts_with() another utility function dplyr. step told R columns wanted changed three called MVCRATE_05, MVCRATE_12 MVCRATE_17The names_to = argument defines column name new dataset delineate three variables (e.g. MVCRATE_05, etc). case wanted value year word MVCRATE_12. accomplish extra work:First, note used option names_sep = '_'. another utility function says want break string parts wherever designated separated (e.g. underscore, _) occurs. take column name MVCRATE_05 break underscore return two parts: MVCRATE 05.breaking two produce two answers, make two variable names names_to = hold . Thus names_to = c(\".value\", \"year\"). words column labeled .variable hold value MVCRATE column year hold value 05'.value' actually special value instance. way designating first part essentially junk. automatically discarded.values_to = 'mvcrate'. define name new dataset hold actual value (e.g. MVC mortality rate .)mutate() step just way take year fragment (e.g. 05, 12, 17) make calendar years first making numeric, simply adding 2000.final step, st_as_sf() manipulations actually removed objects designation class sf. Importantly, remove geom column, object recognized (e.g. tmap) spatial object. st_as_sf() simply declares fact sf.best way wrap head around start trying reshape transpose data hand. may need look additional help examples online, time become intuitive.see might gone work, use tm_facets() (read abouttmap_facets() ).","code":"\n# this code shows the first 6 rows (the head) of the relevant variables\nmvc %>% \n  st_set_geometry(NULL) %>%\n  select(GEOID, NAME, MVCRATE_05, MVCRATE_14, MVCRATE_17) %>%\n  head()##   GEOID                     NAME MVCRATE_05 MVCRATE_14 MVCRATE_17\n## 1 13001  Appling County, Georgia   22.51111   21.57497   53.99276\n## 2 13003 Atkinson County, Georgia   61.75889   12.16101   35.96260\n## 3 13005    Bacon County, Georgia   66.33813   44.32231    0.00000\n## 4 13007    Baker County, Georgia   25.20797   30.72197   31.25000\n## 5 13009  Baldwin County, Georgia   12.95784   17.42578   28.94936\n## 6 13011    Banks County, Georgia   23.97650   43.72779   32.19921\nmvc_long <- mvc %>%\n  select(GEOID, NAME, MVCRATE_05, MVCRATE_14, MVCRATE_17) %>%\n  as_tibble() %>%\n  pivot_longer(cols = starts_with(\"MVCRATE\"),\n               names_to = c(\".value\", \"year\"),\n               values_to = \"mvc_rate\",\n               names_sep = \"_\") %>%\n  mutate(year = 2000 + as.numeric(year)) %>%\n  st_as_sf()\nmvc_long %>%\n  st_set_geometry(NULL) %>%\n  head()## # A tibble: 6 × 4\n##   GEOID NAME                      year MVCRATE\n##   <chr> <chr>                    <dbl>   <dbl>\n## 1 13001 Appling County, Georgia   2005    22.5\n## 2 13001 Appling County, Georgia   2014    21.6\n## 3 13001 Appling County, Georgia   2017    54.0\n## 4 13003 Atkinson County, Georgia  2005    61.8\n## 5 13003 Atkinson County, Georgia  2014    12.2\n## 6 13003 Atkinson County, Georgia  2017    36.0\ntm_shape(mvc_long) +\n  \n  tm_fill('MVCRATE') + \n  tm_borders() +\ntm_facets(by = 'year')"},{"path":"dplyr.html","id":"pivot_wider","chapter":"Week 17 Tips for using dplyr","heading":"17.8.2 pivot_wider()","text":"course also possible go way, long wide. often easier. code return original shape:Take look output:appears returned 1 row per county. steps?, start removing class designation sf calling as_tibble()mutate() called re-create variable become column names.longer need old year variable omit select(-year)Finally pivot_wider() call arguments defining current variable contains informatino new column name (names_from =) current variable contains information population cells within column (values_from =).","code":"\nmvc_wide <- mvc_long %>%\n  as_tibble() %>%\n  mutate(my_var = paste0('MVCRATE ', year)) %>%\n  select(-year) %>%\n  pivot_wider(names_from = my_var,\n              values_from = MVCRATE) %>%\n  st_as_sf()\nmvc_wide %>%\n  st_set_geometry(NULL) %>%\n  head()## # A tibble: 6 × 5\n##   GEOID NAME                     `MVCRATE 2005` `MVCRATE 2014` `MVCRATE 2017`\n##   <chr> <chr>                             <dbl>          <dbl>          <dbl>\n## 1 13001 Appling County, Georgia            22.5           21.6           54.0\n## 2 13003 Atkinson County, Georgia           61.8           12.2           36.0\n## 3 13005 Bacon County, Georgia              66.3           44.3            0  \n## 4 13007 Baker County, Georgia              25.2           30.7           31.2\n## 5 13009 Baldwin County, Georgia            13.0           17.4           28.9\n## 6 13011 Banks County, Georgia              24.0           43.7           32.2"},{"path":"intro-tmap.html","id":"intro-tmap","chapter":"Week 18 Tips for using tmap","heading":"Week 18 Tips for using tmap","text":"Base-R capable data visualization plotting capabilities, fall short anything simple maps spatial data. Many packages including sp ggplot2 also functionality specifically optimized data visualization needs spatial epidemiologist. brief introductions packages.semester workhorse mapping/cartography tool tmap (thematic mapping) package. package builds grammar graphics logic built ggplot2 data visualizations conceived series layers information (e.g. axes, plot space, data points, lines, fill, legends, titles, etc) systematically stacked one top another. tmap start spatial object (e.g. data object either sf sp class) build visualization similarly combining adding together sequential layers., use data motor vehicle crash mortality dataset Georgia counties (vector polygon spatial data file), along information highways (vector line data file) trauma centers (vector point data).First load package, tmap browse help index:seeing range functions within tmap, import three datasets stored geopackage format begin visualizing:","code":"\n# load the tmap and sf packages\nlibrary(tmap)\nlibrary(sf)\n\nhelp('tmap')\n# import (read) three spatial datasets stored in geopackage format\nmvc <- st_read('GA_MVC/ga_mvc.gpkg')\nhwy <- st_read('GA_MVC/ga_hwy.gpkg')\ntrauma <- st_read('GA_MVC/trauma_centers.gpkg')"},{"path":"intro-tmap.html","id":"tmap-mode","chapter":"Week 18 Tips for using tmap","heading":"18.1 tmap mode","text":"One nice feature tmap two modes plotting maps. may develop general preference one another, although opinion serve slightly different purposes.plot mode produces conventional static maps viewed plot pane R-Studio, can saved file. main maps dissemination papers, posters, many presentations.view mode interactive plot html browser-like window. mode allows user interact map including panning, zooming, clicking spatial objects view underlying data. great data exploration, extensions web-served maps. However useful non-web-based dissemination want control map.select mode function tmap_mode() either 'plot' 'view' parentheses. Note set mode, subsequent maps mode…must re-submit tmap_mode() call switch back . default, tmap_mode() 'plot', means produces static maps. plot static maps, switch ’view' mode compare.","code":""},{"path":"intro-tmap.html","id":"qtm","chapter":"Week 18 Tips for using tmap","heading":"18.2 Quick maps: qtm()","text":"function qtm() stands Quick Thematic Maps, provides step simple plot() functions quickly plotting spatial objects. fundamental argument submitting qtm() name object plotted.produces geometry information (note unlike plot(), plot map every variable!).produce choropleth map (e.g. one objects shaded represent underlying statistic value), simply add name variable.Can tell legend cut-points determined? ’ll talk matters change later.Now try switching tmap_mode():Try things view mode:default visible R Studio Viewer pane; icon screen arrow allows show new window…biggerZoom outPanHover counties (see hovering?)Click counties (see click?)Underneath zoom + / - icon like stack papers. changes background map (background information change zoom /?)Click icon looks like stack pages. lets change background map (assuming currently connected internet)change back (like) :","code":"\nqtm(mvc)\nqtm(mvc, 'MVCRATE_05')\ntmap_mode('view')\ntmap_mode('plot')"},{"path":"intro-tmap.html","id":"customizing-qtm-for-polygons","chapter":"Week 18 Tips for using tmap","heading":"18.2.1 Customizing qtm() for polygons","text":"polygon data, might like control several features including title, color palette, style continuous variables categorized legend.syntax customizes original plot several ways:changing fill.style (style continuous variables categorized order plot sequential ramp choropleth map) default (fixed equal intervals) quantile style (default quantiles \\(n=5\\) quintiles although schemes including tertiles quartiles possible also)choosing custom color palette, case Yellow-Green-Blue (YlGnBu) palette, one several built-options.Providing informative title legend, rather default variable name.Notice label legend code “/n” inserted middle line. use forward slash creates called escape character. case “/n” inside character string inserts line break. “(2017)” line “MVC Mortality”.","code":"\nqtm(mvc,\n    fill = 'MVCRATE_17', \n    fill.style = 'quantile', \n    fill.palette = 'YlGnBu',\n    fill.title = 'MVC Mortality \\n(2017)')"},{"path":"intro-tmap.html","id":"customizing-qtm-for-lines","chapter":"Week 18 Tips for using tmap","heading":"18.2.2 Customizing qtm() for lines","text":"qtm() (tmap generally) can also handle types spatial data including line shape objects, can provide customization results. Try highway dataset:basic plot highways uses default colors sizes, plot uses lines.lwd= argument specify line width thickness. lines.col= sets color.","code":"\nqtm(hwy, \n    lines.lwd = 2, \n    lines.col = 'red')"},{"path":"intro-tmap.html","id":"customizing-qtm-for-points","chapter":"Week 18 Tips for using tmap","heading":"18.2.3 Customizing qtm() for points","text":"surprisingly, similar control point spatial objects, case locations trauma centers.symbols.size symbols.shape specified, symbolized variables modifying size shape. also settings color. study help documentation, notice arguments require numbers (thus use LEVEL_number integer) allow character/factors (thus use LEVEL).","code":"\nqtm(trauma,\n    symbols.size = 'LEVEL_number', \n    symbols.shape = 'LEVEL')"},{"path":"intro-tmap.html","id":"finding-valid-color-options","chapter":"Week 18 Tips for using tmap","heading":"18.2.4 Finding valid color options","text":"base R many ways specify colors including using standardized character strings, well HEX codes complicated alphanumeric labels used across industries identify unique colors. one many lists base-R color names: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdfHowever mapping often want just single colors, reasonable sets colors symbolizing sequential values, categorical values, diverging values. coming weeks talk choose color style symbolizing maps. several sources finding effective color palettes spatial mapping. One best resources choosing color palette iw Color Brewer website.Another source color palettes actually built right tmap package (actually part add-package installed called tmaptools). tool actually directly derived Color Brewer site , helps make clear name palettes tmap.Occasionally next step caused session R crash. Therefore usually open second instance R Studio just next thing. simply go Session R Studio menu click New Session. creates another completely independent instance R Studio (e.g. none packages data loaded current session present new session unless specify ).use package name (tmaptools) followed double colon (::)? shortcut R lets call single function package without loading package. Basically says “go look package called tmaptools load specified function”. use shortcut (general, tmaptools) one two situations:function name two packages, specifying package identifies one mean. instance soon learn package dplyr function select() package also name function another package handling spatial data called raster. often use dplyr::select() disambiguate.situations like tmaptools::palette_explorer() really need one function currently need anything else package.may discover experimentation, tmaptools::palette_explorer() function actually small interactive app opens new window lets see array color palettes. can see divided sequential, divergent, categorical color ramps can move slider change many categories see color ranges. thing want explorer abbreviated names left color ramp.","code":"\ntmaptools::palette_explorer()"},{"path":"intro-tmap.html","id":"building-maps-with-tmap","chapter":"Week 18 Tips for using tmap","heading":"18.3 Building maps with tmap","text":"qtm() great quickly making map, want control map, want shift full functions tmap.","code":""},{"path":"intro-tmap.html","id":"building-blocks-in-tmap","chapter":"Week 18 Tips for using tmap","heading":"18.3.1 Building blocks in tmap","text":"tmap produces maps using grammar graphics approach means building final product ‘sum’ several fundamental components, plus possible options layers. three fundamental components maps tmap:Specify spatial object map using tm_shape().Following call tm_shape() generally specify layers wish symbolize map. words specifying shape doesn’t plot anything…just starting point. layers actual things object/shape plot. case polygons usually use tm_fill() specify layer fill polygon, although layers available (e.g. see base derived layers listed look help('tmap')).Finally, many instances want customize map layout features, title legend, might like add elements North arrow scale bar.given map, various layers steps connected together R code plus sign (+); highlights map sum many parts.NOTE: use pipe (%>%) plus (+) seemingly connect steps together ! perhaps unfortunate ggplot2 tmap use pipe dplyr. Beware choose correct connector function hand!pipe (%>%) links together separate functions. contrast plus (+) tmap ggplot2 add sub-parts instructions main function called.Note steps 1 2 can repeated many spatial objects wish layer. wanted put points lines top polygon shape, specify tm_shape() corresponding layers spatial object turn.code replicates first map qtm(), basically says, “Start object mvc symbolize two layers: first fills polygons represent MVCRATE_17 second adds polygon borders:Look help documentation tm_fill() see myriad ways can customize map! ’s little overwhelming, ’d suggest looking style palette arguments, using -mentioned palette_explorer() try different colors different styles cut-points.Reverse order color paletteBy default, color palettes – sequential, divergent, categorical – arranged particular order. want colors go opposite direction? can specify putting hyphen ‘negative sign’ inside quotes name selected color palette. example:","code":"\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17') +\n  tm_borders()\nm1<- tm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          palette = 'BuPu',\n          style = 'quantile') +\n  tm_layout(main.title = \"BuPu color in default order\",\n            inner.margins = c(0.01, 0.01, 0.05, 0.2)) +\n  tm_borders()\nm2<- tm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          palette = '-BuPu',\n          style = 'quantile') +\n  tm_layout(main.title = \"BuPu color in reverse order\",\n            inner.margins = c(0.01, 0.01, 0.05, 0.2)) +\n  tm_borders()\ntmap_arrange(m1, m2)"},{"path":"intro-tmap.html","id":"customizing-text-on-maps","chapter":"Week 18 Tips for using tmap","heading":"18.3.2 Customizing text on maps","text":"several ways may wish customize text maps. example may want provide name legend, new labels categories, title, subtitle caption whole map.give title legend map use title = 'xxx' tm_fill() (layer function) call.change labels legendTo add source credits annotationThe tm_fill() option creates 5 bins categories plotting default. reason unnecessary put n = 5 specify many categories. However explicit number categories provide vector 5 labels correspond categories. course one choose non-default number categories (e.g. n = 3 n = 7), custom labels provided many labels categories.","code":"\n  # First, I create a vector of my custom legend labels\n  # (note, there must be same number of labels as there are categories in map)\nmyLabels <- c('Low (Q1)', 'Q2', 'Q3', 'Q4', 'Hi (Q5)')\n\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          style = 'quantile',\n          title = 'MVC Rate in 2017',\n          n = 5, \n          labels = myLabels) +\n  tm_borders() +\ntm_layout(title = 'Motor Vehicle Crashes per capita in Georgia',\n          legend.outside = T) +\ntm_credits('Source: Georgia OASIS, retrieved 2019')"},{"path":"intro-tmap.html","id":"adding-two-or-more-spatial-objects-in-one-map","chapter":"Week 18 Tips for using tmap","heading":"18.3.3 Adding two or more spatial objects in one map","text":"Just like ArcGIS, additional spatial layers can added produce informative map. instance interested highways trauma centers related motor vehicle mortality rates add layers.Several things note code:three separate spatial objects plotted, called starting tm_shape() followed additional function specific layer. See help documentation, Tenekes article Canvas table layers available kinds shapes (e.g. polygons, points, lines).step added feature (e.g. call parentheses) connected together plus signsWithin step (e.g. within parentheses), arguments separated commasI organize code vertically think makes readable one line.Try changing arguments try substituting different options!","code":"\ntm_shape(mvc) + \n  tm_fill('MVCRATE_17',\n          style = 'quantile',\n          palette = 'Purples') +\n  tm_borders() +\n\ntm_shape(hwy) + \n  tm_lines(lwd = 2, col = 'red') +\n  \ntm_shape(trauma) + \n  tm_bubbles(shape = 'LEVEL',\n             col = 'pink')"},{"path":"intro-tmap.html","id":"controlling-layout-and-map-elements","chapter":"Week 18 Tips for using tmap","heading":"18.4 Controlling layout and map elements","text":"audience map , making look ‘just right’ may critical. However, creating map share colleagues, stakeholders, public, cartographic design important effective visual communication.tmap wide range tools customize way single map (even set maps) looks. fact many can feel overwhelming first. best advice use help documentation often, experiment lot! repeat help documentation completely, provide guidance several common layout needs options.Note focus examples static maps presented tmap_mode('plot'). Many options behave using interactive tmap_mode('view'), interactive html plots dynamically resize, formatting may differ. See ?tm_view() information options specific interactive mode.","code":""},{"path":"intro-tmap.html","id":"understanding-the-graphic-space-in-tmap","chapter":"Week 18 Tips for using tmap","heading":"18.4.1 Understanding the graphic space in tmap","text":"see discussion , many tools adjust size position elements fit way want, accomplish desired graphic layout. one recurring source frustration understanding parameters move parts graphical space. apparent, see plot really set nested plot spaces border margin width control.see can ‘turn ’ global option called design.mode using function tmap_options(). colorize different parts plot space, messaging names space. can help figure whether need control inner.margins, outer.margins move things panel overall plot space.example uses two-map plot illustrate information returned:see text output (interprets colors), actually several different plot spaces.device (yellow) means full extent output device, whether screen, .png .pdfouter.margins (indicated green) shows far edges plot area edge graphic devicemaster shape (indicated red) actual plotted map. see instructions adjusted inner.margins; margins distance red area blue area. want map smaller inside frame, use inner.margins shrink size red area","code":"\n# Turn 'on' the design.mode option\ntmap_design_mode(T)\n\n# Plot a map of two rates, side by side (e.g. see small multiples below)\ntm_shape(mvc) + \n  tm_fill(c('MVCRATE_05', 'MVCRATE_17'),\n          palette = 'Purples',\n          style = 'quantile') +\ntm_borders()\n# turn off the design.mode unless you want to see it on the next map you plot\ntmap_design_mode(F)"},{"path":"intro-tmap.html","id":"controlling-map-layout","chapter":"Week 18 Tips for using tmap","heading":"18.4.2 Controlling map layout","text":"function tm_layout() controls title, margins, aspect ratio, colors, frame, legend, among many things. Type ?tm_layout() review help documentation see long list arguments can modify. Arguments via tm_layout() function incorporated map ‘adding’ (e.g. using + sign) tmap object, just add tm_fill() tm_borders().Explaining code :Aspects layout specified different steps:tm_fill() permits specification title legend. Notice inclusion \\n within title. noted , called escape character particular one forces line break/carriage return, let wrap title legend onto two linestm_borders() familiar, use alpha argument specify transparency borders, resulting lighter color. alpha parameter ranging 0 (fully transparent, invisible) 1 (transparency). can use alpha many different settings; useful maps many units (e.g. map U.S. counties) diminish visual impact boundaries using transparency.tm_layout() function many purposes, two things: add overall title map, adjust spacing inside frame line things fit:inner.margins controls big mapped figure relation overall frame; using argument way squish things around legend elements fit without bumping .\nargument expects vector four number going bottom, left, top, right\nvalues vector four numbers can range 0 1, representing relative amount space map object frame. use inner.margins = c(0.02, 0.02, 0.1, 0.2) means little extra space bottom left (0.02 ), extra space top (0.1) even right (0.2). arrived values trial error necessary keep legend bumping map.\nargument expects vector four number going bottom, left, top, rightThe values vector four numbers can range 0 1, representing relative amount space map object frame. use inner.margins = c(0.02, 0.02, 0.1, 0.2) means little extra space bottom left (0.02 ), extra space top (0.1) even right (0.2). arrived values trial error necessary keep legend bumping map.Explaining code :code differed three ways:used option request histogram legend, using legend.hist = T tm_fill() functionI moved entire legend outside frame specifying legend.outside = T tm_layout() function. Note addition shifting legend outside, can also control location using legend.position (changing location inside frame) legend.outside.position (controlling position outside frame).","code":"\n# Using tm_fill and tm_layout to control layout and text\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          style = 'quantile',\n          palette = 'BuPu',\n          title = 'Deaths per 100,000, \\n2017') +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = 'Car crash mortality in Georgia',\n            inner.margins = c(0.02, 0.02, 0.1, 0.2))\n# Adding a histogram to the legend and moving the legend outside of the frame\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          style = 'quantile',\n          palette = 'BuPu',\n          title = 'Deaths per 100,000, \\n2017',\n          legend.hist = T) +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = 'Car crash mortality in Georgia',\n            legend.outside = T)"},{"path":"intro-tmap.html","id":"adding-map-elements","chapter":"Week 18 Tips for using tmap","heading":"18.4.3 Adding map elements","text":"Finally, may wonder add map elements like north arrows, scale bars, captions, etc.extremely busy map many elements, illustrating features:","code":"\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17',\n          style = 'quantile',\n          palette = 'BuPu',\n          title = 'Deaths per 100,000, \\n2017',\n          legend.hist = T) +\n  tm_borders(alpha = 0.2) +\n  tm_layout(main.title = 'Car crash mortality in Georgia',\n            legend.outside = T, \n            inner.margins = c(0.1, 0.02, 0.02, 0.1)) +\n  tm_compass(type = '4star', \n             size = 2,\n             position = c('right', 'top')) +\n  tm_scale_bar(position = c('left', 'bottom')) +\n  tm_credits('Source: Georgia OASIS') +\n  tm_grid(alpha = 0.2)"},{"path":"intro-tmap.html","id":"change-the-global-style-of-a-map","chapter":"Week 18 Tips for using tmap","heading":"18.4.4 Change the global style of a map","text":"tmap several pre-defined ‘styles’ ‘themes’ maps. may strategy chose epidemiologic maps, quick easy way achieve certain ‘feel’ map. style simply means set options preset (user can still modify individual elements) produce particular look. see examples global map produced using ten different styles, type tmap_style_catalog() console. computer took approximately 60-90 seconds produce ten separate .png files sub-folder project. can browse see styles differ. Two examples shown :\nFIGURE 4.1: tmap style: Natural\n\nFIGURE 18.1: tmap style: Classic\n","code":""},{"path":"intro-tmap.html","id":"making-small-multiple-maps","chapter":"Week 18 Tips for using tmap","heading":"18.5 Making small-multiple maps","text":"Small multiples refers production multiple maps presented set. often desire small multiples way visually compare two features easy put map.three ways prepare small multiples tmap. look , notice differ respect number legends produced, range legends, content flexibility customization within map panels.","code":""},{"path":"intro-tmap.html","id":"small-multiples-as-a-vector-of-variables","chapter":"Week 18 Tips for using tmap","heading":"18.5.1 Small multiples as a vector of variables","text":"plot side--side maps two variables spatial object, simply call vector variable names specifying layer symbolization.strategy produced single map variable listed vector, map unique legend, determined breaks data variable.","code":"\ntm_shape(mvc) + \n  tm_fill(c('MVCRATE_05', 'MVCRATE_17'),\n          palette = 'Purples',\n          style = 'quantile',\n          title = c('Mortality, 2005', 'Mortality, 2017')) +\n  tm_borders() +\n  tm_layout(inner.margins = c(0.02, 0.02, 0.1, 0.2),\n            legend.position = c('right', 'top'))"},{"path":"intro-tmap.html","id":"tmap-facet","chapter":"Week 18 Tips for using tmap","heading":"18.5.2 Small multiples with facets","text":"Facet plotting something common package ggplot2. refers production two plot figures stratified ‘grouping’ variable. Typically facet plots ggplot2, scale \\(x\\) \\(y\\) axis held constant across set plots values plotted readily comparable.tmap, facet plotting means creating multiple map plots distinguished slicing stratifying spatial units along group. Faceting can useful highlighting patterns among different sub-groups spatial data. Unlike ggplot2, scale legend bounds x, y coordinate extent enforced across panel maps default. Instead min/max x, y coordinates can vary according scope content panel (e.g. default, free.coords = T).default range cut-points legend held constant across maps (e.g. single legend produced represent data maps).like force consistency panels (e.g. either better contextualization comparability), can specified. Argument free.coords = FALSE (e.g. map min/max x, y coordinate range) free.scale=FALSE (e.g. map spatial scale ‘zoom’ appropriate contents panel).strange facet map produced stratifying NCHS urban/rural six-level categorization scheme. First code happens default, setting free.coords free.scales FALSE. can see default, map frame zooms maximize selected object, scale different . contrast forced maintain constant scale easier see relative size locations subset.","code":"\n# Basic facet map with defaults\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17') +\n  tm_borders() +\n  tm_facets(by = 'nchs_code')\n# With facet parameters set to FALSE\ntm_shape(mvc) +\n  tm_fill('MVCRATE_17') +\n  tm_borders() +\n  tm_facets(by = 'nchs_code', free.coords = FALSE, free.scales = FALSE)"},{"path":"intro-tmap.html","id":"facets-for-time-series","chapter":"Week 18 Tips for using tmap","heading":"18.5.3 Facets for time-series","text":"small multiples vector--variables facets differ:One point, might obvious first, distinguishes first two methods small multiple map productions use data separate maps. Notice first option (supplying vector variables plot using c() call within tm_fill() example) good mapping things wide data. words maps separate columns different maps.contrast tm_facets() creates separate maps stratifying rows data. words good mapping things long data. used idea long versus wide data might seem confusing, relatively common distinction data handling.extension idea wanted map time-series (e.g. maps disease rates year series years), create long dataset year. Imagine dataset row data every county Year 1; separate dataset row data every county Year 2; . stacking datasets dataset becomes long number geographic units \\(\\times\\) number years. easily ArcGIS, perfectly allowable sf class spatial objects. plotting, simply use tm_facets() = YEAR produce series.example taking current ‘wide’ dataset (e.g. currently 3 years separate columns), making long dataset (e.g. single column MVCRATE, separate column year distinguish year-rate talking ). produce time-series faceted maps. case use tidy functionality pivot_* verbs (e.g. read use pivot verbs )Now, plot long sf object ignoring fact three rows data every county. Can tell happens?Notice maps ? Try changing YEAR == 2017 different year. can see ignored long format, tmap essentially plotted Georgia counties 3 times, last layer (e.g. 2017) top thus one see. beware…Now let’s take advantage long format dataset facet sub-divide dataset separate maps delineated year variable:","code":"\nnrow(mvc) # N = 159 rows corresponds to N=159 Georgia counties## [1] 159\nmvc_long <- mvc %>%\n  select(GEOID, NAME, MVCRATE_05, MVCRATE_14, MVCRATE_17) %>%\n  as_tibble() %>%\n  pivot_longer(cols = starts_with(\"MVCRATE\"),\n               names_to = c(\".value\", \"year\"),\n               values_to = \"mvc_rate\",\n               names_sep = \"_\") %>%\n  mutate(year = 2000 + as.numeric(year)) %>%\n  st_as_sf()\nnrow(mvc_long) # N =477 rows corresponds to 3 years each for N =159 counties  ## [1] 477\n# This is the WRONG way to plot a long dataset!\ntm_shape(mvc_long) +\n  tm_fill('MVCRATE') +\n  tm_borders()\n# If you want a single map from a long dataset, use the subset() function ...\ntm_shape(subset(mvc_long, year == 2017)) +\n  tm_fill('MVCRATE') +\n  tm_borders()\ntm_shape(mvc_long) +\n  tm_fill('MVCRATE') + \n  tm_borders() +\ntm_facets(by = 'year', ncol = 1)"},{"path":"intro-tmap.html","id":"small-multiples-with-tmap_arrange","chapter":"Week 18 Tips for using tmap","heading":"18.6 Small multiples with tmap_arrange()","text":"third way make small multiples, one gives maximum control separate panel, create one time, combining panel using function tmap_arrange(). notable difference name map object create , provide list names tmap_arrange().example used two totally different shape objects illustrate point tmap_arrange() particularly good combining things simply wide long subsets single dataset. approach also good taking totally different approach symbolizing two variables dataset, doesn’t assume trying keep anything .","code":"\nm1 <- tm_shape(mvc) +\n  tm_fill('MVCRATE_05') +\n  tm_borders()\n\nm2 <- tm_shape(trauma) +\n  tm_symbols(shape = 'LEVEL',\n             col = 'LEVEL')\n\ntmap_arrange(m1, m2)"},{"path":"intro-tmap.html","id":"summarizing-small-multiples","chapter":"Week 18 Tips for using tmap","heading":"18.7 Summarizing small multiples","text":"Small multiples common visualization GIS software like ArcGIS. small multiples need create multiple data frames manipulate Layout view; often difficult get consistent scales, legends, coordinates.R, idea faceting quite common much potential spatial epidemiology, emphasized . summarize overarching differences among three approaches future reference.","code":""},{"path":"intro-tmap.html","id":"saving-maps","chapter":"Week 18 Tips for using tmap","heading":"18.8 Saving maps","text":"Saving maps use programs applications important. Images can saved output formats available R image functions. words can save files .png, .pdf, .jpg, .tiff, etc.quick way use export button plot pane R studio.Recall way graphic R looks shaped part active graphic device. screen plot pane default graphic device things arranged look good screen. However save different graphic device (e.g. jpg device), things might look different. sometimes trial--error troubleshooting width, height, dpi options.specify save via code, rather export button (good idea terms reproducible code!) use tmap_save(). save final two-panel map created previous step :now skills make wide variety maps R. fine-tune tmap works customize desired purpose, likely spend lot time looking help documentation online resources. sometimes tedious, process figuring make just map want valuable. time able create sophisticated maps quickly efficiently.","code":"\n# First make it an object by giving it a name, m3\nm3 <- tmap_arrange(m1, m2)\n\ntmap_save(m3, filename = 'mvc_maps.png')"},{"path":"kde-extract.html","id":"kde-extract","chapter":"Week 19 Using kernel density estimates to quantify an exposure for Spatial Epi","heading":"Week 19 Using kernel density estimates to quantify an exposure for Spatial Epi","text":"discussed Disease Mapping IV, kernel density estimation can used map diseases (e.g. outcomes), can also used create continuous surfaces epidemiologic exposure covariate. create continuous geographically weighted surface exposure, might wonder extract join polygon point data analysis.GOAL: purpose appendix take exposure defined points (e.g. locations power plants, health clinics, waste sites), connect sf data frame outcome (e.g. aggregated polygon) analysis.","code":""},{"path":"kde-extract.html","id":"prepare-the-exposure-surface","chapter":"Week 19 Using kernel density estimates to quantify an exposure for Spatial Epi","heading":"19.1 Prepare the exposure surface","text":"tutorial assumes data points object. Recall retrieved data latitude longitude numbers columns, can use sf tools create spatial points object.","code":""},{"path":"kde-extract.html","id":"step-1.","chapter":"Week 19 Using kernel density estimates to quantify an exposure for Spatial Epi","heading":"19.1.1 Step 1.","text":"kernel density points, use sparr package requires data get converted sf older sp","code":"\n# df_sf is an sf object where each row is a point feature representing the location of an FQHC health care clinic\n\n# This converts sf to sp\ndf_sp <- df_sf %>% as(\"Spatial\")"},{"path":"kde-extract.html","id":"step-2.","chapter":"Week 19 Using kernel density estimates to quantify an exposure for Spatial Epi","heading":"19.1.2 Step 2.","text":"Create observation window (owin) defines outer bounds study area. NOTE: point dataset exposure polygon representing study area must CRS projection.code , object study_area polygon defining outer bounds included study area. akin county boundary used Disease Mapping IV.","code":"\n# Create an observation window within which the kernel density estimation will be constrained\nstudy_owin <- maptools::as.owin.SpatialPolygons(study_area)"},{"path":"kde-extract.html","id":"step-3.","chapter":"Week 19 Using kernel density estimates to quantify an exposure for Spatial Epi","heading":"19.1.3 Step 3.","text":"Now follow steps Disease Mapping IV (corresponding lab) create ppp object create kernel density. Finally, convert kernel density raster object.kernel density represent?interpretation resulting surface depends data analyzing. Imagine points representing Federally Qualified Health Centers (FQHCs). converting discrete locations kernel density surface, quantifying average density FQHCs per square unit area (e.g. per square km).Therefore, meaning raster grid value fact density vicinity particular grid cell. “vicinity” defined kernel bandwidth.create ppp object:code , use fixed bandwidth 5km (note h0=5000 corresponds 5km projection unit square meters). Note use bandwidth diagnostics, use adaptive bandwidths setting, just Disease Mapping IV.Density Intensity?Remember, discussion Disease Mapping IV distinction “density” “intensity”. “intensity” ratio units (e.g. FQHCs int case) per area (e.g. per square km). true “density” intensity one location relative study area. means “density” always sum 1 across study area (e.g. formal statistical probability density).creating surface exposure, usually want units meaningful. Thus, intensity relevant tells us many units space. code sets density=FALSE indicate preference.Now can take img object output bivariate.density() convert raster format. Recall crs() coordinate reference information gets stripped away process, simply need re-define .plot resulting raster, values legend quite small, example, telling us many FQHCs every square meter!! Obviously many. multiply \\(1,000,000\\), values number FQHCs per square kilometer. still might small even multiply bigger number make number FQHCs per 10 per 100 square km.","code":"\n# Create the birth ppp object\nfqhc_ppp <- ppp(x = st_coordinates(df_sp)[, 1], \n             y = st_coordinates(df_sp)[, 2],\n             window = study_owin)\nfqhc_kde <- bivariate.density(pp = fqhc_ppp, \n                              h0 = 5000, \n                              density = FALSE,\n                              edge = 'diggle')\nfqhc_kde_raster <- raster(fqhc_kde$z)\ncrs(fqhc_kde_raster) <- \"EPSG:5070\""},{"path":"kde-extract.html","id":"step-4.","chapter":"Week 19 Using kernel density estimates to quantify an exposure for Spatial Epi","heading":"19.1.4 Step 4.","text":"Everything now similar Disease Mapping IV. now want extract values. words want know average continuous FQHC intensity given point measured health outcome. health outcomes aggregated polygons (e.g. rates per census tract county), might choose use centroid polygon place extract. , hand, points actual street address individuals, use locations extraction. words, goal extract raster specific points.target outcome unit analysis polygon named tracts-ms. extraction want points, use centroid every tract point reference.goal now extract values raster (e.g. merge join point raster connect raster cell value undelying point). point object needs class sp , convert first (e.g. d_sp <- df_tract_point %>% (\"Spatial\")).Now use extract() function raster package:code specifies raster first, object containing point information second. produce vector many values points df_tract_point. Now simply join onto analytic dataset outcome bring exposure outcome together!","code":"\ndf_tract_point <- st_centroid(df_tracts_ms)\nexposure_value <- raster::extract(fqhc_kde_raster, df_tract_point)\ntracts_ms$fqhc_inensity <- exposure_value"}]
